# 1. Architecture Overview

A-R-E-S Mini is built on a modular, two-phase architecture: **Ingestion** and **Retrieval**. The entire system is designed around a "best-of-both-worlds" philosophy: leveraging the robust, declarative orchestration of LangChain Expression Language (LCEL) while injecting custom, high-performance components where they matter most.

## Design Philosophy

-   **Declarative Flow:** Instead of a complex, hard-coded orchestrator, the entire RAG pipeline is an LCEL chain. This makes the flow of data explicit, easy to debug, and simple to modify.
-   **Component Specialization:** We avoid generic abstractions for performance-critical tasks. A-R-E-S Mini uses custom components that faithfully replicate advanced RAG logic:
    -   **Chunking:** `ChonkieTextSplitter` for high-performance, semantically-aware splitting.
    -   **Embedding:** `LateChunkingEmbedder` for creating context-aware embeddings.
    -   **Retrieval:** `CustomQdrantRetriever` for direct, efficient hybrid search.
-   **Flexible Model Providers:** Key components like embedding and reranking are not locked into a single inference engine. The system uses a factory pattern to select between a centralized `vLLM` server or direct, local model loading via `local_transformers`, controlled by a single line in the configuration.
-   **Configuration-Driven:** All system behavior is controlled via the `ares_mini_config.toml` file.

## System Workflow

### Ingestion Pipeline

The ingestion pipeline converts raw documents into an optimized, multi-layered representation. It's designed for massive parallelism to handle large document sets efficiently.

**High-Level Flow:** `File -> Load -> Describe -> Chunk -> Embed (Hybrid + Late Chunking) -> Store`

1.  **Load & Describe:** The document is loaded, and a high-level title and summary are generated by an LLM.
2.  **Chunk:** The document is split into small, manageable chunks, while retaining page number metadata.
3.  **Embed:** Each chunk is converted into two types of vectors: a dense vector (for semantic meaning) and a sparse vector (for keyword relevance). This process is enhanced by **Late Chunking** and **AutoContext Headers** to make the embeddings deeply context-aware.
4.  **Store:** The vectors and metadata are stored in **Qdrant**, while the raw text and document structure are stored in a **SQLite** database for later use by the RSE process.

### Retrieval & Generation Pipeline

This pipeline is an adaptive system that first reasons about the user's query before executing a multi-stage refinement process to build the best possible context for the LLM.

**High-Level Flow:** `Query -> Plan -> Route -> Retrieve (Hybrid) -> Rerank -> RSE -> MMR -> Generate`

1.  **Plan & Route:** An LLM first analyzes the query to determine the best retrieval strategy (e.g., broad search on summaries vs. deep search on chunks).
2.  **Retrieve:** The `CustomQdrantRetriever` executes a hybrid search in Qdrant.
3.  **Refine:** The retrieved chunks go through a multi-stage refinement pipeline:
    -   **Reranking:** A powerful cross-encoder re-scores the chunks for higher accuracy.
    -   **Relevant Segment Extraction (RSE):** An algorithm reconstructs long, coherent segments of text, filling in contextual gaps.
    -   **MMR:** The final set of segments is diversified to avoid redundancy.
4.  **Generate:** The refined context is passed to the final LLM to generate a precise, source-grounded answer.