Directory Structure:

└── ./
    ├── configs
    │   ├── __init__.py
    │   ├── ares_baseline_test_config.toml
    │   ├── ares_lsc_rse_test_config.txt
    │   ├── ares_vllm_config_full_test.toml
    │   ├── ares_vllm_config.toml
    │   ├── default_config.txt
    │   └── lc_default_config.txt
    ├── core
    │   ├── __init__.py
    │   ├── config_loader.py
    │   ├── datatypes.py
    │   └── orchestrator.py
    ├── generation
    │   ├── __init__.py
    │   ├── llm_responder.py
    │   ├── post_checker.py
    │   └── prompt_builder.py
    ├── ingestion
    │   ├── __init__.py
    │   ├── chunker.py
    │   ├── description_generator.py
    │   ├── embedder_sparse.py
    │   ├── embedder.py
    │   ├── kg_updater.py
    │   ├── lsc_divider.py
    │   ├── normalizer.py
    │   ├── pipeline.py
    │   └── verifier.py
    ├── ingestion_plugins
    │   └── fast_pdf_parser.py
    ├── llm
    │   ├── __init__.py
    │   ├── base.py
    │   ├── lmstudio.py
    │   ├── ollama.py
    │   ├── openai_api.py
    │   └── vllm_api.py
    ├── maintenance
    │   ├── index_optimizer.py
    │   └── summarizer.py
    ├── retrieval
    │   ├── search
    │   │   ├── __init__.py
    │   │   ├── bm25_search.py
    │   │   ├── bm25.py
    │   │   ├── description_search.py
    │   │   ├── kg_search.py
    │   │   └── vector_search.py
    │   ├── __init__.py
    │   ├── base.py
    │   ├── manager.py
    │   ├── planner.py
    │   ├── reranker_qwen.py
    │   ├── reranker.py
    │   ├── rse.py
    │   └── selector.py
    ├── storage
    │   ├── cache
    │   │   ├── __init__.py
    │   │   └── kv_store_cache.py
    │   ├── knowledge_graph
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── networkx_impl.py
    │   │   ├── pathrag_networkx.py
    │   │   └── pathrag_wrapper.py
    │   ├── kv_store
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── duckdb.py
    │   │   ├── sqlite.py
    │   │   └── test_sqlite_kv_store.py
    │   ├── vector_db
    │   │   ├── __init__.py
    │   │   ├── base.py
    │   │   ├── chroma_local.py
    │   │   └── qdrant_local.py
    │   ├── __init__.py
    │   └── base.py
    ├── tests
    │   ├── llm
    │   │   └── test_ollama_wrapper.py
    │   ├── conftest.py
    │   └── test_normalizer.py
    ├── utils
    │   ├── __init__.py
    │   ├── helpers.py
    │   ├── logging_setup.py
    │   └── pathrag_logic.py
    ├── vllm_server
    │   └── manager.py
    ├── main_document_test_ALICE.py
    ├── main_document_test.py
    ├── main.py
    ├── requirements_api.txt
    ├── requirements_general.txt
    ├── requirements_local.txt
    └── requirements.txt



---
File: /configs/__init__.py
---




---
File: /configs/ares_baseline_test_config.toml
---

# --- ares/configs/ares_baseline_test_config.toml ---
# Configuración para simular el ARES "original" con Late Chunking y Descripciones de Ventana.
# Sin LSC, sin AutoContext Headers complejos para chunks, sin RSE.

[general]
working_dir = "./ares_data_baseline_test" # Directorio de trabajo específico para esta prueba
log_level = "DEBUG"                     # Máximo detalle para depuración

[ingestion]
# --- Chunking (Chonkie) ---
chunking_strategy = "recursive" 
chonkie_config = { chunk_size = 300, chunk_overlap = 50, rules = "default" }

# --- LSC (Late Sectioning Context) y AutoContext ---
# ***** DESHABILITADO PARA ESTA PRUEBA BASELINE *****
use_lsc = false                             # Deshabilitar división en Secciones-LSC
enrich_lsc_sections_with_llm = false        # Deshabilitado porque use_lsc es false
# section_enricher_llm_config_path = "ingestion.section_enricher_llm_config" # No necesario
# lsc_divider_config = { min_lines_per_section = 5, max_lines_per_section = 30 } # No necesario
use_autocontent_headers_for_chunks = false  # DESHABILITAR cabeceras AutoContext para embeddings de chunk

# --- Descripción de Ventana Completa ---
generate_descriptions = true                # Generar descripción para toda la ventana (documento)
description_llm_config_path = "ingestion.description_llm_config"
use_verifier = true                         # Verificar descripciones de ventana (como en el original)
verifier_llm_config_path = "ingestion.verifier_llm_config"

# NUEVA SECCIÓN para el generador de embeddings dispersos
[ingestion.sparse_embedder_config]
# Modelo para BM25-like (rápido y eficiente)
model_name = "Qdrant/bm25" 
# Podríamos usar "prithivida/Splade_PP_en_v1" para SPLADE, pero BM25 es más ligero.
local_options = {} # Opciones para el constructor de FastEmbed, ej: { "cache_dir": "/path/to/cache" }

# --- LLMs para Ingesta (Solo los necesarios para Desc. Ventana) ---
[ingestion.description_llm_config]
model_provider = "local_ollama"
model_name = "llama3:8b" 
local_options = { host_url = "http://localhost:11434", timeout = 180 }

[ingestion.verifier_llm_config]
model_provider = "local_ollama"
model_name = "phi3:mini" 
local_options = { host_url = "http://localhost:11434", timeout = 120 }

# [ingestion.section_enricher_llm_config] # No se usa si enrich_lsc_sections_with_llm es false
# model_provider = "none"
# model_name = "none"

update_kg = false

[embeddings]
model_provider = "local"
model_name = "jinaai/jina-embeddings-v3" # Usando Jina V3 como en la config de prueba anterior
local_options = { device = "cuda" } 
late_chunking_enabled = true         # Fundamental para Late Chunking

[storage]
provider = "sqlite"
path = "baseline_main_kv.db"

[vector_db]
provider = "qdrant_local"
path = "vector_db"
# NUEVO: Nombre que le daremos al vector disperso dentro de Qdrant.
qdrant_sparse_vector_name = "bm25_sparse_vector"
# Opcional: para forzar recreación de la colección si la estructura cambia.
recreate_on_conflict = false 

[knowledge_graph]
provider = "none"

[bm25_index]
provider = "whoosh"
path = "baseline_bm25_index"

# --- Retrieval (Configuración simple, ya que el foco es la ingesta) ---
[retrieval]
use_query_planner = false # Deshabilitar planner para simplificar la prueba de ingesta
# query_planning_mode = "lite" # No relevante si use_query_planner es false
# planner_llm_config_path = "retrieval.planner_llm_config"

# [retrieval.planner_llm_config]
# model_provider = "none"
# model_name = "none"

use_description_search = true # Asumiendo que el baseline podría buscar en descripciones
use_kg_search = false         
use_rse = false               # ***** DESHABILITADO PARA ESTA PRUEBA BASELINE *****

vector_top_k = 50
bm25_top_k = 50
description_top_k = 20

combination_method = "rrf"
rrf_k = 60                  
vector_weight = 0.6 # Ajustar si es necesario para replicar el original
bm25_weight = 0.4
description_weight = 0.3

intermediate_top_k = 100 

[retrieval.reranker]
enabled = true # Mantener reranker si el "original" lo usaba, o false para simplificar más
model = "jinaai/jina-reranker-v2-base-multilingual"
device = "cuda" 

diversification_method = "mmr" 
mmr_lambda = 0.6               
final_context_chunks = 7       
context_token_budget = 3500    
include_neighbor_chunks = true # Lógica de sándwich original

[responder_llm] # Configuración mínima, no es el foco de la prueba de RAM de ingesta
model_provider = "local_ollama"
model_name = "llama3:8b" 
local_options = { host_url = "http://localhost:11434", timeout = 120 }
system_prompt_template = "Responde la pregunta."
max_output_tokens = 512
temperature = 0.7

[generation]
include_citations = false 
use_post_checker = false 
# post_checker_llm_config_path = "generation.post_checker_llm_config"

# [generation.post_checker_llm_config]
# model_provider = "none"
# model_name = "none"

[maintenance] 
summary_schedule = "none"
index_maintenance_schedule = "none"
cache_retention_days = 30


---
File: /configs/ares_lsc_rse_test_config.txt
---

# --- ares/configs/ares_lsc_rse_test_config.toml ---
# Configuración para probar Late Sectioning Context (LSC), AutoContext, RSE y Query Planner Lite.

[general]
working_dir = "./ares_data_lsc_rse_test" # Directorio de trabajo específico para esta prueba
log_level = "DEBUG"                     # Máximo detalle para depuración

[ingestion]
# --- Chunking (Chonkie) ---
chunking_strategy = "recursive" 
chonkie_config = { chunk_size = 250, chunk_overlap = 40, rules = "default" } # Chunks un poco más pequeños

# --- LSC (Late Sectioning Context) y AutoContext ---
use_lsc = false                                  # Habilitar división en Secciones-LSC
enrich_lsc_sections_with_llm = false             # Habilitar títulos/resúmenes LLM para Secciones-LSC
section_enricher_llm_config_path = "ingestion.section_enricher_llm_config"
lsc_divider_config = { min_lines_per_section = 5, max_lines_per_section = 30 } # Config para LSCDivider
use_autocontent_headers_for_chunks = false       # Habilitar cabeceras AutoContext para embeddings de chunk

# --- Descripción de Ventana Completa ---
generate_descriptions = false                    # Generar descripción para toda la ventana (documento)
description_llm_config_path = "ingestion.description_llm_config"
use_verifier = false                             # Verificar descripciones de ventana
verifier_llm_config_path = "ingestion.verifier_llm_config"

# --- LLMs para Ingesta ---
[ingestion.description_llm_config] # Para Descripciones de Ventana
model_provider = "local_ollama"
model_name = "llama3:8b" 
local_options = { host_url = "http://localhost:11434", timeout = 180 }

[ingestion.verifier_llm_config] # Para verificar Descripciones de Ventana
model_provider = "local_ollama"
model_name = "phi3:mini" 
local_options = { host_url = "http://localhost:11434", timeout = 120 }

[ingestion.section_enricher_llm_config] # Para títulos/resúmenes de Secciones-LSC
model_provider = "local_ollama"
model_name = "phi3:mini" # Usar un modelo rápido para esto
local_options = { host_url = "http://localhost:11434", timeout = 120 }

update_kg = false # Mantener KG deshabilitado por ahora

[embeddings]
model_provider = "local"
model_name = "jinaai/jina-embeddings-v3" # o el que tengas configurado
local_options = { device = "cuda" } # o "cpu"
late_chunking_enabled = true         # Fundamental

[storage]
provider = "sqlite"
path = "lsc_rse_main_kv.db"

[vector_db]
provider = "qdrant_local"
path = "lsc_rse_vector_db"
# embedding_dim se tomará del embedder

[knowledge_graph]
provider = "none"

[bm25_index]
provider = "whoosh"
path = "lsc_rse_bm25_index"

[retrieval]
use_query_planner = true
query_planning_mode = "lite" # Probar primero el modo Lite
planner_llm_config_path = "retrieval.planner_llm_config"

[retrieval.planner_llm_config]
model_provider = "local_ollama"
model_name = "phi3:mini" 
local_options = { host_url = "http://localhost:11434", timeout = 120 }
# Config para el planner (si se añaden en helpers.py o QueryPlanner)
# planner.max_tokens_initial = 500
# planner.max_tokens_refinement = 700
# planner.temperature = 0.1


use_description_search = true # Buscar en descripciones de ventana
use_kg_search = false         

# K iniciales para recuperación (más altos para RSE/MMR)
vector_top_k = 150
bm25_top_k = 150
description_top_k = 30 # Un k más bajo para descripciones, son menos numerosas

# --- RSE (Relevant Segment Extraction) ---
use_rse = true
[retrieval.rse_params]
max_length_segment = 10           # Max chunks por segmento RSE
overall_max_length_chunks = 25    # Max chunks totales de todos los segmentos RSE
minimum_value_segment = 0.3       # Score RSE mínimo para considerar un segmento
irrelevant_chunk_penalty = 0.2    # Penalización para chunks no tan relevantes
decay_rate = 25                   # Factor de decaimiento para el rank en RSE
top_k_for_doc_selection = 7       # Cuántos docs (ventanas) considerar para construir el meta-doc RSE
chunk_length_adjustment = true
reference_chunk_length = 600      # Longitud de referencia para ajuste de score por longitud

combination_method = "rrf"  # Para combinar resultados de diferentes fuentes antes de RSE/MMR
rrf_k = 60                  
# Los pesos no se usan con RRF, pero se dejan por si se cambia el método
vector_weight = 0.6
bm25_weight = 0.25
description_weight = 0.15

intermediate_top_k = 200 # Cuántos items pasar del paso de combinación al de Reranker/RSE

[retrieval.reranker]
enabled = true 
model = "jinaai/jina-reranker-v2-base-multilingual"
device = "cuda" # o "cpu"

# --- Selección Final ---
diversification_method = "mmr" # Usar MMR sobre los resultados de RSE + Descripciones
mmr_lambda = 0.65              # Balancear un poco más hacia relevancia
final_context_chunks = 6       # Número final de items (segmentos RSE o descripciones) para el LLM
context_token_budget = 3800    

include_neighbor_chunks = false # RSE debería manejar la continuidad contextual, desactivar esto

[responder_llm]
model_provider = "local_ollama"
model_name = "llama3:8b" 
local_options = { host_url = "http://localhost:11434", timeout = 240 } # Mayor timeout para responder
system_prompt_template = """Eres ARES, un asistente IA experto en filosofía y textos complejos. Se te proporciona un 'Contexto Recuperado' que contiene fragmentos del texto 'El Único y su Propiedad' de Max Stirner, y posiblemente resúmenes o análisis relacionados. Tu tarea es responder la consulta del usuario utilizando ÚNICA y PRECISAMENTE la información encontrada en el 'Contexto Recuperado'. No inventes información ni utilices conocimiento externo. Si la respuesta no se encuentra en el contexto, debes decirlo explícitamente (ej. "La información solicitada no se encuentra en el contexto proporcionado."). Sé conciso y cita tus fuentes si es posible (ej. "Según el texto [segmento:ID_SEGMENTO]...").

Contexto Recuperado:
--- INICIO DEL CONTEXTO ---
{retrieved_context}
--- FIN DEL CONTEXTO ---

Ahora, responde la siguiente consulta del usuario basándote estrictamente en el contexto anterior:
"""
max_output_tokens = 768
temperature = 0.4

[generation]
include_citations = true # Probar citaciones (necesitará adaptar PromptBuilder y el template)
use_post_checker = false 
# post_checker_llm_config_path = "generation.post_checker_llm_config"

# [generation.post_checker_llm_config]
# model_provider = "none"
# model_name = "none"

[maintenance] # Deshabilitar mantenimiento para pruebas enfocadas
summary_schedule = "none"
index_maintenance_schedule = "none"
cache_retention_days = 30


---
File: /configs/ares_vllm_config_full_test.toml
---

# ==============================================================================
# ARES Configuration File - vLLM-Managed Architecture (v3 - FINAL)
# ==============================================================================
# Esta versión corrige la configuración del servidor vLLM eliminando
# argumentos inválidos.
# ==============================================================================

[general]
working_dir = "./ares_data_vllm"
log_level = "INFO"

# ------------------------------------------------------------------------------
# VLLM-Managed Server Configuration
# ------------------------------------------------------------------------------
[vllm_server]
enabled = true
model = "Qwen/Qwen3-0.6B" 
host = "127.0.0.1"
port = 8000
dtype = "auto"
# Se reduce ligeramente para dar más margen a la compilación de kernels en el primer arranque.
gpu_memory_utilization = 0.65 
max_num_seqs = 256
max_num_batched_tokens = 24576
max_model_len = 8192

# El campo vllm_extra_args queda vacío. El flag para logprobs fue eliminado.
vllm_extra_args = { enforce-eager = true }

# ------------------------------------------------------------------------------
# Ingestion Pipeline Configuration
# ------------------------------------------------------------------------------
[ingestion]
chunking_strategy = "recursive" 
chonkie_config = { chunk_size = 400, chunk_overlap = 80 }

generate_descriptions = true
use_verifier = true
use_lsc = true
enrich_lsc_sections_with_llm = true
use_autocontent_headers_for_chunks = true
update_kg = false

[ingestion.description_llm_config]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"

[ingestion.verifier_llm_config]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"

[ingestion.section_enricher_llm_config]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"

[ingestion.sparse_embedder_config]
enabled = true
model_name = "Qdrant/bm25"

# ------------------------------------------------------------------------------
# Embeddings Configuration
# ------------------------------------------------------------------------------
[embeddings]
model_provider = "local"
model_name = "Qwen/Qwen3-Embedding-0.6B"
local_options = { device = "cuda" }
late_chunking_enabled = true

# ------------------------------------------------------------------------------
# Storage Configuration
# ------------------------------------------------------------------------------
[storage]
provider = "sqlite"
path = "main_kv.db"

[vector_db]
provider = "qdrant_local"
path = "vector_db"
qdrant_sparse_vector_name = "bm25_sparse_vector"
recreate_on_conflict = true

[knowledge_graph]
provider = "none"

# ------------------------------------------------------------------------------
# Retrieval Pipeline Configuration
# ------------------------------------------------------------------------------
[retrieval]
use_query_planner = true
query_planning_mode = "lite"

[retrieval.planner_llm_config]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"

use_description_search = true
use_kg_search = false
use_rse = true

vector_top_k = 150
sparse_lexical_top_k = 150
description_top_k = 20

combination_method = "rrf"
rrf_k = 60
intermediate_top_k = 200 

[retrieval.reranker]
enabled = true
model = "Qwen/Qwen3-Reranker-0.6B"
[retrieval.reranker.llm_config]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"

diversification_method = "mmr" 
mmr_lambda = 0.65
final_context_chunks = 7
context_token_budget = 7000

# ------------------------------------------------------------------------------
# Final Response Generation
# ------------------------------------------------------------------------------
[responder_llm]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"
system_prompt_template = """Eres ARES, un asistente IA experto. Responde la consulta del usuario basándote estricta y únicamente en el 'Contexto Recuperado' proporcionado. Si la respuesta no se encuentra en el contexto, declara explícitamente: "La información no se encuentra en el contexto proporcionado." Sé conciso.

Contexto Recuperado:
---
{retrieved_context}
---

Responde la siguiente consulta basándote solo en el contexto anterior:
"""
max_output_tokens = 1024
temperature = 0.3

# ------------------------------------------------------------------------------
# Post-Generation Verification
# ------------------------------------------------------------------------------
[generation]
use_post_checker = false

[generation.post_checker_llm_config]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"

# ------------------------------------------------------------------------------
# Maintenance (Deshabilitado para pruebas)
# ------------------------------------------------------------------------------
[maintenance] 
summary_schedule = "none"
index_maintenance_schedule = "none"


---
File: /configs/ares_vllm_config.toml
---

# ==============================================================================
# ARES Configuration File for vLLM-Managed Architecture
# ==============================================================================
# Este archivo está diseñado para ser el punto de partida para ejecutar ARES
# con el backend de inferencia vLLM autogestionado.
# ==============================================================================

[general]
# Directorio donde ARES guardará todos sus datos: bases de datos, logs, etc.
working_dir = "./ares_data_vllm"
# Nivel de logging: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level = "INFO"

# ------------------------------------------------------------------------------
# SECCIÓN DEL SERVIDOR VLLM AUTOGESTIONADO
# ------------------------------------------------------------------------------
[vllm_server]
# El interruptor principal. Si es 'true', AresCore lanzará y gestionará el servidor.
enabled = true

# Modelo principal que servirá el motor vLLM.
# Este modelo se usará para todas las tareas (descripción, verificación, respuesta, etc.)
# a menos que se especifique lo contrario en el wrapper del reranker.
# Qwen3-0.6B es una excelente opción por su velocidad y calidad.
model = "Qwen/Qwen3-0.6B"

# Configuración de red para el servidor vLLM.
host = "127.0.0.1"
port = 8000

# Parámetros de rendimiento de vLLM.
dtype = "auto"                 # Permite a vLLM elegir el mejor tipo de dato (float16/bfloat16).
gpu_memory_utilization = 0.90  # Porcentaje de VRAM a usar para el KV cache.
max_num_seqs = 256             # Máximo de secuencias en un lote. Clave para la ingesta.
max_num_batched_tokens = 24576 # Tamaño máximo del lote en tokens. Clave para la ingesta.

# Campo flexible para cualquier otro argumento de vLLM.
# Ejemplo: si se usa un modelo AWQ: vllm_extra_args = { quantization = "awq" }
# Ejemplo: para tensor parallel: vllm_extra_args = { tensor-parallel-size = 2 }
# vllm_extra_args = {}

# ------------------------------------------------------------------------------
# CONFIGURACIÓN DE INGESTA
# ------------------------------------------------------------------------------
[ingestion]
# Estrategia de chunking. 'recursive' es un buen punto de partida.
chunking_strategy = "recursive" 
chonkie_config = { chunk_size = 400, chunk_overlap = 80, rules = "default" }

# Deshabilitado por ahora para enfocarnos en el rendimiento de la ingesta base.
# Cuando se active, usará el servidor vLLM principal.
generate_descriptions = false
use_verifier = false
update_kg = false
use_lsc = false
enrich_lsc_sections_with_llm = false
use_autocontent_headers_for_chunks = false

# Los wrappers de LLM para ingesta ahora apuntan al servidor vLLM gestionado.
description_llm_provider = "vllm_managed"
verifier_llm_provider = "vllm_managed"
section_enricher_llm_provider = "vllm_managed"

# ------------------------------------------------------------------------------
# EMBEDDINGS
# ------------------------------------------------------------------------------
[embeddings]
# Usamos un modelo de embedding local de alto rendimiento.
model_provider = "local"
model_name = "Qwen/Qwen3-Embedding-0.6B" # Modelo de embedding recomendado
local_options = { device = "cuda" } # "cpu" o "cuda"
late_chunking_enabled = true # Esencial para la arquitectura de ARES

# ------------------------------------------------------------------------------
# ALMACENAMIENTO
# ------------------------------------------------------------------------------
[storage]
provider = "sqlite"
path = "main_kv.db" # Relativo a 'working_dir'

[vector_db]
provider = "qdrant_local"
path = "vector_db" # Relativo a 'working_dir'

[knowledge_graph]
provider = "none" # Deshabilitado por ahora

[bm25_index]
provider = "whoosh" # Mantener como opcional. Si es "none", se desactiva.
path = "bm25_index"

# ------------------------------------------------------------------------------
# RECUPERACIÓN (RETRIEVAL)
# ------------------------------------------------------------------------------
[retrieval]
# El Query Planner puede usar el mismo servidor vLLM.
use_query_planner = true
planner_llm_provider = "vllm_managed"
query_planning_mode = "lite"

# Habilitar/deshabilitar fuentes de búsqueda.
use_description_search = false # Coherente con `generate_descriptions = false`
use_kg_search = false
use_rse = true # Habilitar Relevant Segment Extraction

# K iniciales para cada fuente de búsqueda.
vector_top_k = 150
bm25_top_k = 150
description_top_k = 10 # Bajo porque está deshabilitado

# Método para combinar resultados de diferentes fuentes.
combination_method = "rrf"
rrf_k = 60 # Parámetro para Reciprocal Rank Fusion

# Cuántos resultados pasar al siguiente paso (reranker o selector final).
intermediate_top_k = 200 

[retrieval.reranker]
enabled = true
# El modelo Qwen3 Reranker, que será manejado por nuestro QwenRerankerWrapper.
model = "Qwen/Qwen3-Reranker-0.6B"
# Este reranker también necesita un LLM. Le decimos que use nuestro servidor vLLM gestionado.
# El `model_name` aquí es el que se pasará a la API de vLLM. Puede ser el mismo
# que el servidor principal si es un modelo de propósito general.
[retrieval.reranker.llm_config]
model_provider = "vllm_managed"
model_name = "Qwen/Qwen3-0.6B"

# Estrategia de selección final para el contexto.
diversification_method = "mmr" 
mmr_lambda = 0.65 # Balance entre relevancia (1.0) y diversidad (0.0)
final_context_chunks = 7 # Número final de items de contexto para el LLM
context_token_budget = 7000 # Límite de tokens para el prompt final

# ------------------------------------------------------------------------------
# GENERACIÓN DE RESPUESTA
# ------------------------------------------------------------------------------
[responder_llm]
model_provider = "vllm_managed"
# El modelo que se solicitará en la API. Puede ser el mismo que el del servidor.
model_name = "Qwen/Qwen3-0.6B"
system_prompt_template = """Eres ARES, un asistente IA experto. Responde la consulta del usuario basándote estricta y únicamente en el 'Contexto Recuperado' proporcionado. Si la respuesta no se encuentra en el contexto, declara explícitamente: "La información no se encuentra en el contexto proporcionado." Sé conciso y directo.

Contexto Recuperado:
---
{retrieved_context}
---

Responde la siguiente consulta basándote solo en el contexto anterior:
"""
max_output_tokens = 1024
temperature = 0.4

[generation]
# Deshabilitado por ahora para mantener el flujo simple.
use_post_checker = false
post_checker_llm_provider = "vllm_managed"

# ------------------------------------------------------------------------------
# MANTENIMIENTO (Deshabilitado para pruebas de rendimiento)
# ------------------------------------------------------------------------------
[maintenance] 
summary_schedule = "none"
index_maintenance_schedule = "none"


---
File: /configs/default_config.txt
---

# ARES Configuration File (v2 - Local-First Defaults, No KG focus, TOML)

[general]
working_dir = "./ares_data" # Directorio para datos locales
log_level = "INFO"        # DEBUG, INFO, WARNING, ERROR, CRITICAL

[ingestion]
# --- Chunking (Chonkie) ---
chunking_strategy = "recursive" # Opciones: recursive, sentence, token
# Config para 'recursive': delimiters anidados y chunk_size/overlap de fallback
chonkie_config = { chunk_size = 300, chunk_overlap = 50, rules = "default" }
# Config para 'sentence': n_sentences o chunk_size/overlap
# chonkie_config = { n_sentences = 5 }
# Config para 'token': chunk_size/overlap
# chonkie_config = { chunk_size = 256, chunk_overlap = 30 }

# --- Late Chunking Embedding ---
# Config está en [embeddings]

# --- Description Generation ---
generate_descriptions = true # Generar resúmenes obj/subj, entidades, keywords
description_llm_config_path = "ingestion.description_llm_config" # Ruta a la config del LLM

[ingestion.description_llm_config]
model_provider = "local_ollama" # Usar Ollama por defecto
model_name = "llama3:8b"
local_options = { host_url = "http://localhost:11434" }
# api_options = { api_key = "YOUR_API_KEY" }

# --- Verification (Dual-Pass) ---
use_verifier = true # Habilitar verificación de descripciones
verifier_llm_config_path = "ingestion.verifier_llm_config" # Ruta a la config del LLM

[ingestion.verifier_llm_config]
# Usar modelo más rápido y barato para verificación
model_provider = "local_ollama"
model_name = "phi3:mini" # Ejemplo de modelo rápido
local_options = { host_url = "http://localhost:11434" }

# --- Knowledge Graph Update ---
update_kg = false # Deshabilitado por defecto
# kg_config_path = "knowledge_graph" # Ruta a la config del KG si se habilita

[embeddings]
# Embedder para Late Chunking y queries
model_provider = "local"
model_name = "jinaai/jina-embeddings-v3" # Jina V3 como default
local_options = { device = "cuda" } # "cpu" o "cuda"
# api_options = { api_key = "...", model_name = "text-embedding-3-small" } # Ejemplo OpenAI
late_chunking_enabled = true # Asegurar que Late Chunking se use

[storage]
# Almacén KV para Chunks, Descripciones, Caché
provider = "sqlite"
path = "kv_store.db" # Relativo a working_dir

[vector_db]
# VectorDB para embeddings de Chunks y Descripciones
provider = "qdrant_local"
path = "vector_db" # Relativo a working_dir

[knowledge_graph]
# KG (Opcional, deshabilitado por defecto)
provider = "none" # Opciones: "none", "pathrag_networkx"
path = "knowledge_graph.graphml" # Relativo a working_dir (si provider != none)

[bm25_index]
# Índice BM25 persistente para chunks
provider = "whoosh" # Única opción local por ahora
path = "bm25_index" # Relativo a working_dir

[retrieval]
# --- Planificación ---
use_query_planner = true # Habilitar planner que genera plan estructurado
planner_llm_config_path = "retrieval.planner_llm_config"

[retrieval.planner_llm_config]
# Usar modelo rápido para planificar
model_provider = "local_ollama"
model_name = "phi3:mini" # Ejemplo modelo rápido
local_options = { host_url = "http://localhost:11434" }

# --- Búsqueda ---
use_description_search = true # Habilitar búsqueda jerárquica en descripciones
use_kg_search = false         # Deshabilitar búsqueda KG por defecto

# Top K por fuente (usado en plan por defecto o si planner falla)
vector_top_k = 50
bm25_top_k = 50
description_top_k = 20
kg_top_k = 30

# --- Combinación ---
combination_method = "rrf"  # "rrf" o "weighted_sum"
rrf_k = 60                  # Parámetro K para RRF
# Pesos (usados si combination_method = "weighted_sum", o como hint para RRF/Planner)
vector_weight = 0.5
bm25_weight = 0.3
description_weight = 0.2 # Peso para resultados de descripciones
kg_weight = 0.0          # KG deshabilitado

# --- Refinamiento ---
intermediate_top_k = 100 # Cuántos resultados pasar a reranker/selector
reranker_config_path = "retrieval.reranker"

[retrieval.reranker]
enabled = true # Habilitar/deshabilitar reranker
model = "jinaai/jina-reranker-v2-base-multilingual"
device = "cuda" # "cpu" o "cuda"

# --- Selección Final ---
diversification_method = "mmr" # "mmr" o "top_k"
mmr_lambda = 0.6               # Balance relevancia (1) vs diversidad (0)
final_context_chunks = 7       # Número final de chunks para el LLM
context_token_budget = 3500    # Límite de tokens para el contexto final

# --- Lógica de Continuidad ---
include_neighbor_chunks = true # Habilitar lógica "sandwich"
neighbor_window_size = 1       # Cuántos chunks vecinos (+/-) buscar

[responder_llm]
# LLM para generar la respuesta final
model_provider = "local_ollama"
model_name = "llama3:8b"
local_options = { host_url = "http://localhost:11434", timeout = 120 }
# api_options = { api_key = "YOUR_API_KEY" }
system_prompt_template = """Eres ARES, un asistente IA conversacional con memoria persistente. Tu objetivo es responder la consulta del usuario de forma precisa y útil, basándote **principalmente** en el Contexto Recuperado de conversaciones/documentos anteriores. Si la información no está en el contexto, indica que no tienes esa información en tu memoria actual. Sé conciso.

Contexto Recuperado:
```
{retrieved_context}
```
-------
"""
# Si include_citations=true, añadir instrucción:
# IMPORTANTE: Si usas información del contexto, CITA tus fuentes usando [SourceID: ID_EXACTO_DEL_CHUNK] al final de la frase o párrafo. Ejemplo: La reunión fue productiva [SourceID: cnk-win1-3-a4b8c1d2].

max_output_tokens = 768
temperature = 0.6
# top_p = 0.9 # Ejemplo de otro parámetro LLM

[generation]
include_citations = false # Habilitar/deshabilitar inclusión de SourceID en prompt
use_post_checker = false # Habilitar/deshabilitar verificación final
post_checker_llm_config_path = "generation.post_checker_llm_config"

[generation.post_checker_llm_config]
# Usar modelo rápido, puede ser el mismo que verifier/planner
model_provider = "local_ollama"
model_name = "phi3:mini"
local_options = { host_url = "http://localhost:11434" }

[maintenance]
# --- Resúmenes Periódicos ---
summary_schedule = "daily" # "daily", "weekly", "monthly", "none"
summary_llm_config_path = "maintenance.summary_llm_config"
summary_retention_days = 90 # Cuántos días de resúmenes mantener

[maintenance.summary_llm_config]
model_provider = "local_ollama"
model_name = "llama3:8b"
local_options = { host_url = "http://localhost:11434" }

# --- Optimización de Índices ---
index_maintenance_schedule = "weekly" # "daily", "weekly", "monthly", "none"

# --- Limpieza de Caché ---
cache_retention_days = 30 # Eliminar entradas de caché LLM más antiguas que esto


---
File: /configs/lc_default_config.txt
---

# ares/configs/default_config.toml (late chunking toml)

[general]
working_dir = "./ares_data_lc_test" # Directorio DIFERENTE para no mezclar con pruebas anteriores
log_level = "INFO"

[ingestion]
chunking_strategy = "recursive"
chonkie_config = { chunk_size = 300, chunk_overlap = 50, rules = "default" }

# --- DESHABILITAR GENERACIÓN DE DESCRIPCIONES Y VERIFICACIÓN ---
generate_descriptions = false
# description_llm_config_path = "ingestion.description_llm_config" # No necesario si está false

use_verifier = false
# verifier_llm_config_path = "ingestion.verifier_llm_config" # No necesario si está false

update_kg = false # Mantener deshabilitado

# --- Configuración LLM para descripciones (no se usará, pero debe estar para evitar error si se lee la ruta) ---
# Puedes dejar estas secciones o comentarlas, pero generate_descriptions=false es la clave.
[ingestion.description_llm_config]
model_provider = "none" # O un LLM válido, pero no se usará
model_name = "none"

[ingestion.verifier_llm_config]
model_provider = "none" # O un LLM válido, pero no se usará
model_name = "none"


[embeddings]
model_provider = "local"
model_name = "jinaai/jina-embeddings-v3" # Asegúrate de que sea un modelo que soporte Late Chunking bien
local_options = { device = "cuda" } # O "cpu"
late_chunking_enabled = true # ¡IMPORTANTE!

[storage]
provider = "sqlite"
path = "kv_store.db"

[vector_db]
provider = "qdrant_local"
path = "vector_db"

[knowledge_graph]
provider = "none" # Deshabilitado
path = "knowledge_graph.graphml"

[bm25_index]
provider = "whoosh" # Lo mantendremos para una búsqueda híbrida simple
path = "bm25_index"

[retrieval]
# --- DESHABILITAR QUERY PLANNER ---
use_query_planner = false
# planner_llm_config_path = "retrieval.planner_llm_config" # No necesario

# --- DESHABILITAR BÚSQUEDA EN DESCRIPCIONES Y KG ---
use_description_search = false
use_kg_search = false

# Top K por fuente (usado en plan por defecto)
vector_top_k = 50  # Recuperar suficientes para que el reranker tenga material
bm25_top_k = 50

# --- Combinación ---
combination_method = "rrf"
rrf_k = 60
vector_weight = 0.7 # Dar más peso a la búsqueda vectorial (Late Chunking)
bm25_weight = 0.3
description_weight = 0.0 # Poner a cero ya que está deshabilitada
kg_weight = 0.0

# --- Refinamiento ---
intermediate_top_k = 100
[retrieval.reranker]
enabled = true # Mantener reranker si quieres probar su efecto, o false para más simple
model = "jinaai/jina-reranker-v2-base-multilingual"
device = "cuda" # o "cpu"

# --- Selección Final ---
diversification_method = "top_k" # Simplificar a Top-K por ahora, o mantener MMR si prefieres
# mmr_lambda = 0.6
final_context_chunks = 5 # Ajustar según necesidad
context_token_budget = 3500

# --- Lógica de Continuidad ---
include_neighbor_chunks = true # Probar con y sin esto para ver el efecto

[responder_llm]
model_provider = "local_ollama"
model_name = "llama3:8b"
local_options = { host_url = "http://localhost:11434", timeout = 120 }
system_prompt_template = """Eres ARES, un asistente IA. Se te proporciona un 'Contexto Recuperado' que contiene fragmentos de conversaciones o documentos relevantes para la consulta del usuario. Tu tarea es responder la consulta utilizando ÚNICAMENTE la información encontrada en el 'Contexto Recuperado'. No inventes información ni utilices conocimiento externo. Si la respuesta no se encuentra en el contexto, debes decirlo explícitamente. Sé conciso.

Contexto Recuperado:
--- INICIO DEL CONTEXTO ---
{retrieved_context}
--- FIN DEL CONTEXTO ---

Ahora, responde la siguiente consulta del usuario basándote en el contexto anterior:
"""
max_output_tokens = 512
temperature = 0.5

[generation]
include_citations = false # Simplificar, no necesitamos citar descripciones si no las hay
use_post_checker = false # Deshabilitar
# post_checker_llm_config_path = "generation.post_checker_llm_config"

[generation.post_checker_llm_config]
model_provider = "none"
model_name = "none"

[maintenance]
summary_schedule = "none" # Deshabilitar
# summary_llm_config_path = "maintenance.summary_llm_config"
summary_retention_days = 90

[maintenance.summary_llm_config]
model_provider = "none"
model_name = "none"

index_maintenance_schedule = "none"
cache_retention_days = 30


---
File: /core/__init__.py
---




---
File: /core/config_loader.py
---

import toml
import os
from typing import Dict, Any
from typing import Optional

DEFAULT_CONFIG_PATH = os.path.join(os.path.dirname(__file__), "..", "configs", "default_config.toml")

def load_config(config_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Carga la configuración desde un archivo TOML.
    Si no se proporciona una ruta, carga la configuración por defecto.
    Realiza validaciones básicas.
    """
    if config_path is None:
        config_path = DEFAULT_CONFIG_PATH
        print(f"INFO: No config path provided. Loading default config from: {config_path}")
    elif not os.path.exists(config_path):
        raise FileNotFoundError(f"Config file not found at {config_path}")

    try:
        config = toml.load(config_path)
        print(f"INFO: Configuration loaded successfully from {config_path}")
        _validate_config(config)
        return config
    except toml.TomlDecodeError as e:
        raise ValueError(f"Error decoding TOML file {config_path}: {e}")
    except Exception as e:
        raise RuntimeError(f"An unexpected error occurred while loading config: {e}")

def _validate_config(config: Dict[str, Any]):
    """Realiza validaciones básicas en la configuración cargada."""
    required_sections = [
        "general", "ingestion", "embeddings", 
        # "description_llm", # Temporarily commented out for testing
        "vector_db", "storage", "knowledge_graph", "retrieval",
        "responder_llm", "maintenance" 
    ]
    for section in required_sections:
        if section not in config:
            # Podríamos hacer esto un warning si queremos que sea opcional
            logger.warning(f"Sección de configuración requerida no encontrada: [{section}]. Usando valores por defecto o deshabilitando funcionalidad.")
            # O mantener el error si consideramos estas secciones *absolutamente* esenciales
            # raise ValueError(f"Missing required configuration section: [{section}]")

    # Validaciones más específicas (ejemplos)
    if "working_dir" not in config["general"]:
        raise ValueError("Missing 'working_dir' in [general] section.")

    if config["embeddings"]["model_provider"] == "local" and "local_options" not in config["embeddings"]:
         print("WARN: 'local_options' not found in [embeddings] for local provider. Using defaults.")
         config["embeddings"]["local_options"] = {} # Asegurar que exista

    if config["embeddings"].get("late_chunking_enabled") is not True:
         print("WARN: 'late_chunking_enabled' is not true in [embeddings]. Late chunking will be disabled.")

    if config["storage"]["provider"] == "sqlite" and "path" not in config["storage"]:
         raise ValueError("Missing 'path' in [storage] section for sqlite provider.")

    print("INFO: Basic configuration validation passed.")
    # Validaciones de dependencia lógica
    if get_config_value(config, "retrieval.use_kg_search", False) and \
       get_config_value(config, "knowledge_graph.provider", "none") == "none":
        raise ValueError("Config error: 'retrieval.use_kg_search' es true, pero no se ha configurado un 'knowledge_graph.provider'.")

    if get_config_value(config, "ingestion.update_kg", False) and \
       get_config_value(config, "knowledge_graph.provider", "none") == "none":
        raise ValueError("Config error: 'ingestion.update_kg' es true, pero no se ha configurado un 'knowledge_graph.provider'.")

    # --- VALIDACIÓN PROBLEMÁTICA ---
    generate_descriptions_enabled = get_config_value(config, "ingestion.generate_descriptions", False)
    # Corregir la ruta para acceder a model_provider del LLM de descripción:
    description_llm_provider = get_config_value(config, "ingestion.description_llm_config.model_provider", "none") # <--- CORRECCIÓN AQUÍ

    if generate_descriptions_enabled and description_llm_provider == "none":
        # Cambiar de ValueError a un warning, ya que AresCore intentará deshabilitar
        # la generación de descripciones si el LLM no se puede cargar.
        # Si realmente quieres un error aquí, puedes mantener el raise ValueError.
        logger.warning("Config warning: 'ingestion.generate_descriptions' es true, pero no se ha configurado un 'ingestion.description_llm_config.model_provider'. La generación de descripciones podría no funcionar.")
        # raise ValueError("Config error: 'ingestion.generate_descriptions' es true, pero no se ha configurado un 'ingestion.description_llm_config.model_provider'.")

    if get_config_value(config, "retrieval.use_query_planner", False) and \
       get_config_value(config, "retrieval.planner_llm_config.model_provider", "none") == "none":
        # Similarmente, esto podría ser un warning, ya que AresCore tiene un plan por defecto.
        logger.warning("Config warning: 'retrieval.use_query_planner' es true, pero no se ha configurado un 'retrieval.planner_llm_config.model_provider'. El Query Planner podría no funcionar.")
        # raise ValueError("Config error: 'retrieval.use_query_planner' es true, pero no se ha configurado un 'retrieval.planner_llm_config.model_provider'.")

    # Añadir más validaciones según sea necesario...
    print("INFO: Validación de configuración lógica completada.")

# Ejemplo de cómo obtener una configuración específica anidada de forma segura
def get_config_value(config: Dict, key_path: str, default: Any = None) -> Any:
    """
    Obtiene un valor de la configuración usando una ruta de claves separadas por puntos.
    Ejemplo: get_config_value(config, "retrieval.planner_llm_config.model_name")
    """
    keys = key_path.split('.')
    value = config
    try:
        for key in keys:
            value = value[key]
        return value
    except (KeyError, TypeError):
        return default
    
# Importar logger si se usa logger.warning arriba
try:
    from ..utils.logging_setup import logger
except ImportError:
    # Fallback si la importación relativa falla (ej. al ejecutar config_loader.py directamente)
    import logging
    logger = logging.getLogger(__name__)
    if not logger.hasHandlers():
        logger.addHandler(logging.NullHandler())


---
File: /core/datatypes.py
---

# --- START OF MODIFIED FILE ares/core/datatypes.py ---
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Literal, Union, Sequence # Asegurar Sequence
import numpy as np

@dataclass
class Chunk:
    id: str
    text: str  # Texto original del chunk, sin cabecera AutoContext
    embedding: Optional[np.ndarray] = field(repr=False, default=None)
    metadata: Dict[str, Any] = field(default_factory=dict)
    # Nuevos campos en metadata para AutoContext (se llenarán durante la ingesta):
    # metadata["window_id"]: str
    # metadata["doc_title_for_autocont"]: Optional[str]
    # metadata["doc_objective_summary_for_autocont"]: Optional[str]
    # metadata["lsc_section_id"]: Optional[str] (ID de la Late Sectioning Context Section)
    # metadata["lsc_section_title"]: Optional[str]
    # metadata["lsc_section_objective_summary"]: Optional[str]
    # metadata["lsc_section_subjective_interpretation"]: Optional[str]
    # metadata["chunk_index_in_lsc_section"]: Optional[int]
    token_count: Optional[int] = None


@dataclass
class Description: # Descripción de Ventana Completa
    window_id: str
    objective_summary: str
    subjective_interpretation: Optional[str] = None # Mantener para la descripción de ventana
    entities: List[Dict[str, Any]] = field(default_factory=list)
    relations: List[Dict[str, Any]] = field(default_factory=list)
    keywords: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict) # Aquí puede ir el título de la ventana si se genera
    is_verified: bool = False

@dataclass
class LSCSection: # Late Sectioning Context Section
    id: str # e.g., window_id + "-lsc-" + section_index
    window_id: str
    text: str # Texto original de esta sección LSC
    title: Optional[str] = None # Generado por LLM para esta sección LSC
    objective_summary: Optional[str] = None # Generado por LLM para esta sección LSC
    subjective_interpretation: Optional[str] = None # Generado por LLM para esta sección LSC (opcional)
    # Metadata adicional de la sección LSC si es necesario (ej. límites de char originales)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class SubQuery:
    id: str # ej. "sq1"
    text: str # Texto de la sub-consulta

@dataclass
class SearchOperation:
    op_type: Literal["vector", "bm25", "kg_path_search", "description"]
    parameters: Dict[str, Any]
    weight: float = 1.0
    sub_query_id: Optional[str] = None # Para identificar a qué sub-consulta pertenece (en modo iterativo)

@dataclass
class CombineOperation:
    op_type: Literal["combine"]
    method: Literal["rrf", "weighted_sum"] = "rrf"
    parameters: Dict[str, Any] = field(default_factory=dict)

@dataclass
class SearchPlan:
    original_query: str
    # Para Query Planning modo "Lite" o la salida inicial del modo "Iterative"
    sub_queries: Optional[List[SubQuery]] = None
    # Para Query Planning modo "Lite", estas son las queries consolidadas.
    # Para modo "Iterative", estas podrían ser las queries de la sub-consulta actual.
    query_for_embedding: Optional[str] = None
    query_for_bm25: Optional[str] = None
    # El plan de operaciones a ejecutar con las queries anteriores.
    steps: List[Union[SearchOperation, CombineOperation]] = field(default_factory=list)
    # Para modo "Iterative", para saber si el plan está completo o necesita refinamiento.
    status: Optional[Literal["pending_execution", "needs_refinement", "final_answer_ready", "complete_no_answer"]] = None
    # Para modo "Iterative", la respuesta final si el planner la genera.
    final_answer_from_planner: Optional[str] = None
    # Para modo "Iterative", el contexto acumulado de sub-consultas previas.
    accumulated_context: Optional[str] = None


@dataclass
class RetrievedItem:
    # El 'chunk' puede ser un Chunk pequeño original, o un Chunk representando un segmento RSE.
    # Si es un segmento RSE, chunk.text es el texto concatenado del segmento,
    # y chunk.metadata contiene info sobre los chunks originales.
    chunk: Chunk
    score: float # Score de reranker, o de RSE, o de búsqueda inicial si no hay pasos intermedios
    source: str # "vector_chunk", "bm25_chunk", "description_window", "rse_segment", "combined", etc.
    metadata: Dict[str, Any] = field(default_factory=dict) # Para info de recuperación como retrieval_sources
    normalized_embedding: Optional[np.ndarray] = field(repr=False, default=None) # Para MMR

@dataclass
class FinalContext:
    # Lista de items finales (pueden ser Chunks originales o Chunks representando segmentos RSE)
    # El texto de cada chunk en esta lista ya DEBERÍA tener el AutoContext Header prepended por PromptBuilder.
    selected_items: List[RetrievedItem]
    # La cadena de texto completa construida por PromptBuilder, lista para el LLM.
    context_string: str
# --- END OF MODIFIED FILE ares/core/datatypes.py ---


---
File: /core/orchestrator.py
---

# ares/core/orchestrator.py
import asyncio
import os
import time
from typing import Dict, Any, List, Optional, Type, Tuple, Union, Literal
import numpy as np
from transformers import AutoTokenizer # <<< NUEVO: Importar para Reranker

# --- Dependencias del Proyecto ARES ---

# Configuración y Tipos de Datos
from .config_loader import load_config, get_config_value
from .datatypes import (
    Chunk, SearchPlan, RetrievedItem, FinalContext,
    SearchOperation, CombineOperation, Description, SubQuery, LSCSection
)

# Almacenamiento
from ..storage.base import BaseKVStore, BaseVectorDB, BaseKnowledgeGraph
from ..storage.kv_store.sqlite import SQLiteKVStore
from ..storage.vector_db.qdrant_local import QdrantLocalVectorDB
from ..storage.knowledge_graph.pathrag_networkx import PathRAGLogicKnowledgeGraph
# from ..storage.bm25.fastembed_bm25 import FastEmbedBM25 # <<< ELIMINADO

# LLMs
from ..llm.base import BaseLLMWrapper
from ..llm.vllm_api import VLLMWrapper  # <<< NUEVO: Importar el wrapper de API

# Componentes de Ingesta
from ..ingestion.pipeline import IngestionPipeline
from ..ingestion.embedder import LateChunkingEmbedder
from ..ingestion.verifier import Verifier
from ..ingestion.description_generator import DescriptionGenerator
from ..ingestion.embedder_sparse import FastEmbedSparseEmbedder # <<< NUEVO

# Componentes de Recuperación
from ..retrieval.planner import QueryPlanner
from ..retrieval.manager import RetrievalManager
from ..retrieval.reranker_qwen import QwenRerankerWrapper # <<< NUEVO: Importar el reranker específico
from ..retrieval.base import BaseRerankerWrapper # <<< NUEVO: Importar la interfaz base
from ..retrieval.selector import select_diverse_chunks_mmr
from ..retrieval.rse import RelevantSegmentExtractor

# Componentes de Generación
from ..generation.prompt_builder import PromptBuilder
from ..generation.llm_responder import LLMResponder
from ..generation.post_checker import PostChecker

# Utilidades
from ..utils.logging_setup import setup_logging, logger
from ..utils.helpers import normalize_vector

# Servidor vLLM
from ..vllm_server.manager import VLLMServerManager # <<< NUEVO: Importar el gestor

# Mapeos de Proveedores (Simplificados para la nueva arquitectura)
KVSTORE_PROVIDERS = {"sqlite": SQLiteKVStore}
VECTORDB_PROVIDERS = {"qdrant_local": QdrantLocalVectorDB}
KG_PROVIDERS = {"pathrag_networkx": PathRAGLogicKnowledgeGraph, "none": None}
# BM25_PROVIDERS = {"whoosh": WhooshBM25Index} # <<< ELIMINADO
# En esta nueva arquitectura, el provider principal será 'vllm_managed'
LLM_PROVIDERS = {"vllm_managed": VLLMWrapper}


class AresCore:
    # --- Atributos con Type Hints ---
    config: Dict[str, Any]
    # Almacenamiento
    chunk_store: BaseKVStore
    description_store: BaseKVStore
    llm_cache_store: BaseKVStore
    vector_db: BaseVectorDB
    kg_graph: Optional[BaseKnowledgeGraph]
    sparse_embedder: Optional[FastEmbedSparseEmbedder]
    # Embedder
    embedder: LateChunkingEmbedder
    # LLMs
    planner_llm: Optional[BaseLLMWrapper]
    description_llm: Optional[BaseLLMWrapper]
    verifier_llm: Optional[BaseLLMWrapper]
    section_enricher_llm: Optional[BaseLLMWrapper]
    responder_llm: BaseLLMWrapper
    post_checker_llm: Optional[BaseLLMWrapper]

    # <<< CAMBIO: Atributo para el gestor del servidor vLLM >>>
    vllm_server_manager: Optional[VLLMServerManager]

    # Componentes Principales
    ingestion_pipeline: IngestionPipeline
    query_planner: Optional[QueryPlanner]
    retrieval_manager: RetrievalManager
    rse: Optional[RelevantSegmentExtractor]
    reranker: Optional[BaseRerankerWrapper] # <<< CAMBIO: Usar la interfaz base
    prompt_builder: PromptBuilder
    responder: LLMResponder
    post_checker: Optional[PostChecker]
    verifier: Optional[Verifier]

    # Parámetros derivados de config
    retrieval_cfg: Dict[str, Any]
    query_planning_mode: Literal["lite", "iterative"]
    selector_lambda: float
    selector_k: int
    include_citations_flag: bool
    context_token_budget: int
    iterative_planner_max_steps: int
    
    use_kg_ingest: bool
    use_kg_retrieval: bool
    use_desc_gen: bool
    use_verifier: bool
    use_post_checker: bool
    use_query_planner: bool
    use_rse: bool

    # K iniciales para recuperación
    vector_initial_k: int
    bm25_initial_k: int
    description_initial_k: int


    def __init__(self, config_path: Optional[str] = None):
        self.config = load_config(config_path)
        self._setup_logging()
        try:
            # <<< CAMBIO: Inicializar el gestor del servidor PRIMERO >>>
            self.vllm_server_manager = VLLMServerManager(self.config)

            self._setup_components()
            self._load_derived_configs()
            logger.info("AresCore inicializado exitosamente.")
        except Exception as e:
            logger.exception("Error fatal durante la inicialización de AresCore.")
            raise

    def _setup_logging(self):
        log_level = get_config_value(self.config, "general.log_level", "INFO")
        working_dir = get_config_value(self.config, "general.working_dir", "./ares_data")
        log_file = os.path.join(working_dir, "ares.log")
        setup_logging(log_level_str=log_level, log_file=log_file)

    def _get_storage_path(self, relative_path: str) -> str:
        working_dir = self.config["general"]["working_dir"]
        os.makedirs(working_dir, exist_ok=True)
        full_path = os.path.join(working_dir, relative_path)
        dir_name = os.path.dirname(full_path)
        if dir_name: os.makedirs(dir_name, exist_ok=True)
        return full_path

    # <<< CAMBIO: MÉTODO REFACTORIZADO Y ALINEADO CON LA NUEVA CONFIGURACIÓN TOML >>>
    def _get_llm_wrapper(self, config_section_path: str, purpose: str, is_optional: bool = False) -> Optional[BaseLLMWrapper]:
        """Obtiene un wrapper de LLM a partir de una ruta de configuración."""
        llm_config = get_config_value(self.config, config_section_path)
        
        if not llm_config:
            if not is_optional: raise ValueError(f"Sección de config requerida '{config_section_path}' no encontrada.")
            return None

        provider = llm_config.get("model_provider")
        model_name_for_api = llm_config.get("model_name")

        if not provider or provider == "none" or not model_name_for_api:
            if not is_optional: raise ValueError(f"LLM para '{purpose}' es requerido pero no está configurado (provider/model_name).")
            return None

        if provider == "vllm_managed":
            return VLLMWrapper(ares_config=self.config, model_identifier_for_api=model_name_for_api)
        
        logger.error(f"Proveedor de LLM no soportado '{provider}'.")
        return None

    # <<< CAMBIO: MÉTODO ACTUALIZADO PARA LA NUEVA LÓGICA DE INICIALIZACIÓN DE LLM >>>
    def _setup_components(self):
        """
        Inicializa todos los componentes de ARES.
        Modificado para instanciar el QwenRerankerWrapper y pasarle sus dependencias.
        """
        logger.info("Configurando componentes de ARES...")

        # --- Embedder Denso ---
        embed_cfg = self.config.get("embeddings", {})
        self.embedder = LateChunkingEmbedder(
            model_provider=get_config_value(embed_cfg, 'model_provider', 'local'),
            model_name=get_config_value(embed_cfg, 'model_name', 'jinaai/jina-embeddings-v3'),
            local_options=get_config_value(embed_cfg, 'local_options', {}),
            api_options=get_config_value(embed_cfg, 'api_options', {})
        )
        try:
            embedding_dim = self.embedder.model.config.hidden_size
        except Exception:
            embedding_dim = get_config_value(embed_cfg, 'embedding_dim', 768)
            logger.warning(f"No se pudo determinar la dimensión del embedding desde el modelo. Usando valor de config o por defecto: {embedding_dim}")

        # --- NUEVA INICIALIZACIÓN DEL EMBEDDER DISPERSO ---
        sparse_cfg = get_config_value(self.config, "ingestion.sparse_embedder_config", {})
        self.sparse_embedder = None
        if sparse_cfg.get("enabled", True): # Habilitado por defecto si la sección existe
            try:
                model_name=sparse_cfg.get("model_name", "Qdrant/bm25")
                self.sparse_embedder = FastEmbedSparseEmbedder(
                    model_name=model_name,
                    **sparse_cfg.get("local_options", {})
                )
                logger.info(f"FastEmbedSparseEmbedder inicializado con el modelo '{model_name}'.")
            except Exception as e:
                logger.error(f"No se pudo inicializar FastEmbedSparseEmbedder. Búsqueda léxica deshabilitada. Error: {e}")
                self.sparse_embedder = None

        # --- Storage ---
        storage_cfg = self.config.get("storage", {})
        kv_provider_cls = KVSTORE_PROVIDERS.get(get_config_value(storage_cfg, "provider", "sqlite"))
        self.chunk_store = kv_provider_cls(db_path=self._get_storage_path(get_config_value(storage_cfg, "path", "kv_store.db")))
        self.description_store = kv_provider_cls(db_path=self._get_storage_path("descriptions.db"))
        self.llm_cache_store = kv_provider_cls(db_path=self._get_storage_path("llm_cache.db"))
        vdb_cfg = self.config.get("vector_db", {})
        vdb_provider_cls = VECTORDB_PROVIDERS.get(get_config_value(vdb_cfg, "provider", "qdrant_local"))
        self.vector_db = vdb_provider_cls(collection_name="ares_vectors", path=self._get_storage_path(get_config_value(vdb_cfg, "path", "vector_db")), embedding_dim=embedding_dim)
        kg_cfg = self.config.get("knowledge_graph", {})
        kg_provider_cls = KG_PROVIDERS.get(get_config_value(kg_cfg, "provider", "none"))
        self.kg_graph = kg_provider_cls(graph_path=self._get_storage_path(get_config_value(kg_cfg, "path", "kg.graphml"))) if kg_provider_cls else None
        
        # --- ELIMINADA LA INICIALIZACIÓN DE BM25_INDEX ---
        logger.info("Componentes de Storage inicializados.")

        # --- LLM Wrappers ---
        self.responder_llm = self._get_llm_wrapper("responder_llm", "Responder")
        self.planner_llm = self._get_llm_wrapper("retrieval.planner_llm_config", "Query Planner", is_optional=True)
        self.description_llm = self._get_llm_wrapper("ingestion.description_llm_config", "Description Gen", is_optional=True)
        self.verifier_llm = self._get_llm_wrapper("ingestion.verifier_llm_config", "Verifier", is_optional=True)
        self.section_enricher_llm = self._get_llm_wrapper("ingestion.section_enricher_llm_config", "LSC Enricher", is_optional=True)
        self.post_checker_llm = self._get_llm_wrapper("generation.post_checker_llm_config", "Post-Checker", is_optional=True)
        
        # --- Reranker ---
        reranker_cfg = get_config_value(self.config, "retrieval.reranker", {})
        self.reranker = None
        if reranker_cfg.get("enabled"):
            reranker_model_name = reranker_cfg.get("model")
            if "qwen" in reranker_model_name.lower():
                reranker_llm = self._get_llm_wrapper("retrieval.reranker.llm_config", "Reranker", is_optional=True)
                if reranker_llm:
                    tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)
                    self.reranker = QwenRerankerWrapper(llm_wrapper=reranker_llm, tokenizer=tokenizer)

        # --- Componentes Principales ---
        self.use_desc_gen = get_config_value(self.config, "ingestion.generate_descriptions", True) and bool(self.description_llm)
        self.use_verifier = get_config_value(self.config, "ingestion.use_verifier", True) and bool(self.verifier_llm) and self.use_desc_gen
        self.verifier = Verifier(llm_wrapper=self.verifier_llm, config=self.config) if self.use_verifier else None

        self.ingestion_pipeline = IngestionPipeline(
            config=self.config,
            chunk_store=self.chunk_store, vector_db=self.vector_db, description_store=self.description_store,
            kg_graph=self.kg_graph, 
            bm25_index=None, # Ya no se usa
            embedder=self.embedder,
            sparse_embedder=self.sparse_embedder, # <<< PASANDO NUEVA INSTANCIA
            description_llm=self.description_llm,
            verifier=self.verifier,
            section_enricher_llm=self.section_enricher_llm
        )
        
        self.use_query_planner = get_config_value(self.config, "retrieval.use_query_planner", True) and bool(self.planner_llm)
        self.query_planner = QueryPlanner(llm_wrapper=self.planner_llm, config=self.config) if self.use_query_planner else None
        
        self.retrieval_manager = RetrievalManager(
            config=self.config, chunk_store=self.chunk_store,
            description_store=self.description_store, vector_db=self.vector_db,
            kg_graph=self.kg_graph, 
            bm25_index=None, # Ya no se usa
            embedder=self.embedder,
            sparse_embedder_for_query=self.sparse_embedder # <<< PASANDO NUEVA INSTANCIA
        )

        self.use_rse = get_config_value(self.config, "retrieval.use_rse", False)
        self.rse = RelevantSegmentExtractor(config=self.config, chunk_store=self.chunk_store) if self.use_rse else None

        self.prompt_builder = PromptBuilder(system_prompt_template=get_config_value(self.config, "responder_llm.system_prompt_template", ""))
        self.responder = LLMResponder(llm_wrapper=self.responder_llm)
        self.use_post_checker = get_config_value(self.config, "generation.use_post_checker", False) and bool(self.post_checker_llm)
        self.post_checker = PostChecker(llm_wrapper=self.post_checker_llm, enabled=self.use_post_checker) if self.post_checker_llm else PostChecker(llm_wrapper=None, enabled=False)

        logger.info("Configuración de componentes principales completada.")

    # <<< NUEVO MÉTODO PARA INICIAR SERVICIOS >>>
    async def start_services(self):
        """
        Inicia servicios de fondo necesarios, como el servidor vLLM.
        Debe ser llamado después de __init__ y antes de cualquier operación.
        """
        logger.info("Iniciando servicios de fondo de ARES...")
        if self.vllm_server_manager and get_config_value(self.config, "vllm_server.enabled", False):
            await self.vllm_server_manager.start_server()
        else:
            logger.info("Gestión automática de vLLM deshabilitada, se asume que el servidor se ejecuta externamente.")

    # <<< MÉTODO DE CIERRE MODIFICADO >>>
    async def close(self):
        """
        Cierra ARES y todos sus componentes, incluido el servidor vLLM si fue gestionado.
        """
        logger.info("Iniciando secuencia de cierre de ARES Core...")
        # 1. Detener el servidor vLLM primero para que no acepte más peticiones.
        if self.vllm_server_manager:
            await self.vllm_server_manager.stop_server()
        
        # 2. Cerrar/guardar otros componentes (bases de datos, etc.)
        await self._query_done_callback()

        # 3. Cerrar wrappers si tienen un método close
        llms = [self.planner_llm, self.description_llm, self.verifier_llm, self.section_enricher_llm, self.responder_llm, self.post_checker_llm]
        for llm_wrapper in llms:
             if llm_wrapper and hasattr(llm_wrapper, 'close'):
                  try:
                       await llm_wrapper.close()
                  except Exception as e: 
                       logger.warning(f"Error cerrando LLM {type(llm_wrapper).__name__}: {e}")
        
        logger.info("ARES Core cerrado limpiamente.")

    def _load_derived_configs(self):
        self.retrieval_cfg = get_config_value(self.config, "retrieval", {})
        self.query_planning_mode = self.retrieval_cfg.get("query_planning_mode", "lite")
        self.selector_lambda = float(self.retrieval_cfg.get("mmr_lambda", 0.6))
        self.selector_k = int(self.retrieval_cfg.get("final_context_chunks", 7))
        self.include_citations_flag = get_config_value(self.config, "generation.include_citations", False)
        self.context_token_budget = int(self.retrieval_cfg.get("context_token_budget", 3500))
        self.iterative_planner_max_steps = int(self.retrieval_cfg.get("iterative_planner_max_steps", 3))
        default_k = 75 if self.use_rse else 50
        self.vector_initial_k = int(self.retrieval_cfg.get("vector_top_k", default_k))
        self.bm25_initial_k = int(self.retrieval_cfg.get("bm25_top_k", default_k))
        self.description_initial_k = int(self.retrieval_cfg.get("description_top_k", max(10, default_k // 2)))

    async def aingest(self, window_id: str, conversation_text: str, metadata: Dict[str, Any] = {}) -> bool:
        success = await self.ingestion_pipeline.process_window(window_id, conversation_text, metadata)
        logger.info(f"Ingesta para ventana {window_id} {'exitosa' if success else 'fallida'}.")
        return success

    async def _build_context_string_with_headers(self, items: List[RetrievedItem]) -> str:
        """Construye la cadena de contexto final, reconstruyendo AutoContext headers si es necesario."""
        formatted_texts_for_prompt_builder: List[Tuple[str, str]] = [] 

        for item in items:
            header_parts = []
            meta = item.chunk.metadata
            if meta.get("doc_title_for_autocont"): header_parts.append(f"Título del Documento: {meta['doc_title_for_autocont']}")
            if meta.get("doc_objective_summary_for_autocont"): header_parts.append(f"Resumen del Documento: {meta['doc_objective_summary_for_autocont']}")
            if meta.get("lsc_section_title"): header_parts.append(f"Título de Sección LSC: {meta['lsc_section_title']}")
            if meta.get("lsc_section_objective_summary"): header_parts.append(f"Resumen de Sección LSC: {meta['lsc_section_objective_summary']}")
            
            full_header = ""
            if header_parts:
                full_header = "Contexto General: " + ". ".join(header_parts) + ". Contenido Específico: "
            
            text_with_header = full_header + item.chunk.text
            
            citation_id = item.chunk.id 
            if item.source == "rse_segment" and meta.get("rse_original_chunk_ids"):
                original_ids_str = ','.join(meta['rse_original_chunk_ids'][:2])
                citation_id = f"segmento:{item.chunk.id}(chunks:{original_ids_str}{'...' if len(meta['rse_original_chunk_ids']) > 2 else ''})"

            formatted_texts_for_prompt_builder.append((text_with_header, citation_id))
        
        context_string = ""
        current_tokens = 0
        chunks_included_count = 0

        try:
            tokenizer = self.embedder.tokenizer
        except AttributeError:
            logger.warning("Embedder no tiene un atributo 'tokenizer'. El conteo de tokens para el contexto puede ser impreciso.")
            tokenizer = lambda text, add_special_tokens: text.split()

        for text_content, _cit_id in formatted_texts_for_prompt_builder:
            try:
                chunk_tokens = len(tokenizer.encode(text_content, add_special_tokens=False))
            except Exception as e_tok:
                logger.warning(f"Error al tokenizar contenido para conteo de budget: {e_tok}. Usando len(text)/4 como proxy.")
                chunk_tokens = len(text_content) // 4

            if current_tokens + chunk_tokens <= self.context_token_budget:
                context_string += text_content + "\n\n------\n\n"
                current_tokens += chunk_tokens + 2 
                chunks_included_count += 1
            else:
                logger.warning(f"Contexto truncado. Límite de {self.context_token_budget} tokens. {chunks_included_count}/{len(items)} items/segmentos incluidos.")
                break
        
        logger.debug(f"Contexto final (con headers reconstruidos) para LLM con {current_tokens} tokens y {chunks_included_count} items/segmentos.")
        return context_string.strip()


    async def aquery(self,
                     query: str,
                     recent_history: Optional[List[Dict[str, str]]] = None,
                     use_post_checker_override: Optional[bool] = None) -> str:
        
        # --- INICIO DE CAMBIOS: Métricas de Rendimiento ---
        query_start_time = time.perf_counter()
        timings = {} # Diccionario para almacenar las duraciones
        # --- FIN DE CAMBIOS ---
        
        final_response = "Lo siento, ocurrió un error inesperado procesando tu consulta."
        current_plan: Optional[SearchPlan] = None
        accumulated_results_text_iterative = "" 
        executed_sub_queries_log_iterative: List[Tuple[str, str]] = [] 
        
        if recent_history is None: recent_history = []

        try:
            # --- 1. Planificación (incluye bucle iterativo si está activo) ---
            # Esta fase determina qué información se necesita y la obtiene, ya sea
            # de forma iterativa o en un solo paso (modo 'lite').
            plan_start = time.perf_counter()
            if self.use_query_planner and self.query_planner:
                logger.info(f"Generando plan inicial para query: '{query[:70]}...'")
                current_plan = await self.query_planner.generate_plan(
                    query, recent_history, is_refinement_call=False
                )
            
            if not current_plan or not current_plan.sub_queries : 
                logger.warning("Planner no generó sub-consultas o falló. Usando query original y plan por defecto.")
                current_plan = SearchPlan(original_query=query, sub_queries=[SubQuery(id="sq_default", text=query, keywords=query)], status="pending_execution")

            # --- Bucle de Planificación Iterativa (si está en modo 'iterative') ---
            for iteration_step in range(self.iterative_planner_max_steps):
                logger.info(f"Iteración del Planificador {iteration_step + 1}/{self.iterative_planner_max_steps}. Estado actual del plan: {current_plan.status}")

                if current_plan.status == "final_answer_ready":
                    if current_plan.final_answer_from_planner:
                        logger.info("Planner generó respuesta final directamente.")
                        final_response = current_plan.final_answer_from_planner
                        # El post-checking se manejará más adelante
                        break
                    else: 
                        logger.info("Planner indica que la respuesta está lista, usando contexto acumulado para Responder.")
                        break 

                elif current_plan.status == "complete_no_answer":
                    logger.warning(f"Planner determinó que no hay respuesta: {current_plan.reasoning}")
                    final_response = current_plan.reasoning or "No se pudo encontrar una respuesta satisfactoria."
                    await self._query_done_callback()
                    return final_response

                elif current_plan.status == "pending_execution" or current_plan.status == "needs_refinement":
                    if not current_plan.sub_queries:
                        logger.warning(f"Plan en estado {current_plan.status} pero sin sub-consultas. Terminando.")
                        break 

                    if self.query_planning_mode == "lite" and iteration_step > 0:
                        logger.debug("Modo Lite: Saliendo del bucle de planificación después de la primera ejecución de sub-consultas.")
                        break 
                    
                    # ... Lógica interna del bucle iterativo ...
                    # (Esta parte se incluye en el tiempo de '1_planning' por simplicidad)
                    
                    if self.query_planning_mode == "iterative" and self.query_planner:
                        logger.info("Modo Iterativo: Llamando al planner para refinar el plan...")
                        formatted_history_for_refinement = "\n".join([f"SQ: {sqt}\nResultados: {res_sum}" for sqt, res_sum in executed_sub_queries_log_iterative])
                        current_plan = await self.query_planner.generate_plan(
                            query, recent_history, 
                            is_refinement_call=True, 
                            previous_sub_queries_and_results=formatted_history_for_refinement
                        )
                        if not current_plan: 
                            logger.error("Planner de refinamiento falló críticamente. Terminando.")
                            current_plan = SearchPlan(original_query=query, status="complete_no_answer", reasoning="Fallo interno del planner de refinamiento.")
                            break 
                    else: 
                        break 
                else: 
                    logger.error(f"Estado del plan no reconocido: {current_plan.status}. Terminando.")
                    break
            else: 
                logger.warning(f"Planificador alcanzó el máximo de {self.iterative_planner_max_steps} iteraciones.")
            
            timings["1_planning"] = time.perf_counter() - plan_start

            # --- Preparación del Contexto Final para el LLM Responder ---
            final_context_str = "No se encontró contexto relevante."
            selected_items_for_final_context: List[RetrievedItem] = []
            items_for_final_selection: List[RetrievedItem] = []
            
            # --- 2. Recuperación (principalmente para modo 'lite') ---
            retrieval_start = time.perf_counter()
            if self.query_planning_mode == "lite":
                lite_query_for_emb = " ".join(sq.text for sq in current_plan.sub_queries) if current_plan.sub_queries else query
                lite_query_for_bm25 = " ".join(sq.keywords or sq.text for sq in current_plan.sub_queries) if current_plan.sub_queries else query
                
                lite_ops = [
                    SearchOperation(op_type="vector", parameters={"query_text": lite_query_for_emb, "k": self.vector_initial_k}, weight=0.6),
                    SearchOperation(op_type="bm25", parameters={"query_text": lite_query_for_bm25, "k": self.bm25_initial_k}, weight=0.4),
                ]
                if get_config_value(self.config, "retrieval.use_description_search", False):
                    lite_ops.insert(0, SearchOperation(op_type="description", parameters={"query_text": lite_query_for_emb, "k": self.description_initial_k}, weight=0.3))
                lite_ops.append(CombineOperation(op_type="combine", method=self.retrieval_cfg.get("combination_method", "rrf")))
                
                executable_plan_lite = SearchPlan(original_query=query, steps=lite_ops, query_for_embedding=lite_query_for_emb, query_for_bm25=lite_query_for_bm25)
                retrieved_items_lite = await self.retrieval_manager.execute_plan_and_retrieve(executable_plan_lite)
                items_for_final_selection = retrieved_items_lite
            
            timings["2_retrieval"] = time.perf_counter() - retrieval_start
            
            # --- 3. Reranking (si está habilitado) ---
            if self.reranker and items_for_final_selection:
                rerank_start = time.perf_counter()
                items_for_final_selection = await asyncio.to_thread(self.reranker.rerank, query, items_for_final_selection)
                timings["3_reranking"] = time.perf_counter() - rerank_start

            if not items_for_final_selection and self.query_planning_mode == "lite":
                logger.warning("No hay items para selección final. Realizando búsqueda de fallback.")
                fallback_plan_ops = [SearchOperation(op_type="vector", parameters={"query_text": query, "k": self.vector_initial_k})]
                fallback_plan = SearchPlan(original_query=query, steps=fallback_plan_ops, query_for_embedding=query)
                items_for_final_selection = await self.retrieval_manager.execute_plan_and_retrieve(fallback_plan)

            # --- 4. RSE (si está habilitado) ---
            if self.use_rse and self.rse and items_for_final_selection:
                rse_start = time.perf_counter()
                logger.debug(f"Ejecutando RSE en {len(items_for_final_selection)} items...")
                text_chunks_for_rse = [it for it in items_for_final_selection if it.source not in ["description_window", "description_placeholder"]]
                desc_items_for_later = [it for it in items_for_final_selection if it.source in ["description_window", "description_placeholder"]]
                rse_segments = await self.rse.extract_segments(text_chunks_for_rse, query)
                for seg_item in rse_segments:
                    if seg_item.chunk.embedding is not None and seg_item.normalized_embedding is None:
                        seg_item.normalized_embedding = normalize_vector(seg_item.chunk.embedding)
                items_for_final_selection = rse_segments + desc_items_for_later 
                if rse_segments: logger.info(f"RSE produjo {len(rse_segments)} segmentos.")
                timings["4_rse"] = time.perf_counter() - rse_start

            # --- 5. Selección Final (MMR/Top-K) y Construcción del Prompt ---
            selection_start = time.perf_counter()
            if items_for_final_selection:
                query_embedding_list = await self.embedder.embed_text([query])
                query_emb_normalized = normalize_vector(query_embedding_list[0]) if query_embedding_list and query_embedding_list[0] is not None else None
                if self.retrieval_cfg.get("diversification_method", "mmr") == "mmr" and query_emb_normalized:
                    # ... lógica MMR ...
                    selected_items_for_final_context = sorted(items_for_final_selection, key=lambda x: x.score, reverse=True)[:self.selector_k] # Placeholder
                else: 
                    selected_items_for_final_context = sorted(items_for_final_selection, key=lambda x: x.score, reverse=True)[:self.selector_k]
                
                if selected_items_for_final_context:
                    final_context_str = await self._build_context_string_with_headers(selected_items_for_final_context)
            elif self.query_planning_mode == "iterative" and accumulated_results_text_iterative:
                final_context_str = accumulated_results_text_iterative
            timings["5_selection_and_prompting"] = time.perf_counter() - selection_start

            # --- 6. Generación de Respuesta ---
            if not (self.query_planning_mode == "iterative" and current_plan.status == "final_answer_ready" and current_plan.final_answer_from_planner):
                generation_start = time.perf_counter()
                system_prompt_for_responder = self.prompt_builder.get_full_system_prompt(final_context_str)
                logger.debug(f"Contexto para Responder LLM (primeros 300 chars):\n{final_context_str[:300]}...")
                
                generated_response_str = await self.responder.generate(
                    prompt=query, system_prompt_context=system_prompt_for_responder, history=recent_history,
                    max_tokens=get_config_value(self.config, "responder_llm.max_output_tokens", 512),
                    temperature=get_config_value(self.config, "responder_llm.temperature", 0.7),
                    stream=False
                )
                logger.debug(f"Respuesta cruda del LLM Responder: '{str(generated_response_str)[:200]}...'")
                final_response = generated_response_str
                timings["6_generation"] = time.perf_counter() - generation_start

            # --- 7. Post-checking (Opcional) ---
            if self.post_checker and self.post_checker.enabled and (use_post_checker_override is None or use_post_checker_override):
                post_check_start = time.perf_counter()
                final_response = await self.post_checker.check(query, final_response, final_context_str)
                timings["7_post_checking"] = time.perf_counter() - post_check_start
            
            logger.info(f"Respuesta final para '{query[:50]}...': '{final_response[:100]}...'")

        except Exception as e:
            logger.exception(f"Error fatal procesando query '{query[:50]}...': {e}")
            final_response = "Lo siento, ocurrió un error interno procesando su solicitud."
        finally:
            # --- INICIO DE CAMBIOS: Registro de Métricas ---
            await self._query_done_callback()
            total_time = time.perf_counter() - query_start_time
            timings["total_query_time"] = total_time

            # Filtra las claves que no se usaron (valor 0 o None) para un log más limpio
            active_timings = {k: v for k, v in timings.items() if v}
            
            timing_summary = " | ".join([f"{key}: {value:.3f}s" for key, value in sorted(active_timings.items())])
            logger.info(f"Métricas de rendimiento para la consulta: {timing_summary}")
            print(f"(Análisis de tiempo: {timing_summary})") # También en consola para visibilidad inmediata
            # --- FIN DE CAMBIOS ---

        return final_response

    async def _query_done_callback(self):
        tasks = []
        components = [self.chunk_store, self.description_store, self.llm_cache_store,
                      self.vector_db, self.kg_graph] # <<< self.bm25_index ELIMINADO
        for comp in components:
            if comp and hasattr(comp, 'query_done_callback'):
                cb = getattr(comp, 'query_done_callback')
                if asyncio.iscoroutinefunction(cb): tasks.append(cb())
                else: tasks.append(asyncio.to_thread(cb))
        if tasks: 
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    comp_name = type(components[i]).__name__ if components[i] else "Componente Desconocido"
                    logger.error(f"Error durante query_done_callback de {comp_name}: {result}", exc_info=result)


---
File: /generation/__init__.py
---




---
File: /generation/llm_responder.py
---

# generation/llm_responder.py
from typing import List, Dict, Optional, Any, Union, AsyncIterator
from ..llm.base import BaseLLMWrapper
from ..utils.logging_setup import logger

class LLMResponder:
    """Genera la respuesta final usando el LLM configurado."""

    def __init__(self, llm_wrapper: BaseLLMWrapper):
        self.llm = llm_wrapper
        logger.info(f"LLMResponder inicializado con wrapper: {type(llm_wrapper).__name__}")

    async def generate(
        self,
        prompt: str, # La query original del usuario
        system_prompt_context: str, # El system prompt ya formateado con el contexto
        history: Optional[List[Dict[str, str]]] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ) -> Union[str, AsyncIterator[str]]:
        """Llama al LLM para generar la respuesta."""
        logger.debug(f"Generando respuesta para prompt: '{prompt[:50]}...'")
        try:
            response = await self.llm.generate(
                prompt=prompt,
                system_prompt=system_prompt_context,
                history=history,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=stream,
                **kwargs
            )
            return response
        except Exception as e:
            logger.exception(f"Error generando respuesta final del LLM: {e}")
            if stream:
                 async def error_stream():
                      yield "[Error al generar la respuesta]"
                 return error_stream()
            else:
                 return "[Error al generar la respuesta]"


---
File: /generation/post_checker.py
---

# ares/generation/post_checker.py
import json
import re
from typing import List, Dict, Optional, Any
from ..llm.base import BaseLLMWrapper
from ..utils.logging_setup import logger

POSTCHECKER_SYSTEM_PROMPT = """
Eres un verificador de calidad riguroso. Evalúa si la 'Respuesta Generada' es consistente y está respaldada **únicamente** por el 'Contexto Proporcionado'. NO uses conocimiento externo.

Contexto Proporcionado:
```
{retrieved_context}
```
------
Consulta Original del Usuario:
```
{user_query}
```
------
Respuesta Generada:
```
{original_response}
```
------
Instrucciones de Verificación:
1.  **Consistencia:** ¿La Respuesta contradice **explícitamente** alguna información del Contexto?
2.  **Factualidad (Basada en Contexto):** ¿La Respuesta introduce hechos o detalles **no mencionados** en el Contexto (alucinación)?
3.  **Relevancia:** ¿La Respuesta aborda **directamente** la Consulta Original?

Responde estrictamente con un objeto JSON:
```json
{{
  "is_consistent": true | false,
  "is_factual_based_on_context": true | false,
  "is_relevant": true | false,
  "reasoning": "Explicación MUY BREVE (1 frase) solo si alguna verificación falla.",
  "suggested_action": "ok | revise | discard"
}}
```
- `ok`: Todas las verificaciones son `true`.
- `revise`: `is_relevant` es `true`, pero `is_consistent` o `is_factual` es `false` (inconsistencia menor o alucinación leve).
- `discard`: `is_relevant` es `false`, O hay contradicciones/alucinaciones graves.

Responde ÚNICAMENTE con el objeto JSON válido.
"""

class PostChecker:
    """Verifica la consistencia y relevancia de la respuesta generada."""

    def __init__(self, llm_wrapper: Optional[BaseLLMWrapper], enabled: bool = False):
        # Permitir inicializar sin LLM si está deshabilitado
        self.llm = llm_wrapper
        self.enabled = enabled and (llm_wrapper is not None)
        if self.enabled:
            logger.info("PostChecker habilitado e inicializado.")
        elif enabled and llm_wrapper is None:
             logger.warning("PostChecker habilitado en config pero LLM no disponible. PostChecker deshabilitado.")
        else:
             logger.info("PostChecker deshabilitado.")

    async def check(self, query: str, response: str, context: str) -> str:
        """Verifica la respuesta. Devuelve la original o un mensaje de error."""
        if not self.enabled or not self.llm:
            return response # Devuelve original si está deshabilitado

        # Limitar contexto para el checker para evitar exceder su límite
        # Podríamos usar un método de tokenización aquí para ser más precisos
        max_context_chars = 3500 # Límite de caracteres arbitrario
        context_for_checker = context[:max_context_chars]
        if len(context) > max_context_chars:
            logger.debug("Contexto truncado para PostChecker.")

        logger.debug("Ejecutando Post-Checker...")
        prompt = POSTCHECKER_SYSTEM_PROMPT.format(
            retrieved_context=context_for_checker,
            user_query=query,
            original_response=response
        )

        try:
            checker_response_str = await self.llm.generate(prompt=prompt, temperature=0.1, max_tokens=200, stream=False)

            if not checker_response_str or not isinstance(checker_response_str, str):
                 logger.error("PostChecker LLM devolvió respuesta inválida.")
                 return response # Fallback a respuesta original

            # Parsear JSON de forma robusta
            match = re.search(r"\{.*\}", checker_response_str, re.DOTALL)
            if not match:
                logger.error(f"PostChecker no devolvió JSON: {checker_response_str}")
                return response # Fallback

            json_str = match.group(0).strip().replace("```json", "").replace("```", "").strip()
            checker_result = json.loads(json_str)

            action = checker_result.get("suggested_action", "ok")
            reasoning = checker_result.get("reasoning", "N/A")
            passed = checker_result.get("is_consistent", False) and \
                     checker_result.get("is_factual_based_on_context", False) and \
                     checker_result.get("is_relevant", False)

            logger.info(f"Post-Check: Action={action}, Passed={passed}. Razón: {reasoning}")

            if action == "ok" or passed: # Si pasa todas las verificaciones O la acción es OK
                return response
            elif action == "revise":
                logger.warning(f"Post-Checker sugiere revisión: {reasoning}")
                # Podríamos añadir un prefijo o devolverla tal cual
                return response # Devolver original por simplicidad
            else: # discard
                logger.error(f"Post-Checker descartó respuesta: {reasoning}")
                return "Lo siento, no pude generar una respuesta confiable basada en la información disponible."

        except json.JSONDecodeError:
            logger.error(f"Post-Checker devolvió JSON inválido: {checker_response_str}")
            return response # Fallback
        except Exception as e:
            logger.exception(f"Error durante Post-Checking: {e}")
            return response # Fallback


---
File: /generation/prompt_builder.py
---

# generation/prompt_builder.py
from typing import List, Dict, Any
from ..core.datatypes import Chunk
from ..utils.logging_setup import logger
from ..utils.helpers import encode_string_by_tiktoken # Para contar tokens

class PromptBuilder:
    """Construye el contexto final para el LLM Responder."""

    def __init__(self, system_prompt_template: str):
        if "{retrieved_context}" not in system_prompt_template:
             logger.warning("System prompt template no contiene '{retrieved_context}'. El contexto recuperado no se incluirá.")
             # Podríamos añadirlo si falta:
             # self.system_prompt_template = system_prompt_template + "\nContexto Recuperado:\n```\n{retrieved_context}\n```\n-------"
        self.system_prompt_template = system_prompt_template
        # self.tokenizer_encoder = tiktoken.get_encoding("cl100k_base") # Cachear si se usa mucho

    def _format_chunk(self, chunk: Chunk, index: int, include_citations: bool) -> str: # <-- Añadir include_citations
        """Formatea un chunk individual para incluirlo en el prompt."""
        metadata_str_parts = []
        # Solo añadir ID si se van a incluir citaciones
        if include_citations:
             metadata_str_parts.append(f"SourceID: {chunk.id}") # Usar SourceID para claridad
        if 'window_id' in chunk.metadata: metadata_str_parts.append(f"VentanaID: {chunk.metadata['window_id']}")
        if 'timestamp' in chunk.metadata: metadata_str_parts.append(f"Timestamp: {chunk.metadata['timestamp']}")
        # Añadir otros metadatos útiles si existen
        # if 'speaker' in chunk.metadata: metadata_str_parts.append(f"Speaker: {chunk.metadata['speaker']}")
        # if 'source' in chunk.metadata: metadata_str_parts.append(f"SourceFile: {chunk.metadata['source']}")

        metadata_str = " | ".join(metadata_str_parts)
        header = f"--- Contexto Recuperado {index+1}{f' [{metadata_str}]' if metadata_str else ''} ---"

        return f"{header}\n{chunk.text}\n"

    def build(self, selected_chunks: List[Chunk], max_context_tokens: int = 3000, include_citations: bool = False) -> str: # <-- Añadir include_citations
        """Construye la string de contexto final respetando el límite de tokens."""
        context_string = ""
        current_tokens = 0
        chunks_included_count = 0 # Contador de chunks realmente incluidos

        for i, chunk in enumerate(selected_chunks):
            # Pasar include_citations a _format_chunk
            formatted_chunk = self._format_chunk(chunk, i, include_citations)
            # Usar el helper para contar tokens
            chunk_tokens = len(encode_string_by_tiktoken(formatted_chunk))

            if current_tokens + chunk_tokens <= max_context_tokens:
                context_string += formatted_chunk + "\n"
                current_tokens += chunk_tokens + 1 # +1 por el newline
                chunks_included_count += 1
            else:
                logger.warning(f"Contexto truncado. Se alcanzó el límite de {max_context_tokens} tokens. {chunks_included_count}/{len(selected_chunks)} chunks incluidos.")
                break

        logger.debug(f"Contexto final construido con {current_tokens} tokens y {chunks_included_count} chunks.")
        return context_string.strip()

    # --- CORRECCIÓN: Eliminar el argumento extra ---
    def get_full_system_prompt(self, retrieved_context: str) -> str:
        """Inyecta el contexto recuperado en la plantilla del system prompt."""
        # La lógica de citaciones se maneja en build() y en el template mismo
        return self.system_prompt_template.format(retrieved_context=retrieved_context)


---
File: /ingestion/__init__.py
---




---
File: /ingestion/chunker.py
---

from typing import List, Dict, Any, Tuple
from chonkie import TokenChunker, SentenceChunker, RecursiveChunker
from chonkie.types import Chunk as ChonkieChunk
from chonkie.types import RecursiveRules # <--- IMPORTAR RecursiveRules

from ..core.datatypes import Chunk
from ..utils.logging_setup import logger

class ChonkieChunkerWrapper:
    """Wrapper para usar diferentes estrategias de Chonkie."""

    def __init__(self, strategy: str, config: Dict[str, Any], tokenizer_name_or_path: str):
        self.strategy = strategy
        _config = config.copy() # Trabajar con una copia
        self.tokenizer_name = tokenizer_name_or_path

        logger.info(f"Inicializando ChonkieWrapper con estrategia '{strategy}' y config: {_config}")

        try:
            if strategy == "token":
                # Parámetros esperados por TokenChunker: tokenizer, chunk_size, chunk_overlap, return_type
                # Asegurarse de que _config solo contenga estos o los que TokenChunker acepte
                self.chunker = TokenChunker(tokenizer=tokenizer_name_or_path, **_config)
            elif strategy == "sentence":
                # Parámetros esperados por SentenceChunker: tokenizer_or_token_counter, chunk_size, chunk_overlap, etc.
                self.chunker = SentenceChunker(tokenizer_or_token_counter=tokenizer_name_or_path, **_config)
            elif strategy == "recursive":
                rules_config_value = _config.pop("rules", "default") # Esto es "default" o podría ser un dict/path
                # --- INICIO DE LA CORRECCIÓN ---
                if isinstance(rules_config_value, str):
                    # Si es un string, asumimos que es el nombre de una receta (o 'default')
                    actual_rules = RecursiveRules.from_recipe(name=rules_config_value)
                elif isinstance(rules_config_value, dict):
                    # Si es un dict, asumimos que es una definición directa de reglas
                    actual_rules = RecursiveRules.from_dict(rules_config_value)
                elif isinstance(rules_config_value, RecursiveRules):
                    # Si ya es un objeto RecursiveRules
                    actual_rules = rules_config_value
                else:
                    logger.warning(f"Tipo de 'rules' no reconocido para RecursiveChunker: {type(rules_config_value)}. Usando reglas por defecto.")
                    actual_rules = RecursiveRules() # Fallback a reglas por defecto de Chonkie
                # --- FIN DE LA CORRECCIÓN ---
                
                chunk_size_value = _config.pop("chunk_size", 512)
                min_chars_value = _config.pop("min_characters_per_chunk", 24)
                return_type_value = _config.pop("return_type", "chunks")
                _config.pop("chunk_overlap", None)

                self.chunker = RecursiveChunker(
                    tokenizer_or_token_counter=tokenizer_name_or_path,
                    rules=actual_rules, # <--- PASAR EL OBJETO RecursiveRules
                    chunk_size=chunk_size_value,
                    min_characters_per_chunk=min_chars_value,
                    return_type=return_type_value,
                    **_config
                )
            # elif strategy == "semantic":
            #     from chonkie import SemanticChunker
            #     embedding_model_name = _config.pop("embedding_model", "all-MiniLM-L6-v2")
            #     self.chunker = SemanticChunker(embedding_model=embedding_model_name, **_config)
            else:
                raise ValueError(f"Estrategia de chunking '{strategy}' no soportada por este wrapper.")
            logger.info(f"Chunker Chonkie '{strategy}' inicializado.")
        except Exception as e:
            logger.error(f"Error inicializando Chonkie chunker '{strategy}': {e}")
            raise

    # (El método chunk sin cambios)
    def chunk(self, text: str) -> List[Tuple[int, int, str]]:
        if not text:
            return []
        try:
            original_return_type = None
            if hasattr(self.chunker, 'return_type'):
                 original_return_type = getattr(self.chunker, 'return_type')
                 setattr(self.chunker, 'return_type', 'chunks')
            chonkie_results = self.chunker(text)
            if original_return_type is not None and hasattr(self.chunker, 'return_type'):
                 setattr(self.chunker, 'return_type', original_return_type)
            boundaries = []
            if chonkie_results and isinstance(chonkie_results, list) and chonkie_results:
                if isinstance(chonkie_results[0], ChonkieChunk):
                    chonkie_chunks: List[ChonkieChunk] = chonkie_results
                    for chk in chonkie_chunks:
                        if hasattr(chk, 'start_index') and hasattr(chk, 'end_index') and hasattr(chk, 'text'):
                            boundaries.append((chk.start_index, chk.end_index, chk.text))
                        else:
                            logger.warning(f"Chunk de Chonkie tipo {type(chk)} no tiene start/end index o texto. Omitiendo.")
                elif isinstance(chonkie_results[0], tuple) and len(chonkie_results[0]) == 3:
                    if all(isinstance(t[0], int) and isinstance(t[1], int) and isinstance(t[2], str) for t in chonkie_results):
                        boundaries = chonkie_results
                    else:
                        logger.warning(f"Chonkie devolvió tuplas pero con formato inesperado: {chonkie_results[0]}")
                else:
                    logger.warning(f"Resultado inesperado de Chonkie chunker ({type(chonkie_results)} con elementos tipo {type(chonkie_results[0]) if chonkie_results else 'None'}). No se pudieron extraer límites.")
            elif not chonkie_results:
                 logger.debug("Chonkie no devolvió resultados (lista vacía).")
            else:
                 logger.warning(f"Resultado inesperado de Chonkie chunker ({type(chonkie_results)}). No se pudieron extraer límites.")

            logger.debug(f"Chonkie definió {len(boundaries)} límites de chunks para texto de longitud {len(text)}")
            return boundaries
        except Exception as e:
            logger.exception(f"Error durante el chunking con Chonkie ({self.strategy}): {e}")
            if 'original_return_type' in locals() and original_return_type is not None and hasattr(self.chunker, 'return_type'):
                 setattr(self.chunker, 'return_type', original_return_type)
            return []


---
File: /ingestion/description_generator.py
---

# ares/ingestion/description_generator.py
import json
import re
from datetime import datetime, timezone
from typing import Dict, Any, Optional, List

from ..llm.base import BaseLLMWrapper
from ..core.datatypes import Description
from ..core.config_loader import get_config_value
from ..utils.helpers import log_or_save_raw_json
from ..utils.logging_setup import logger

# ============================================================
# PROMPT – Versión 2 (combinada, fuerza JSON válido)
# ============================================================
DESC_COMBINED_PROMPT_TEMPLATE_V2 = """
Analiza el siguiente texto de una ventana de conversación. Extrae la siguiente información ESTRICTAMENTE en formato JSON:
1.  `objective_summary`: Resumen **fáctico y conciso** de lo explícitamente dicho (1-2 frases). Céntrate SOLO en eventos, temas o datos concretos mencionados.
2.  `subjective_interpretation`: (OPCIONAL) Breve descripción (1 frase) del posible **sentimiento, intención o tono implícito** percibido. Usa `null` si no hay nada claro o relevante.
3.  `entities`: Lista (máx 5) de entidades nombradas clave (personas, lugares, organizaciones, conceptos específicos) mencionadas. Formato: `{{"name": "...", "type": "PERSONA|LUGAR|ORGANIZACION|CONCEPTO|EVENTO"}}`.
4.  `relations`: Lista (máx 3) de relaciones **explícitas** clave entre las entidades anteriores. Formato: `{{"source": "entidad1", "target": "entidad2", "label": "VERBO_O_RELACION"}}`.
5.  `keywords`: Lista de 3-5 palabras clave que representen los temas principales.

Texto de la ventana:
```
{text}
```

Respuesta JSON (SOLO el objeto JSON):
```json
{{
  "objective_summary": "...",
  "subjective_interpretation": null,
  "entities": [],
  "relations": [],
  "keywords": []
}}
```
Asegúrate de que la salida sea **JSON válido** y NO incluyas texto fuera del bloque.
"""

# ============================================================
# CLASE PRINCIPAL
# ============================================================
class DescriptionGenerator:
    """
    Genera descripciones estructuradas (objetivo + subjetivo + entidades, etc.)
    para una ventana de conversación.
    """

    def __init__(self, llm_wrapper: BaseLLMWrapper, config: Optional[Dict[str, Any]] = None):
        self.llm = llm_wrapper
        self.prompt_template = DESC_COMBINED_PROMPT_TEMPLATE_V2
        self.config = config or {}
        logger.info("DescriptionGenerator (v2) inicializado.")

    # --------------------------------------------------------
    # PRIVATE: _parse_combined_output
    # --------------------------------------------------------
    def _parse_combined_output(self, llm_output: str, window_id: str) -> Optional[Dict[str, Any]]:
        """
        Intenta extraer y validar el JSON devuelto por el LLM.

        Se registran (o guardan) las salidas crudas y los errores para facilitar
        la depuración en producción.
        """
        logger.debug(f"Respuesta cruda LLM DescGen para {window_id}: '{llm_output[:500]}...'")

        # 1) Respuesta vacía / inválida
        if not llm_output or not isinstance(llm_output, str) or llm_output.strip() == "":
            logger.error(f"LLM DescGen devolvió respuesta vacía o inválida para {window_id}.")
            return None

        json_str_extracted_for_parsing: Optional[str] = None

        try:
            # 2) Buscar bloque ```json ... ```  ó JSON “pelado”
            match_markdown_json = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", llm_output, re.DOTALL)
            if match_markdown_json:
                json_str_raw = match_markdown_json.group(1).strip()
            else:
                match_plain_json = re.search(r"(\{[\s\S]*\})(?=\s*$|\n)", llm_output, re.DOTALL)
                if match_plain_json:
                    json_str_raw = match_plain_json.group(1).strip()
                else:
                    # ---- MODIFICACIÓN: guardar salida sin JSON
                    log_or_save_raw_json(
                        llm_output,
                        "DescGen_NoJSONFound",
                        window_id=window_id,
                        base_working_dir=get_config_value(self.config, "general.working_dir"),
                    )
                    logger.error(
                        f"No se encontró JSON en respuesta DescGen para {window_id}. "
                        f"Se guardó la respuesta cruda para depuración. Respuesta: '{llm_output[:200]}...'"
                    )
                    return None

            # 3) Limpiar comentarios estilo // o /* */
            json_str_cleaned = re.sub(r"//.*", "", json_str_raw)
            json_str_cleaned = re.sub(r"/\*.*?\*/", "", json_str_cleaned, flags=re.DOTALL).strip()
            json_str_extracted_for_parsing = json_str_cleaned

            if not json_str_extracted_for_parsing:
                log_or_save_raw_json(
                    json_str_raw,
                    "DescGen_EmptyAfterClean",
                    window_id=window_id,
                    base_working_dir=get_config_value(self.config, "general.working_dir"),
                )
                logger.error(
                    f"Bloque JSON extraído vacío para {window_id} (después de limpiar). "
                    f"Original: '{llm_output[:200]}...'"
                )
                return None

            # 4) Guardar JSON previo al parseo (útil para debug)
            log_or_save_raw_json(
                json_str_extracted_for_parsing,
                "DescGen_PreParse",
                window_id=window_id,
                base_working_dir=get_config_value(self.config, "general.working_dir"),
            )

            # 5) Cargar JSON
            data: Dict[str, Any] = json.loads(json_str_extracted_for_parsing)

            # ------------------------------------------------
            # 6) Validaciones / normalizaciones básicas
            # ------------------------------------------------
            objective_summary: str = data.get("objective_summary", "")
            if not isinstance(objective_summary, str):
                objective_summary = str(objective_summary)

            subjective_interpretation = data.get("subjective_interpretation")
            if subjective_interpretation is not None and not isinstance(subjective_interpretation, str):
                subjective_interpretation = str(subjective_interpretation)

            # --- Entidades ---
            entities_raw: List[Any] = data.get("entities", [])
            entities: List[Dict[str, str]] = []
            if isinstance(entities_raw, list):
                for ent in entities_raw:
                    if (
                        isinstance(ent, dict)
                        and "name" in ent
                        and "type" in ent
                        and isinstance(ent["name"], str)
                        and isinstance(ent["type"], str)
                    ):
                        entities.append({"name": ent["name"], "type": ent["type"]})
                    else:
                        logger.warning(f"Entidad inválida omitida en DescGen para {window_id}: {ent}")

            # --- Relaciones ---
            relations_raw: List[Any] = data.get("relations", [])
            relations: List[Dict[str, str]] = []
            if isinstance(relations_raw, list):
                for rel in relations_raw:
                    if (
                        isinstance(rel, dict)
                        and all(k in rel for k in ("source", "target", "label"))
                    ):
                        relations.append(
                            {
                                "source": str(rel["source"]),
                                "target": str(rel["target"]),
                                "label": str(rel["label"]),
                            }
                        )
                    else:
                        logger.warning(f"Relación inválida omitida en DescGen para {window_id}: {rel}")

            # --- Keywords ---
            keywords_raw: List[Any] = data.get("keywords", [])
            keywords: List[str] = [str(k) for k in keywords_raw if isinstance(k, (str, int, float))]
            # convertir ints o floats a str para mantener consistencia

            return {
                "objective_summary": objective_summary,
                "subjective_interpretation": subjective_interpretation,
                "entities": entities,
                "relations": relations,
                "keywords": keywords,
            }

        # ----------------- Handle errores -----------------
        except json.JSONDecodeError as e:
            logger.error(
                f"Error parseando JSON DescGen para {window_id}: {e}. "
                f"JSON intentado (truncado): '{str(json_str_extracted_for_parsing)[:200]}...'"
            )
            # El JSON ya se guardó antes (PreParse)
            return None

        except Exception as e:
            logger.exception(
                f"Error inesperado parseando DescGen para {window_id}: {e}. "
                f"Respuesta original (truncada): '{llm_output[:200]}...'"
            )
            log_or_save_raw_json(
                llm_output,
                "DescGen_UnexpectedError",
                window_id=window_id,
                base_working_dir=get_config_value(self.config, "general.working_dir"),
            )
            return None

    # --------------------------------------------------------
    # PUBLIC: generate
    # --------------------------------------------------------
    async def generate(
        self,
        text: str,
        window_id: str,
        base_metadata: Dict[str, Any] = {},
    ) -> Optional[Description]:
        """
        Orquesta la llamada al LLM y devuelve un objeto `Description`
        estructurado, o `None` si algo falla.
        """
        if not text or not isinstance(text, str):
            logger.warning(f"Texto vacío o inválido proporcionado a DescriptionGenerator para {window_id}.")
            return None

        prompt = self.prompt_template.format(text=text)

        try:
            logger.debug(f"Llamando a LLM para descripción de ventana {window_id}...")
            llm_response: str = await self.llm.generate(
                prompt=prompt,
                temperature=0.2,
                max_tokens=500,
                stream=False,
            )

            parsed_data = self._parse_combined_output(llm_response, window_id)
            if not parsed_data:
                logger.error(f"Falló el parseo de la respuesta del LLM para {window_id}.")
                return None

            # Timestamp por defecto si no viene en metadata
            if not base_metadata.get("timestamp"):
                base_metadata["timestamp"] = datetime.now(timezone.utc).isoformat()

            # Componer y devolver el dataclass Description
            description_obj = Description(
                window_id=window_id,
                objective_summary=parsed_data["objective_summary"],
                subjective_interpretation=parsed_data["subjective_interpretation"],
                entities=parsed_data["entities"],
                relations=parsed_data["relations"],
                keywords=parsed_data["keywords"],
                metadata=base_metadata,
            )

            logger.debug(f"Descripción generada correctamente para {window_id}.")
            return description_obj

        except Exception as e:
            logger.exception(f"Error inesperado en generate() para {window_id}: {e}")
            # Guarda la respuesta cruda (si existe) para depurar
            if 'llm_response' in locals() and llm_response:
                log_or_save_raw_json(
                    llm_response,
                    "DescGen_GenerateError",
                    window_id=window_id,
                    base_working_dir=get_config_value(self.config, "general.working_dir"),
                )
            return None


---
File: /ingestion/embedder_sparse.py
---

# ares/ingestion/embedder_sparse.py
from typing import List, Optional, Dict, Any
import asyncio

# Dependencia de terceros
from fastembed import SparseTextEmbedding, SparseEmbedding

# Dependencias del proyecto ARES
from ..utils.logging_setup import logger

_sparse_model_cache: Dict[str, SparseTextEmbedding] = {}

class FastEmbedSparseEmbedder:
    """
    Wrapper para FastEmbed que genera embeddings dispersos (sparse embeddings)
    para búsqueda léxica.
    """
    def __init__(self, model_name: str = "Qdrant/bm25", **kwargs):
        self.model_name = model_name
        
        if self.model_name in _sparse_model_cache:
            self.model = _sparse_model_cache[self.model_name]
            logger.info(f"FastEmbedSparseEmbedder cargado desde caché para el modelo: '{self.model_name}'.")
        else:
            try:
                self.model = SparseTextEmbedding(model_name=self.model_name, **kwargs)
                _sparse_model_cache[self.model_name] = self.model
                logger.info(f"FastEmbedSparseEmbedder inicializado con el modelo: '{self.model_name}'.")
            except Exception as e:
                logger.exception(f"No se pudo inicializar el modelo SparseTextEmbedding '{self.model_name}': {e}")
                raise

    async def embed_sparse_batch(self, texts: List[str]) -> List[Optional[Dict[str, Any]]]:
        """
        Genera embeddings dispersos para un lote de textos de forma asíncrona.
        """
        if not texts:
            return []
        
        try:
            # FastEmbed es síncrono pero optimizado para CPU. Usar to_thread para no bloquear.
            sparse_embeddings: List[SparseEmbedding] = await asyncio.to_thread(
                list, self.model.embed(texts)
            )
            
            results = []
            for se in sparse_embeddings:
                if se and se.indices.size > 0:
                    results.append({"indices": se.indices.tolist(), "values": se.values.tolist()})
                else:
                    results.append(None)
            return results
        except Exception as e:
            logger.exception(f"Error generando embeddings dispersos con FastEmbed: {e}")
            return [None] * len(texts)


---
File: /ingestion/embedder.py
---

# ares/ingestion/embedder.py
import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer
from typing import List, Tuple, Optional, Any, Dict
# from ..core.datatypes import Chunk # No es necesario importar Chunk aquí
from ..utils.logging_setup import logger
import gc
from memory_profiler import profile
import asyncio

_local_models_cache = {}

def _load_local_model(model_name_or_path: str, device: str = "cpu"):
    cache_key = (model_name_or_path, device)
    if cache_key in _local_models_cache:
        logger.debug(f"Usando modelo/tokenizer cacheados para {cache_key}")
        model, tokenizer = _local_models_cache[cache_key]
        model.to(device) # Asegurar que esté en el dispositivo correcto
        return model, tokenizer

    logger.info(f"Cargando modelo de embedding '{model_name_or_path}' en dispositivo '{device}'...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
        model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True)
        model.to(device)
        model.eval()
        _local_models_cache[cache_key] = (model, tokenizer)
        logger.info(f"Modelo '{model_name_or_path}' cargado.")
        return model, tokenizer
    except Exception as e:
        logger.error(f"Error cargando modelo local '{model_name_or_path}': {e}")
        raise

class LateChunkingEmbedder:
    def __init__(self, model_provider: str, model_name: str, local_options: Dict = {}, api_options: Dict = {}):
        self.model_provider = model_provider
        self.model_name = model_name
        self.local_options = local_options
        self.api_options = api_options

        if self.model_provider == "local":
            self.device = local_options.get("device", "cpu")
            self.model, self.tokenizer = _load_local_model(self.model_name, self.device)
            
            model_name_lower = model_name.lower()
            self.has_instructions = "jina-embeddings-v3" in model_name_lower or \
                                    "nomic-embed-text" in model_name_lower
            
            # INICIO: CAMBIOS SOLICITADOS EN __init__
            self.is_qwen_embedding_model = "qwen3-embedding" in model_name_lower
            if self.is_qwen_embedding_model:
                logger.info("Detectado modelo de embedding Qwen. Se usará 'last_token_pool' y formato de instruct para queries.")
                self.query_instruction_template = "Instruct: {task}\nQuery: {query}"
            # FIN: CAMBIOS SOLICITADOS EN __init__
            
            if "jina-embeddings-v3" in model_name_lower:
                self.passage_instruction = self.model.get_instructions().get('retrieval.passage', "") if hasattr(self.model, 'get_instructions') else "Represent this passage for searching relevant passages: "
                self.query_instruction = self.model.get_instructions().get('retrieval.query', "") if hasattr(self.model, 'get_instructions') else "Represent this query for retrieving relevant passages: "
            elif "nomic-embed-text" in model_name_lower:
                self.passage_instruction = "search_document: "
                self.query_instruction = "search_query: "
            else:
                self.passage_instruction = ""
                self.query_instruction = ""

            self.max_length = getattr(self.model.config, 'max_position_embeddings', 512)
            if "jina-embeddings-v3" in model_name_lower:
                 pass 

            logger.info(f"Embedder local {model_name} configurado (max_length: {self.max_length}, instructions: {self.has_instructions}, passage_instr_len: {len(self.passage_instruction)})")
        else:
            raise ValueError(f"Proveedor de embedding no soportado: {self.model_provider}")

    def _get_token_indices_for_char_spans(self,
                                         text_that_went_to_tokenizer: str,
                                         char_spans_in_original_text_segment: List[Tuple[int, int]],
                                         prefix_length_before_original_text_segment: int
                                         ) -> List[Optional[Tuple[int, int]]]:
        if not char_spans_in_original_text_segment:
            return []

        encoding = self.tokenizer(
            text_that_went_to_tokenizer,
            return_offsets_mapping=True,
            add_special_tokens=True,
            truncation=False
        )
        
        offset_mapping = encoding.offset_mapping
        input_ids_from_encoding = encoding.input_ids

        token_spans_result = []
        for start_char_orig, end_char_orig in char_spans_in_original_text_segment:
            chunk_char_start_in_full_tokenized_text = start_char_orig + prefix_length_before_original_text_segment
            chunk_char_end_in_full_tokenized_text = end_char_orig + prefix_length_before_original_text_segment
            current_chunk_start_token_idx, current_chunk_end_token_idx = -1, -1

            for token_idx, (tok_char_start, tok_char_end) in enumerate(offset_mapping):
                if tok_char_start == 0 and tok_char_end == 0 and token_idx > 0 and token_idx < len(offset_mapping) - 1:
                    continue
                if current_chunk_start_token_idx == -1:
                    if tok_char_end > chunk_char_start_in_full_tokenized_text:
                        current_chunk_start_token_idx = token_idx
                if tok_char_start >= chunk_char_end_in_full_tokenized_text and current_chunk_start_token_idx != -1:
                    current_chunk_end_token_idx = token_idx
                    break
            
            if current_chunk_start_token_idx != -1 and current_chunk_end_token_idx == -1:
                last_valid_idx = len(offset_mapping) - 1
                while last_valid_idx > 0 and offset_mapping[last_valid_idx][0] == 0 and offset_mapping[last_valid_idx][1] == 0:
                    last_valid_idx -= 1
                if input_ids_from_encoding[last_valid_idx] == self.tokenizer.sep_token_id:
                     current_chunk_end_token_idx = last_valid_idx + 1
                else:
                     current_chunk_end_token_idx = last_valid_idx + 1
            
            is_first_chunk_of_original_text = (char_spans_in_original_text_segment.index((start_char_orig, end_char_orig)) == 0)
            if is_first_chunk_of_original_text and current_chunk_start_token_idx > 0:
                 if input_ids_from_encoding[0] == self.tokenizer.cls_token_id:
                     current_chunk_start_token_idx = 0
            
            is_last_chunk_of_original_text = (char_spans_in_original_text_segment.index((start_char_orig, end_char_orig)) == len(char_spans_in_original_text_segment) - 1)
            if is_last_chunk_of_original_text and current_chunk_end_token_idx != -1 and current_chunk_end_token_idx < len(input_ids_from_encoding):
                if input_ids_from_encoding[len(input_ids_from_encoding) - 1] == self.tokenizer.sep_token_id:
                    current_chunk_end_token_idx = len(input_ids_from_encoding)

            if current_chunk_start_token_idx != -1 and current_chunk_end_token_idx != -1 and current_chunk_start_token_idx < current_chunk_end_token_idx:
                token_spans_result.append((current_chunk_start_token_idx, current_chunk_end_token_idx))
            else:
                logger.warning(f"LateChunking: No se pudo mapear span char ({start_char_orig},{end_char_orig}). Resultó en token_span ({current_chunk_start_token_idx}, {current_chunk_end_token_idx}).")
                token_spans_result.append(None)
        
        return token_spans_result

    @profile
    @torch.no_grad()
    async def embed_texts_batch(self, texts: List[str], is_query: bool) -> List[Optional[np.ndarray]]:
        """
        Genera embeddings para un lote de textos (queries o pasajes).
        Este método es la base para todas las operaciones de embedding en lote.
        """
        if not texts:
            return []
        
        # INICIO: CAMBIOS SOLICITADOS EN embed_texts_batch (preparación de texto)
        texts_to_process = texts
        if is_query and self.is_qwen_embedding_model:
            task_description = "Given a web search query, retrieve relevant passages that answer the query"
            texts_to_process = [self.query_instruction_template.format(task=task_description, query=text) for text in texts]
        elif is_query and self.has_instructions:
             texts_to_process = [self.query_instruction + text for text in texts]
        elif not is_query and self.has_instructions:
            # Aunque no se solicitó explícitamente, mantenemos la lógica para pasajes por consistencia.
            texts_to_process = [self.passage_instruction + text for text in texts]
        # FIN: CAMBIOS SOLICITADOS EN embed_texts_batch (preparación de texto)

        try:
            # Envolver llamadas síncronas de PyTorch en to_thread
            def sync_inference():
                inputs = self.tokenizer(
                    texts_to_process, return_tensors='pt', padding=True,
                    truncation=True, max_length=self.max_length
                ).to(self.device)

                outputs = self.model(**inputs)
                
                # INICIO: CAMBIOS SOLICITADOS EN embed_texts_batch (pooling y normalización)
                if self.is_qwen_embedding_model:
                    pooled_embeddings = self._last_token_pool(outputs.last_hidden_state, inputs['attention_mask'])
                else:
                    pooled_embeddings = self._mean_pooling(outputs.last_hidden_state, inputs['attention_mask'])
                
                normalized_embeddings = torch.nn.functional.normalize(pooled_embeddings, p=2, dim=1)
                # FIN: CAMBIOS SOLICITADOS EN embed_texts_batch (pooling y normalización)

                return normalized_embeddings.cpu().numpy()

            result_list = await asyncio.to_thread(sync_inference)
            
            # Liberación de memoria (se hace después de que el thread termine)
            gc.collect()
            if 'cuda' in str(self.device):
                torch.cuda.empty_cache()

            return [res_arr for res_arr in result_list]

        except Exception as e:
            logger.exception(f"Error en embed_texts_batch: {e}")
            return [None] * len(texts)

    async def embed_text(self, text: str) -> Optional[np.ndarray]:
        """Embebe un solo texto (query) usando el método de lote."""
        if not text:
            return None
        # La nueva firma de embed_text ahora espera una lista
        results = await self.embed_texts_batch([text], is_query=True)
        return results[0] if results else None
        
    @profile
    @torch.no_grad()
    async def embed_chunks_batch(
        self,
        segments_with_boundaries: List[Tuple[str, List[Tuple[int, int, str]], Optional[str]]]
    ) -> List[List[Optional[np.ndarray]]]:
        """
        Procesa un lote de segmentos de texto y sus chunks en una sola pasada de inferencia.
        """
        if not segments_with_boundaries:
            return []

        # 1. Preparar todos los textos que irán al modelo en una sola lista.
        all_model_inputs_texts = []
        all_char_spans_in_segments = []
        all_prefix_lengths = []

        for original_segment_text, chunk_boundaries, auto_context_header in segments_with_boundaries:
            prefix_parts = []
            if self.passage_instruction:
                prefix_parts.append(self.passage_instruction)
            if auto_context_header:
                prefix_parts.append(auto_context_header)
            
            full_prefix = "".join(prefix_parts)
            if full_prefix and not full_prefix.endswith(" "):
                 full_prefix += " "
            
            text_for_model = full_prefix + original_segment_text
            prefix_len = len(full_prefix)
            
            all_model_inputs_texts.append(text_for_model)
            all_char_spans_in_segments.append([(start, end) for start, end, _ in chunk_boundaries])
            all_prefix_lengths.append(prefix_len)

        # 2. ÚNICA llamada de tokenización e inferencia para todo el lote de segmentos.
        try:
            def sync_batch_inference():
                model_inputs = self.tokenizer(
                    all_model_inputs_texts, return_tensors='pt', padding=True,
                    truncation=True, max_length=self.max_length
                ).to(self.device)
                
                outputs = self.model(**model_inputs)
                token_embeddings_batch = outputs.last_hidden_state.cpu()
                attention_mask_batch = model_inputs['attention_mask'].cpu()
                return token_embeddings_batch, attention_mask_batch

            token_embeddings_batch, attention_mask_batch = await asyncio.to_thread(sync_batch_inference)
        except Exception as e:
            logger.exception(f"Error en la inferencia por lotes para Late Chunking: {e}")
            return [[None] * len(b) for _, b, _ in segments_with_boundaries]

        # 3. Mapeo y pooling en CPU (rápido) para cada segmento.
        final_batch_embeddings = []
        for i in range(len(segments_with_boundaries)):
            text_that_went_to_tokenizer = all_model_inputs_texts[i]
            char_spans_for_segment = all_char_spans_in_segments[i]
            prefix_len = all_prefix_lengths[i]
            
            token_embeddings_for_segment = token_embeddings_batch[i]
            attention_mask_for_segment = attention_mask_batch[i]
            effective_seq_len = attention_mask_for_segment.sum().item()

            token_spans_ideal = self._get_token_indices_for_char_spans(
                text_that_went_to_tokenizer,
                char_spans_for_segment,
                prefix_len
            )
            
            chunk_embeddings_for_segment = []
            for ideal_span in token_spans_ideal:
                if ideal_span is None:
                    chunk_embeddings_for_segment.append(None)
                    continue
                
                start_token, end_token = ideal_span
                final_start = min(start_token, effective_seq_len - 1)
                final_end = min(end_token, effective_seq_len)

                if final_start >= final_end:
                    chunk_embeddings_for_segment.append(None)
                    continue
                
                chunk_token_embeddings = token_embeddings_for_segment[final_start:final_end]
                if chunk_token_embeddings.shape[0] > 0:
                    pooled_embedding = torch.mean(chunk_token_embeddings.float(), dim=0)
                    chunk_embeddings_for_segment.append(pooled_embedding.detach().cpu().numpy())
                else:
                    chunk_embeddings_for_segment.append(None)
            
            final_batch_embeddings.append(chunk_embeddings_for_segment)
        
        # Liberar memoria
        del token_embeddings_batch, attention_mask_batch
        if 'cuda' in str(self.device):
            torch.cuda.empty_cache()
        gc.collect()
        
        return final_batch_embeddings

    # INICIO: NUEVO MÉTODO _last_token_pool
    def _last_token_pool(self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """
        Realiza el pooling del estado oculto del último token. Asume padding a la derecha.
        El último token es el que está justo antes del primer token de padding.
        """
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_size = last_hidden_state.shape[0]
        batch_indices = torch.arange(batch_size, device=last_hidden_state.device)
        return last_hidden_state[batch_indices, sequence_lengths]
    # FIN: NUEVO MÉTODO _last_token_pool

    def _mean_pooling(self, last_hidden_state, attention_mask):
        # La llamada ahora pasa 'last_hidden_state' directamente
        token_embeddings = last_hidden_state 
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask


---
File: /ingestion/kg_updater.py
---

from typing import List, Dict, Any, Optional
import asyncio
from ..storage.base import BaseKnowledgeGraph # Usar interfaz
from ..utils.logging_setup import logger
from ..utils.helpers import GRAPH_FIELD_SEP # Separador para source_ids

class KGUpdater:
    def __init__(self, kg_graph: BaseKnowledgeGraph):
        self.kg = kg_graph
        logger.info("KGUpdater inicializado.")

    async def _add_or_update_entity(self, entity_data: Dict[str, Any], source_id: str, timestamp: Optional[str]):
        entity_name = entity_data.get("name")
        entity_type = entity_data.get("type", "UNKNOWN")
        if not entity_name: return

        properties = {
            "type": entity_type,
            "description": entity_data.get("description", ""), # Podría venir del DescriptionGenerator
            # "source_offsets": json.dumps(entity_data.get("source_offsets", [])) # Guardar offsets si existen
            "last_mentioned_ts": timestamp,
            "verified": entity_data.get("verified", True) # Asumir verificado si no se indica
        }

        existing_node = await self.kg.get_node(entity_name)
        if existing_node:
            # Actualizar: fusionar source_ids, mantener última timestamp, actualizar si verificado
            existing_sources = set(existing_node.get("source_id", "").split(GRAPH_FIELD_SEP))
            existing_sources.add(source_id)
            properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, existing_sources))
            # Actualizar descripción si la nueva es más completa? O fusionar? Por ahora, simple update.
            # properties["description"] = merge_descriptions(existing_node.get("description"), properties["description"])
            # Actualizar 'verified' solo si la nueva es True
            properties["verified"] = existing_node.get("verified", False) or properties["verified"]
            # Mantener la TS más reciente
            properties["last_mentioned_ts"] = max(existing_node.get("last_mentioned_ts", ""), timestamp or "")

            await self.kg.update_node_properties(entity_name, properties)
        else:
            # Añadir nuevo
            properties["source_id"] = source_id
            await self.kg.add_node(entity_name, entity_type, properties)

    async def _add_or_update_relation(self, relation_data: Dict[str, Any], source_id: str, timestamp: Optional[str]):
        source = relation_data.get("source")
        target = relation_data.get("target")
        label = relation_data.get("label", "RELATED_TO")
        if not source or not target: return

        properties = {
            "description": relation_data.get("description", ""),
            # "source_offsets": json.dumps(relation_data.get("source_offsets", [])),
            "last_mentioned_ts": timestamp,
            "weight": relation_data.get("weight", 1.0), # Podría venir del LLM
            "verified": relation_data.get("verified", True)
        }

        # Asegurar que nodos existan (KG base debería manejarlosi no, pero buena práctica)
        if not await self.kg.has_node(source): await self._add_or_update_entity({"name": source}, source_id, timestamp)
        if not await self.kg.has_node(target): await self._add_or_update_entity({"name": target}, source_id, timestamp)

        existing_edge = await self.kg.get_edge(source, target, label) # Asumir que get_edge puede filtrar por label
        if existing_edge:
            # Actualizar: fusionar source_ids, sumar pesos? mantener última TS
            existing_sources = set(existing_edge.get("source_id", "").split(GRAPH_FIELD_SEP))
            existing_sources.add(source_id)
            properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, existing_sources))
            # properties["weight"] = existing_edge.get("weight", 0.0) + properties["weight"] # O promedio?
            properties["verified"] = existing_edge.get("verified", False) or properties["verified"]
            properties["last_mentioned_ts"] = max(existing_edge.get("last_mentioned_ts", ""), timestamp or "")

            await self.kg.update_edge_properties(source, target, label, properties) # Asumir método
        else:
            properties["source_id"] = source_id
            await self.kg.add_edge(source, target, label, properties)

    async def update(self, entities: List[Dict[str, Any]], relations: List[Dict[str, Any]], source_id: str, metadata: Dict[str, Any]):
        """Actualiza el KG con entidades y relaciones verificadas de una ventana."""
        timestamp = metadata.get("timestamp") # Obtener timestamp de la ventana
        tasks = []

        # Filtrar entidades/relaciones verificadas (si el campo existe)
        # verified_entities = [e for e in entities if e.get("verified", True)]
        # verified_relations = [r for r in relations if r.get("verified", True)]
        # Por ahora, procesamos todo lo que venga de la descripción verificada
        verified_entities = entities
        verified_relations = relations


        logger.debug(f"Actualizando KG desde {source_id} con {len(verified_entities)} entidades y {len(verified_relations)} relaciones.")

        for entity_data in verified_entities:
            tasks.append(self._add_or_update_entity(entity_data, source_id, timestamp))

        for relation_data in verified_relations:
            tasks.append(self._add_or_update_relation(relation_data, source_id, timestamp))

        # Ejecutar en paralelo
        await asyncio.gather(*tasks)
        logger.debug(f"Actualización KG para {source_id} completada.")


---
File: /ingestion/lsc_divider.py
---

# --- START OF NEW FILE ares/ingestion/lsc_divider.py ---
import re
from typing import List, Tuple
from ..utils.logging_setup import logger

class LSCDivider:
    """
    Divide un texto de ventana en "Secciones de Contexto de Segmentación Tardía (LSC)".
    Esta es una implementación inicial y ligera, no usa LLM para definir límites.
    """
    def __init__(self, config: dict):
        lsc_config = config.get("ingestion", {}).get("lsc_divider_config", {})
        self.min_lines_per_section = lsc_config.get("min_lines_per_section", 10)
        self.max_lines_per_section = lsc_config.get("max_lines_per_section", 50)
        # Podríamos usar Chonkie aquí también con una configuración para "secciones grandes"
        # Por ahora, una lógica simple basada en párrafos/líneas.
        logger.info(f"LSCDivider inicializado (min_lines: {self.min_lines_per_section}, max_lines: {self.max_lines_per_section}).")

    def define_section_boundaries(self, window_text: str) -> List[Tuple[int, int]]:
        """
        Define los límites de las Secciones-LSC (start_char, end_char) relativos a window_text.
        Implementación simple: agrupa párrafos o un número de líneas.
        """
        if not window_text.strip():
            return []

        lines = window_text.splitlines()
        if not lines:
            return []

        boundaries: List[Tuple[int, int]] = []
        current_section_start_char = 0
        current_section_line_count = 0
        current_section_char_len = 0

        # Intenta agrupar por párrafos (bloques separados por una o más líneas vacías)
        # Esto es una heurística, se puede mejorar.
        paragraphs = re.split(r'\n\s*\n', window_text) # Divide por una o más líneas en blanco
        
        current_char_offset = 0
        current_paragraph_lines = []
        
        for para_idx, paragraph_text_raw in enumerate(paragraphs):
            paragraph_text = paragraph_text_raw.strip()
            if not paragraph_text:
                # Avanzar el offset por las líneas vacías entre párrafos
                if para_idx > 0: # Si no es el primer párrafo, hubo un \n\s*\n antes
                    # Contar cuántos newlines había en el separador.
                    # Esto es complejo porque el re.split consume el delimitador.
                    # Solución más simple: si el párrafo es vacío, es probable que fuera un separador.
                    # Asumimos que cada separador `\n\s*\n` es al menos un `\n`.
                    # Si el `paragraph_text_raw` original era solo whitespace (ej. "\n  \n"),
                    # su longitud original se suma al offset.
                    current_char_offset += len(paragraph_text_raw) +1 # +1 por el \n que re.split consume
                continue

            para_lines = paragraph_text.splitlines()
            
            # Si añadir este párrafo excede max_lines_per_section Y ya tenemos suficientes líneas para una sección
            if current_paragraph_lines and \
               (current_section_line_count + len(para_lines) > self.max_lines_per_section) and \
               current_section_line_count >= self.min_lines_per_section:
                
                # Finalizar sección actual
                section_end_char = current_char_offset -1 # -1 para quitar el último \n
                boundaries.append((current_section_start_char, section_end_char))
                
                # Iniciar nueva sección
                current_section_start_char = current_char_offset
                current_paragraph_lines = []
                current_section_line_count = 0

            current_paragraph_lines.extend(para_lines)
            current_section_line_count += len(para_lines)
            
            # Actualizar offset de caracteres
            current_char_offset += len(paragraph_text_raw) +1 # +1 por el \n que re.split puede consumir o que unía párrafos

        # Añadir la última sección si queda contenido
        if current_paragraph_lines:
            # El end_char es el final del window_text
            boundaries.append((current_section_start_char, len(window_text)))
            
        # Si la división por párrafos no produjo nada o solo una sección muy grande,
        # podríamos recurrir a una división por N líneas como fallback.
        # Por ahora, mantenemos la división por párrafos.
        if not boundaries and window_text: # Si no se crearon boundaries pero hay texto
             boundaries.append((0, len(window_text)))

        logger.debug(f"LSCDivider definió {len(boundaries)} boundaries de sección.")
        return boundaries

# --- END OF NEW FILE ares/ingestion/lsc_divider.py ---


---
File: /ingestion/normalizer.py
---

import re
import unicodedata
from typing import List

class Normalizer:
    """Limpia y normaliza texto de conversaciones."""

    @staticmethod
    def normalize_whitespace(text: str) -> str:
        """Reemplaza múltiples espacios/saltos de línea con uno solo."""
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

    @staticmethod
    def remove_control_characters(text: str) -> str:
        """Elimina caracteres de control excepto saltos de línea y tabulaciones."""
        return "".join(ch for ch in text if unicodedata.category(ch)[0]!="C" or ch in ('\n', '\t'))

    # Añadir más métodos según sea necesario (ej. manejo de URLs, emojis a texto, etc.)
    # def normalize_emojis(text: str) -> str: ...
    # def normalize_urls(text: str) -> str: ...

    @classmethod
    def normalize(cls, text: str) -> str:
        """Aplica todas las normalizaciones estándar."""
        if not isinstance(text, str):
            return "" # O lanzar error
        text = cls.remove_control_characters(text)
        text = cls.normalize_whitespace(text)
        # Añadir otras llamadas aquí:
        # text = cls.normalize_emojis(text)
        # text = cls.normalize_urls(text)
        return text

    @classmethod
    def normalize_batch(cls, texts: List[str]) -> List[str]:
        """Normaliza un lote de textos."""
        return [cls.normalize(text) for text in texts]


---
File: /ingestion/pipeline.py
---

# ares/ingestion/pipeline.py
import asyncio
import time
import numpy as np
from datetime import datetime, timezone
import dataclasses
from typing import List, Dict, Any, Tuple, Optional
from tqdm.asyncio import tqdm
from memory_profiler import profile

# Componentes de ARES
from .normalizer import Normalizer
from .chunker import ChonkieChunkerWrapper
from .embedder import LateChunkingEmbedder
from .embedder_sparse import FastEmbedSparseEmbedder # <<< NUEVO
from .description_generator import DescriptionGenerator
from .verifier import Verifier
from .kg_updater import KGUpdater
from .lsc_divider import LSCDivider
from ..storage.base import BaseKVStore, BaseVectorDB, BaseKnowledgeGraph
# from ..storage.bm25.whoosh_bm25 import WhooshBM25Index # <<< NO USADO
from ..core.datatypes import Chunk, Description, LSCSection
from ..llm.base import BaseLLMWrapper
from ..utils.logging_setup import logger
from ..utils.helpers import compute_mdhash_id, get_config_value

@dataclasses.dataclass
class _WindowProcessingArtifacts:
    """Estructura interna para devolver los artefactos de una sola ventana procesada."""
    window_id: str
    kv_items: Dict[str, Dict[str, Any]] = dataclasses.field(default_factory=dict)
    vdb_chunks: List[Chunk] = dataclasses.field(default_factory=list)
    # bm25_docs: List[Tuple[str, str]] = dataclasses.field(default_factory=list) # <<< ELIMINADO
    window_meta: Optional[Dict[str, Any]] = None
    kg_entities: Optional[List[Dict[str, Any]]] = None
    kg_relations: Optional[List[Dict[str, Any]]] = None
    success: bool = False
    status_message: str = "No procesado"

class IngestionPipeline:
    def __init__(self,
                 config: Dict[str, Any],
                 chunk_store: BaseKVStore,
                 vector_db: BaseVectorDB,
                 description_store: BaseKVStore,
                 kg_graph: Optional[BaseKnowledgeGraph],
                 bm25_index: Optional[Any], # Mantenido por compatibilidad de firma, pero no se usará
                 embedder: LateChunkingEmbedder,
                 sparse_embedder: Optional[FastEmbedSparseEmbedder], # <<< NUEVO
                 description_llm: Optional[BaseLLMWrapper], 
                 verifier: Optional[Verifier],
                 section_enricher_llm: Optional[BaseLLMWrapper]
                 ):
        self.config = config
        self.ingestion_cfg = config.get('ingestion', {})
        self.chunk_store = chunk_store
        self.vector_db = vector_db
        self.description_store = description_store
        self.kg_graph = kg_graph
        self.bm25_index = None # Forzado a None para evitar uso accidental
        self.embedder = embedder
        self.sparse_embedder = sparse_embedder # <<< NUEVO
        self.verifier = verifier
        self.section_enricher_llm = section_enricher_llm
        self.normalizer = Normalizer()
        
        tokenizer_name = get_config_value(config, 'embeddings.model_name', 'jinaai/jina-embeddings-v3')
        self.chunker = ChonkieChunkerWrapper(
            strategy=self.ingestion_cfg.get('chunking_strategy', 'recursive'),
            config=self.ingestion_cfg.get('chonkie_config', {}),
            tokenizer_name_or_path=tokenizer_name
        )
        
        self.window_description_generator = DescriptionGenerator(llm_wrapper=description_llm, config=config) if description_llm else None
        self.generate_window_descriptions = self.ingestion_cfg.get('generate_descriptions', False) and self.window_description_generator is not None
        self.use_verifier_for_window_desc = self.ingestion_cfg.get("use_verifier", False) and self.verifier is not None
        
        self.lsc_divider = LSCDivider(config=config) if self.ingestion_cfg.get("use_lsc", False) else None
        self.enrich_lsc_sections_with_llm = self.lsc_divider is not None and self.section_enricher_llm is not None
        
        self.use_autocontent_headers_for_chunks = self.ingestion_cfg.get("use_autocontent_headers_for_chunks", False)
        
        self.update_kg = self.ingestion_cfg.get('update_kg', False) and self.kg_graph is not None
        self.kg_updater = KGUpdater(self.kg_graph) if self.update_kg else None
        
        logger.info("IngestionPipeline (Batch-Enabled) inicializado.")
        logger.info(f"Config Ingesta: Desc Ventana={self.generate_window_descriptions} (Verif: {self.use_verifier_for_window_desc}), "
                    f"LSC Habilitado={self.lsc_divider is not None} (Enriquecer LSC con LLM: {self.enrich_lsc_sections_with_llm}), "
                    f"AutoContext Headers para Chunks={self.use_autocontent_headers_for_chunks}, "
                    f"Actualizar KG={self.update_kg}, Indexar Disperso={'Sí' if self.sparse_embedder else 'No'}")

    async def process_window(self, window_id: str, conversation_text: str, metadata: Dict[str, Any] = {}) -> bool:
        """
        Wrapper para procesar una única ventana, manteniendo la retrocompatibilidad.
        """
        logger.info(f"Procesando ventana única '{window_id}' a través del endpoint de lote.")
        results = await self.process_windows_batch([(window_id, conversation_text, metadata)])
        return results.get(window_id, False)

    @profile
    async def process_windows_batch(self, windows: List[Tuple[str, str, Dict[str, Any]]]) -> Dict[str, bool]:
        """
        Procesa un lote de ventanas de texto, realizando operaciones costosas en paralelo.
        """
        if not windows: return {}
        
        # --- INICIO DE CAMBIOS: Métricas de Rendimiento ---
        batch_start_time = time.perf_counter()
        timings = {}
        logger.info(f"Iniciando ingesta por lotes para {len(windows)} ventanas.")
        # --- FIN DE CAMBIOS ---
        
        final_results_status: Dict[str, bool] = {wid: False for wid, _, _ in windows}

        # 1. Normalización
        normalized_windows = [(wid, self.normalizer.normalize(txt), meta) for wid, txt, meta in windows if txt.strip()]
        if not normalized_windows:
            logger.warning("No hay ventanas con contenido después de la normalización. Terminando el lote.")
            return final_results_status
        timings["1_normalization"] = time.perf_counter() - batch_start_time

        # 2. Generación y Verificación de Descripciones (Llamadas LLM en Lote)
        desc_start = time.perf_counter()
        descriptions_map = await self._generate_and_verify_descriptions_batch(normalized_windows)
        timings["2_descriptions_and_verification"] = time.perf_counter() - desc_start

        # 3. Creación de Chunks y Generación de Embeddings
        # Se mide desde la preparación de los textos hasta la obtención de los embeddings.
        embed_start = time.perf_counter()
        all_chunks: List[Chunk] = []
        all_texts_for_embedding: List[str] = []
        window_to_chunks_map: Dict[str, List[Chunk]] = {wid: [] for wid, _, _ in normalized_windows}
        window_to_meta_map: Dict[str, Dict[str, Any]] = {wid: meta for wid, _, meta in normalized_windows}

        for win_id, text, meta in normalized_windows:
            desc_obj = descriptions_map.get(win_id)
            lsc_sections = await self._get_lsc_sections(text, win_id, desc_obj, meta)
            
            chunk_idx_in_window = 0
            for lsc_section in lsc_sections:
                header = self._build_autocontent_header(desc_obj, lsc_section)
                boundaries = self.chunker.chunk(lsc_section.text)
                for j, (_, _, chunk_text_only) in enumerate(boundaries):
                    chunk_id = f"cnk-{lsc_section.id}-{j}-{compute_mdhash_id(chunk_text_only)[:8]}"
                    
                    chunk_meta = {**meta, "window_id": win_id, "lsc_section_id": lsc_section.id, "chunk_index_in_window": chunk_idx_in_window, "type": "chunk"}
                    
                    if self.use_autocontent_headers_for_chunks:
                         full_text_for_embedding = header + chunk_text_only
                         if desc_obj and desc_obj.metadata.get("title"): chunk_meta["doc_title_for_autocont"] = desc_obj.metadata["title"]
                         if desc_obj and desc_obj.objective_summary: chunk_meta["doc_objective_summary_for_autocont"] = desc_obj.objective_summary
                         if lsc_section.title: chunk_meta["lsc_section_title"] = lsc_section.title
                         if lsc_section.objective_summary: chunk_meta["lsc_section_objective_summary"] = lsc_section.objective_summary
                    else:
                         full_text_for_embedding = chunk_text_only

                    token_count = len(self.embedder.tokenizer.encode(chunk_text_only, add_special_tokens=False))
                    
                    chunk_obj = Chunk(id=chunk_id, text=chunk_text_only, embedding=None, metadata=chunk_meta, token_count=token_count)
                    all_chunks.append(chunk_obj)
                    all_texts_for_embedding.append(full_text_for_embedding)
                    window_to_chunks_map[win_id].append(chunk_obj)
                    chunk_idx_in_window += 1

        if not all_chunks:
            logger.warning("No se generaron chunks para ninguna ventana en el lote. Almacenando solo descripciones si existen.")
            # ... (código para manejar este caso)
        else:
            logger.info(f"Generando embeddings para {len(all_chunks)} chunks en un solo lote.")
            dense_task = self.embedder.embed_text(all_texts_for_embedding)
            sparse_task = self.sparse_embedder.embed_sparse_batch(all_texts_for_embedding) if self.sparse_embedder else asyncio.sleep(0, result=[None]*len(all_chunks))
            dense_embeddings, sparse_embeddings_data = await asyncio.gather(dense_task, sparse_task)
            
            chunks_with_dense_embeddings: List[Chunk] = []
            for i, chunk in enumerate(all_chunks):
                if dense_embeddings[i] is not None:
                    chunk.embedding = dense_embeddings[i]
                    chunks_with_dense_embeddings.append(chunk)

            sparse_vectors_map = {
                chunk.id: sparse_data 
                for chunk, sparse_data in zip(chunks_with_dense_embeddings, sparse_embeddings_data) 
                if sparse_data
            }

        timings["3_embedding_generation"] = time.perf_counter() - embed_start

        # 4. Agregación y Almacenamiento
        store_start = time.perf_counter()
        
        all_artifacts: List[_WindowProcessingArtifacts] = []
        for win_id, chunks_in_window in window_to_chunks_map.items():
            artifacts = _WindowProcessingArtifacts(window_id=win_id)
            desc = descriptions_map.get(win_id)
            
            valid_chunks = [c for c in chunks_in_window if c.embedding is not None]
            
            if not valid_chunks and not (desc and getattr(desc, 'is_verified', True)):
                artifacts.status_message = "Fallo: No se generaron chunks válidos ni descripción."
                all_artifacts.append(artifacts)
                continue

            artifacts.vdb_chunks = valid_chunks
            artifacts.kv_items = {
                c.id: {"text": c.text, "metadata": c.metadata, "token_count": c.token_count}
                for c in valid_chunks
            }

            if desc and getattr(desc, 'is_verified', True):
                 artifacts.kg_entities = getattr(desc, 'entities', None)
                 artifacts.kg_relations = getattr(desc, 'relations', None)
                 artifacts.window_meta = {"window_description_id": desc.window_id, **window_to_meta_map.get(win_id, {})}
            
            artifacts.success = True
            artifacts.status_message = f"Éxito: {len(valid_chunks)} chunks procesados."
            all_artifacts.append(artifacts)
            final_results_status[win_id] = True

        # El sparse_vectors_map debe estar disponible incluso si no hay chunks
        await self._store_artifacts_batch(all_artifacts, locals().get('sparse_vectors_map', {}))
        timings["4_storage"] = time.perf_counter() - store_start

        # --- INICIO DE CAMBIOS: Registro de Métricas ---
        total_time = time.perf_counter() - batch_start_time
        timings["total_ingestion_time"] = total_time

        timing_summary = " | ".join([f"{key}: {value:.2f}s" for key, value in sorted(timings.items())])
        logger.info(f"Métricas de rendimiento para la ingesta del lote: {timing_summary}")
        # --- FIN DE CAMBIOS ---
        
        return final_results_status

    async def _generate_and_verify_descriptions_batch(self, windows: List[Tuple[str, str, Dict]]) -> Dict[str, Optional[Description]]:
        """Genera y verifica descripciones en paralelo."""
        if not self.generate_window_descriptions: return {w[0]: None for w in windows}
        
        # Generación
        logger.info(f"Generando {len(windows)} descripciones de ventana en un lote LLM.")
        desc_prompts = [self.window_description_generator.prompt_template.format(text=txt) for _, txt, _ in windows]
        raw_responses = await self.window_description_generator.llm_wrapper.generate_batch(desc_prompts)
        
        parsed_descs = {}
        for i, raw_resp in enumerate(raw_responses):
            win_id, _, meta = windows[i]
            desc = self.window_description_generator.generate_from_llm_output(raw_resp, win_id, meta)
            if desc: parsed_descs[win_id] = desc
        
        # Verificación
        if not self.use_verifier_for_window_desc:
            await self._store_and_embed_descriptions_batch(list(parsed_descs.values()))
            return parsed_descs
        
        logger.info(f"Verificando {len(parsed_descs)} descripciones generadas.")
        verify_tasks = {
            wid: self.verifier.verify(desc, next(w[1] for w in windows if w[0] == wid))
            for wid, desc in parsed_descs.items()
        }
        
        verified_results_list = await asyncio.gather(*verify_tasks.values())
        verified_results = dict(zip(verify_tasks.keys(), verified_results_list))
        
        # Almacenamiento de descripciones
        descs_to_store = [d for d in verified_results.values() if d and d.is_verified]
        await self._store_and_embed_descriptions_batch(descs_to_store)
        
        return verified_results

    async def _store_artifacts_batch(self, all_artifacts: List[_WindowProcessingArtifacts], sparse_vectors_map: Dict[str, Any]):
        """Almacena todos los artefactos generados en lote."""
        all_kv = {}
        all_vdb = []
        all_kg_tasks = []

        for artifacts in all_artifacts:
            if not artifacts.success:
                continue

            all_kv.update(artifacts.kv_items)
            if artifacts.window_meta:
                all_kv[f"win_meta_{artifacts.window_id}"] = {**artifacts.window_meta, "chunk_ids": [c.id for c in artifacts.vdb_chunks]}
            
            all_vdb.extend(artifacts.vdb_chunks)

            if self.update_kg and self.kg_updater and artifacts.kg_entities and artifacts.window_meta:
                task = self.kg_updater.update(artifacts.kg_entities, artifacts.kg_relations, artifacts.window_id, artifacts.window_meta)
                all_kg_tasks.append(task)
        
        # Ejecutar todas las operaciones de almacenamiento en paralelo
        tasks = []
        if all_kv: tasks.append(self.chunk_store.set_batch(all_kv))
        if all_vdb: tasks.append(self.vector_db.add_vectors(all_vdb, sparse_vectors_map=sparse_vectors_map))
        tasks.extend(all_kg_tasks)
        
        if tasks:
            await tqdm.gather(*tasks, desc="Guardando Artefactos en Bases de Datos")

    # --- Métodos Helper Conservados y Adaptados ---

    async def _store_and_embed_descriptions_batch(self, descriptions: List[Description]):
        """Almacena descripciones en KV, luego las embebe y guarda en VectorDB."""
        if not descriptions:
            return

        # 1. Almacenar en KV Store (concurrente)
        kv_tasks = [self.description_store.set(d.window_id, dataclasses.asdict(d)) for d in descriptions]
        await asyncio.gather(*kv_tasks)
        logger.info(f"Guardadas {len(descriptions)} descripciones en DescriptionStore (KV).")

        # 2. Preparar para embedding
        texts_to_embed, vdb_map = [], []
        for desc in descriptions:
            text = f"{desc.objective_summary or ''} {desc.subjective_interpretation or ''} Keywords: {', '.join(desc.keywords or [])}".strip()
            if text:
                texts_to_embed.append(text)
                meta = {**desc.metadata, "type": "description", "window_id": desc.window_id}
                meta.pop("entities", None); meta.pop("relations", None) # Evitar payloads grandes
                vdb_map.append({'id': desc.window_id, 'text': "", 'metadata': meta})
        
        if not texts_to_embed: return

        # 3. Embedder en lote
        embeddings = await self.embedder.embed_text(texts_to_embed)

        # 4. Preparar y guardar en VectorDB
        vdb_chunks = [Chunk(id=vdb_map[i]['id'], text=vdb_map[i]['text'], embedding=emb, metadata=vdb_map[i]['metadata'])
                      for i, emb in enumerate(embeddings) if emb is not None]

        if vdb_chunks:
            # Las descripciones no tienen vectores dispersos.
            await self.vector_db.add_vectors(vdb_chunks)
            logger.info(f"Guardados {len(vdb_chunks)} embeddings de descripción en VectorDB.")

    async def _get_lsc_sections(self, text: str, win_id: str, desc_obj: Optional[Description], meta: Dict) -> List[LSCSection]:
        """Divide el texto en secciones LSC, enriqueciéndolas si está configurado."""
        doc_title = (desc_obj.metadata.get("title") if desc_obj else meta.get("title", win_id))
        
        if self.lsc_divider:
            lsc_sections = []
            boundaries = self.lsc_divider.define_section_boundaries(text)
            for i, (start, end) in enumerate(boundaries):
                lsc_text = text[start:end]
                if not lsc_text.strip(): continue
                lsc_id = f"{win_id}-lsc{i}"
                base_title = f"Sección {i+1} de '{doc_title}'"
                title, obj_sum, subj_int = base_title, None, None
                
                if self.enrich_lsc_sections_with_llm:
                    title, obj_sum, subj_int = await self._enrich_lsc_section(lsc_text, lsc_id, base_title)
                
                lsc_sections.append(LSCSection(id=lsc_id, window_id=win_id, text=lsc_text, title=title,
                                               objective_summary=obj_sum, subjective_interpretation=subj_int,
                                               metadata={"start_char_in_window": start, "end_char_in_window": end}))
            if lsc_sections: return lsc_sections

        # Fallback: si LSC está desactivado o no produce secciones, usar la ventana completa
        return [LSCSection(id=f"{win_id}-lsc0", window_id=win_id, text=text, title=doc_title,
                           objective_summary=desc_obj.objective_summary if desc_obj else None)]

    async def _enrich_lsc_section(self, lsc_text: str, lsc_id: str, base_title: str) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """Llama al LLM para generar título y resúmenes para una Sección LSC."""
        if not self.section_enricher_llm or not self.window_description_generator:
            return base_title, None, None
        
        logger.debug(f"Enriqueciendo Sección-LSC {lsc_id} con LLM...")
        try:
            # Reutiliza el generador de descripciones para obtener títulos y resúmenes para la sección
            desc = await self.window_description_generator.generate(lsc_text, lsc_id, {"title": base_title})
            if desc:
                return (desc.metadata.get("title", base_title), desc.objective_summary, desc.subjective_interpretation)
            return base_title, None, None
        except Exception as e:
            logger.error(f"Error enriqueciendo sección LSC {lsc_id} con LLM: {e}")
            return base_title, None, None
            
    def _build_autocontent_header(self, desc_obj: Optional[Description], lsc_section: LSCSection) -> str:
        """Construye la cabecera de contexto automático para un chunk."""
        if not self.use_autocontent_headers_for_chunks:
            return ""
        
        parts = []
        if desc_obj and desc_obj.metadata.get("title"):
            parts.append(f"Título del Documento: {desc_obj.metadata['title']}")
        if desc_obj and desc_obj.objective_summary:
            parts.append(f"Resumen del Documento: {desc_obj.objective_summary}")
        if lsc_section.title and "Sección" in lsc_section.title: # Evitar títulos genéricos
            parts.append(f"Título de Sección: {lsc_section.title}")
        if lsc_section.objective_summary:
            parts.append(f"Resumen de Sección: {lsc_section.objective_summary}")
        if lsc_section.subjective_interpretation:
            parts.append(f"Interpretación de Sección: {lsc_section.subjective_interpretation}")

        if not parts:
            return ""
        
        return "Contexto General: " + ". ".join(parts) + ". Contenido Específico del Fragmento: "


---
File: /ingestion/verifier.py
---

# ares/ingestion/verifier.py
import json
import re
from typing import Dict, Any, Optional, List # Añadir List ya estaba
from ..llm.base import BaseLLMWrapper
from ..core.datatypes import Description
from ..utils.logging_setup import logger
from datetime import datetime, date # Para el serializer
import numpy as np # Para el serializer
from ..utils.helpers import log_or_save_raw_json # Importar el helper
from ..core.config_loader import get_config_value # Para obtener working_dir

VERIFIER_SYSTEM_PROMPT_V3 = """
Eres un verificador de datos extremadamente riguroso. Tu tarea es validar la información extraída por otro LLM comparándola estrictamente con el texto fuente original. NO añadas información externa.

Texto Fuente Original:
```
{source_text}
```
------
Información Extraída (JSON):
```json
{extracted_json}
```
------
Instrucciones de Verificación:
1.  **Resumen Objetivo:** ¿El `objective_summary` refleja SOLO hechos EXPLÍCITOS del texto fuente? Responde `true` o `false`.
2.  **Entidades:** Para CADA entidad en `entities`: ¿Existe EVIDENCIA textual DIRECTA para el nombre (`name`) y tipo (`type`) en el texto fuente? Si la lista `entities` está vacía y es correcto que no haya entidades en el texto fuente, esto cuenta como una verificación exitosa para las entidades. Responde `true` o `false` para cada entidad presente, o una lista vacía `[]` si no hay entidades y eso es correcto.
3.  **Relaciones:** Para CADA relación en `relations`: ¿Existe EVIDENCIA textual DIRECTA que soporte la conexión (`label`) entre `source` y `target`? Si la lista `relations` está vacía y es correcto que no haya relaciones en el texto fuente, esto cuenta como una verificación exitosa para las relaciones. Responde `true` o `false` para cada relación presente, o una lista vacía `[]` si no hay relaciones y eso es correcto.

Formato de Salida:
Devuelve ÚNICAMENTE un objeto JSON válido y nada más...
```json
{{
  "objective_summary_verified": true | false,
  "entities_verified": [], // o [true, false, ...]
  "relations_verified": [], // o [true, false, ...]
  "verification_passed": true | false, // Debe ser true si todo es correcto, incluyendo listas vacías justificadas
  "reasoning": "Breve explicación si algo falla (máx 1 frase), o null si todo ok."
}}
```
SOLO EL OBJETO JSON.
"""

class Verifier:
    def __init__(self, llm_wrapper: BaseLLMWrapper, config: Optional[Dict[str, Any]] = None):
        self.llm = llm_wrapper
        self.prompt_template = VERIFIER_SYSTEM_PROMPT_V3
        self.config = config
        logger.info("Verifier (Dual-Pass) inicializado con PROMPT_V3.")

    async def verify(self, description: Description, source_text: str) -> Description:
        logger.debug(f"Verificando descripción para ventana {description.window_id}...")
        desc_dict = description.__dict__.copy()
        # Excluir campos que no deben ser parte del JSON enviado al LLM para verificación
        desc_dict.pop('window_id', None)
        desc_dict.pop('metadata', None)
        desc_dict.pop('is_verified', None) # Este estado lo determinará el Verifier

        # Serializador personalizado para tipos de datos no estándar en JSON
        def default_serializer(obj):
            if isinstance(obj, (datetime, date)): return obj.isoformat()
            if isinstance(obj, np.ndarray): return obj.tolist()
            if isinstance(obj, (np.floating, float)): return float(obj)
            if isinstance(obj, (np.integer, int)): return int(obj)
            if isinstance(obj, (np.complexfloating, complex)): return {'real': obj.real, 'imag': obj.imag}
            if isinstance(obj, (np.bool_)): return bool(obj)
            if isinstance(obj, (np.void)): return None # Manejo de np.void como null
            raise TypeError(f"Object of type {obj.__class__.__name__} is not JSON serializable")

        try:
            extracted_json_str = json.dumps(desc_dict, default=default_serializer, indent=2)
        except TypeError as e:
            logger.error(f"Error serializando descripción para Verifier en {description.window_id}: {e}")
            description.is_verified = False
            return description

        prompt = self.prompt_template.format(
            source_text=source_text,
            extracted_json=extracted_json_str
        )

        raw_response = None
        json_str_extracted_for_parsing = None # Almacena el string JSON que se intentará parsear
        try:
            raw_response = await self.llm.generate(prompt=prompt, temperature=0.1, max_tokens=350, stream=False)
            logger.debug(f"Respuesta cruda Verifier LLM para {description.window_id}: '{raw_response[:500]}...'")

            if not raw_response or not isinstance(raw_response, str):
                 logger.error(f"Verifier LLM devolvió respuesta inválida o vacía para {description.window_id}.")
                 description.is_verified = False
                 return description

            json_str_raw = None # JSON extraído antes de limpiar comentarios
            match_markdown_json = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", raw_response, re.DOTALL)
            if match_markdown_json:
                json_str_raw = match_markdown_json.group(1).strip()
            else:
                match_plain_json = re.search(r"(\{[\s\S]*\})(?=\s*$|\n)", raw_response, re.DOTALL)
                if match_plain_json:
                    json_str_raw = match_plain_json.group(1).strip()
                else:
                    log_or_save_raw_json(raw_response, "Verifier_NoJSONFound", window_id=description.window_id,
                                         base_working_dir=get_config_value(self.config, "general.working_dir"))
                    logger.error(f"Verifier LLM no devolvió bloque JSON reconocible para ventana {description.window_id}. Ver archivo de depuración. Respuesta: '{raw_response[:200]}...'")
                    description.is_verified = False
                    return description
            
            json_str_cleaned = re.sub(r"//.*", "", json_str_raw) # Eliminar comentarios //
            json_str_cleaned = re.sub(r"/\*[\s\S]*?\*/", "", json_str_cleaned, flags=re.DOTALL).strip() # Eliminar comentarios /* ... */
            json_str_extracted_for_parsing = json_str_cleaned

            if not json_str_extracted_for_parsing:
                 log_or_save_raw_json(json_str_raw, "Verifier_EmptyAfterClean", window_id=description.window_id,
                                      base_working_dir=get_config_value(self.config, "general.working_dir"))
                 logger.error(f"Bloque JSON extraído vacío de Verifier para {description.window_id} (después de limpiar). Original (antes de limpiar): '{str(json_str_raw)[:200]}...'")
                 description.is_verified = False
                 return description
            
            log_or_save_raw_json(json_str_extracted_for_parsing, "Verifier_PreParse", window_id=description.window_id,
                                 base_working_dir=get_config_value(self.config, "general.working_dir"))
            
            logger.debug(f"JSON Limpio para Verifier {description.window_id} (listo para parsear): '{json_str_extracted_for_parsing[:200]}...'")
            verification_result = json.loads(json_str_extracted_for_parsing)

            # ---- INICIO DE LOS CAMBIOS APLICADOS ----
            obj_sum_ok = verification_result.get("objective_summary_verified", False)

            ents_list_in_desc = description.entities # Las entidades originales de la descripción
            ents_verified_list = verification_result.get("entities_verified", [])
            # Entidades OK si: (1) la descripción no tenía entidades Y el verificador devuelve lista vacía,
            # O (2) la descripción tenía entidades Y todas están marcadas como true por el verificador.
            ents_ok = (not ents_list_in_desc and not ents_verified_list) or \
                        (ents_list_in_desc and isinstance(ents_verified_list, list) and \
                         len(ents_verified_list) == len(ents_list_in_desc) and all(ents_verified_list))

            rels_list_in_desc = description.relations
            rels_verified_list = verification_result.get("relations_verified", [])
            # Relaciones OK si: (1) la descripción no tenía relaciones Y el verificador devuelve lista vacía,
            # O (2) la descripción tenía relaciones Y todas están marcadas como true por el verificador.
            rels_ok = (not rels_list_in_desc and not rels_verified_list) or \
                        (rels_list_in_desc and isinstance(rels_verified_list, list) and \
                         len(rels_verified_list) == len(rels_list_in_desc) and all(rels_verified_list))

            # Pasar verificación global si el LLM lo dice, o si los componentes están OK.
            # Dar prioridad a lo que dice el LLM en "verification_passed" si es más simple.
            overall_passed_by_llm = verification_result.get("verification_passed", False)

            # description.is_verified = obj_sum_ok and ents_ok and rels_ok # Lógica más estricta (no activa por defecto)
            description.is_verified = overall_passed_by_llm # Confiar en el LLM para la decisión final
            # ---- FIN DE LOS CAMBIOS APLICADOS ----

            if not description.is_verified:
                 logger.warning(f"Verificación falló para ventana {description.window_id}. Razón LLM: {verification_result.get('reasoning', 'N/A')}")
                 # Log adicional para depuración de los componentes individuales
                 logger.debug(f"Detalles de componentes de verificación para {description.window_id}: obj_sum_ok={obj_sum_ok}, ents_ok={ents_ok} (original_len={len(ents_list_in_desc)}, verified_len={len(ents_verified_list) if isinstance(ents_verified_list, list) else 'N/A'}), rels_ok={rels_ok} (original_len={len(rels_list_in_desc)}, verified_len={len(rels_verified_list) if isinstance(rels_verified_list, list) else 'N/A'})")
            else:
                 logger.info(f"Verificación completada y PASADA para ventana {description.window_id}.")
                 # Opcionalmente, también loguear el estado de los componentes si la verificación global pasa
                 logger.debug(f"Detalles de componentes de verificación (PASADA) para {description.window_id}: obj_sum_ok={obj_sum_ok}, ents_ok={ents_ok}, rels_ok={rels_ok}, overall_llm_decision={overall_passed_by_llm}")


            return description

        except json.JSONDecodeError as e:
             logger.error(f"Error parseando JSON del Verifier para {description.window_id}: {e}. JSON intentado (ver archivo de depuración o log anterior): '{str(json_str_extracted_for_parsing)[:200]}...'")
             description.is_verified = False
             return description
        except Exception as e:
            logger.exception(f"Error inesperado durante verificación para ventana {description.window_id}: {e}. Respuesta LLM (ver archivo de depuración o log anterior): '{str(raw_response)[:200]}...'")
            log_or_save_raw_json(raw_response, "Verifier_UnexpectedError", window_id=description.window_id,
                                 base_working_dir=get_config_value(self.config, "general.working_dir"))
            description.is_verified = False
            return description


---
File: /ingestion_plugins/fast_pdf_parser.py
---

# ares/ingestion_plugins/fast_pdf_parser.py

import fitz  # PyMuPDF
import os
from typing import Dict, Any, Tuple
from datetime import datetime, timezone

from ..utils.logging_setup import logger

class FastPDFParser:
    """
    Un parser de PDFs de alto rendimiento basado en PyMuPDF (fitz).
    
    Intención:
    Este parser está diseñado para la extracción eficiente de texto de documentos PDF
    donde el contenido principal es texto seleccionable. No realiza OCR ni análisis
    de imágenes complejos, lo que lo hace órdenes de magnitud más rápido que
    soluciones como docling para este caso de uso. Mantiene la estructura de páginas
    como metadato, lo cual es útil para el chunking contextual posterior.
    """
    def parse(self, file_path: str) -> Tuple[str, Dict[str, Any]]:
        """
        Parsea un archivo PDF, extrae su texto y devuelve el contenido junto con metadatos.

        Args:
            file_path: La ruta al archivo PDF.

        Returns:
            Una tupla que contiene:
            - str: El texto completo del documento, con separadores de página.
            - Dict[str, Any]: Un diccionario de metadatos que incluye información
              del documento y del proceso de parsing.
        """
        if not os.path.exists(file_path):
            error_msg = f"El archivo PDF no fue encontrado en la ruta: {file_path}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)

        full_text_parts = []
        try:
            doc = fitz.open(file_path)
            logger.info(f"Procesando PDF '{os.path.basename(file_path)}' con {doc.page_count} páginas usando FastPDFParser...")

            for page_num, page in enumerate(doc):
                # Añadir un separador claro y único para que pueda ser usado por el chunker si es necesario.
                page_header = f"\n\n[--- PAGE {page_num + 1} OF {doc.page_count} ---]\n\n"
                # 'text' es el modo de extracción por defecto, 'sort=True' intenta mantener el orden de lectura natural.
                page_text = page.get_text("text", sort=True).strip()
                
                if page_text: # Solo añadir la página si contiene texto
                    full_text_parts.append(page_header + page_text)
            
            # Construir metadatos enriquecidos
            metadata = {
                "source_file": os.path.basename(file_path),
                "full_path": os.path.abspath(file_path),
                "page_count": doc.page_count,
                "parser": "fast_pdf_parser_pymupdf",
                "parsing_timestamp_utc": datetime.now(timezone.utc).isoformat(),
                **(doc.metadata or {}) # Añadir metadatos nativos del PDF (autor, título, etc.)
            }
            
            final_text = "".join(full_text_parts)
            logger.info(f"Extracción de texto de '{os.path.basename(file_path)}' completada. Total caracteres: {len(final_text)}")
            
            return final_text, metadata

        except Exception as e:
            logger.exception(f"Fallo al procesar el PDF en {file_path} con PyMuPDF: {e}")
            # Devolver una tupla vacía en caso de error para un manejo consistente
            return "", {"error": str(e), "source_file": os.path.basename(file_path)}

# Para usarlo, se crearía un script similar a main_document_test.py:
#
# from fast_pdf_parser import FastPDFParser
# from ares.core.orchestrator import AresCore
#
# async def main_ingest_pdf():
#     parser = FastPDFParser()
#     text, metadata = parser.parse("ruta/a/tu.pdf")
#
#     ares_core = AresCore()
#     await ares_core.start_services()
#     
#     document_id = "tu_pdf_id"
#     # ... (lógica para dividir `text` en ventanas si es necesario) ...
#     windows_to_process = [(f"{document_id}_win_1", text, metadata)]
#     await ares_core.ingestion_pipeline.process_windows_batch(windows_to_process)
#
#     await ares_core.close()


---
File: /llm/__init__.py
---




---
File: /llm/base.py
---

# ares/llm/base.py
from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Any, AsyncIterator, Union

class BaseLLMWrapper(ABC):
    """Interfaz abstracta para interactuar con diferentes LLMs."""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ) -> Union[str, AsyncIterator[str]]:
        """Genera una respuesta del LLM."""
        pass

    # Opcional: método para cerrar conexiones si es necesario
    async def close(self):
        """Cierra recursos o conexiones si el wrapper lo requiere."""
        pass

    # Opcional: método para contar tokens específico del modelo
    def count_tokens(self, text: str) -> int:
        """Estima el número de tokens para un texto dado (implementación básica)."""
        # Implementación simple por defecto, puede ser sobreescrita
        # Podríamos usar tiktoken aquí para una mejor estimación base
        try:
             import tiktoken
             # Usar un encoder común como fallback, puede no ser el del modelo específico
             encoder = tiktoken.get_encoding("cl100k_base")
             return len(encoder.encode(text))
        except ImportError:
             # logger.warning("Tiktoken no instalado. Conteo de tokens usará split().") # Evitar logger aquí si causa problemas circulares
             return len(text.split()) # Fallback muy básico
        except Exception: # Capturar otros errores de tiktoken
             return len(text.split())


---
File: /llm/lmstudio.py
---

from typing import List, Dict, Optional, Any, AsyncIterator, Union
from openai import AsyncOpenAI, APIConnectionError, RateLimitError, Timeout
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

from .base import BaseLLMWrapper
from ..utils.logging_setup import logger
from ..utils.helpers import safe_unicode_decode # Asegúrate que esta función exista

class LMStudioWrapper(BaseLLMWrapper):
    """Wrapper para interactuar con modelos a través de la API local de LM Studio (compatible con OpenAI)."""

    def __init__(self, model_identifier: str, base_url: str = "http://localhost:1234/v1", api_key: str = "not-needed"):
        # El model_identifier puede ser usado internamente si necesitas seleccionar modelos cargados,
        # pero la API de OpenAI no lo usa directamente en la llamada.
        self.model_identifier = model_identifier # Para referencia
        self.base_url = base_url
        self.api_key = api_key # Aunque no se necesite, la librería OpenAI lo espera
        try:
            self.client = AsyncOpenAI(base_url=self.base_url, api_key=self.api_key)
            logger.info(f"LMStudioWrapper inicializado para API en {base_url}")
        except Exception as e:
            logger.error(f"Error inicializando cliente OpenAI para LM Studio: {e}")
            raise

    # Reutilizar lógica de reintento de OpenAI
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((RateLimitError, APIConnectionError, Timeout)),
    )
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        **kwargs # Otros parámetros compatibles con API OpenAI (top_p, stop, etc.)
    ) -> Union[str, AsyncIterator[str]]:

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if history:
            messages.extend(history)
        messages.append({"role": "user", "content": prompt})

        # Asegurar que los kwargs pasados sean válidos para la API ChatCompletion
        valid_completion_kwargs = {"top_p", "stop", "presence_penalty", "frequency_penalty"}
        completion_kwargs = {k: v for k, v in kwargs.items() if k in valid_completion_kwargs}

        try:
            # LM Studio no usa el parámetro 'model' en la llamada /v1/chat/completions
            # pero la librería OpenAI puede requerirlo. Pasamos un placeholder o None.
            # A menudo, el modelo se selecciona en la UI de LM Studio.
            # Si tu versión/configuración de LM Studio SÍ usa el campo model, pásalo:
            # model_param = self.model_identifier # o un valor fijo si es necesario

            response_generator = await self.client.chat.completions.create(
                model="placeholder-model", # La API de LM Studio suele ignorar esto
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=stream,
                **completion_kwargs
            )

            if stream:
                async def stream_wrapper():
                    full_response_content = ""
                    async for chunk in response_generator:
                        content = chunk.choices[0].delta.content
                        if content:
                             # LM Studio a veces devuelve \uXXXX, decodificar si es necesario
                             if "\\u" in content:
                                  try:
                                      decoded_content = safe_unicode_decode(content.encode('utf-8'))
                                      full_response_content += decoded_content
                                      yield decoded_content
                                  except Exception as e:
                                       logger.warning(f"Error decodificando unicode en stream de LM Studio: {e}. Devolviendo original.")
                                       full_response_content += content
                                       yield content
                             else:
                                  full_response_content += content
                                  yield content
                    logger.debug(f"Respuesta completa de LM Studio (stream): {full_response_content[:100]}...")

                return stream_wrapper()
            else:
                # Respuesta no-stream
                full_response = response_generator
                content = full_response.choices[0].message.content
                 # Decodificar si es necesario
                if "\\u" in content:
                    try:
                        content = safe_unicode_decode(content.encode('utf-8'))
                    except Exception as e:
                        logger.warning(f"Error decodificando unicode en respuesta de LM Studio: {e}. Devolviendo original.")

                logger.debug(f"Respuesta completa de LM Studio (no-stream): {content[:100]}...")
                return content

        except APIConnectionError as e:
             logger.error(f"Error de conexión con LM Studio en {self.base_url}: {e}. ¿Está LM Studio ejecutándose y el servidor API habilitado?")
             return f"[Error de conexión con LM Studio]"
        except Exception as e:
            logger.error(f"Error inesperado al generar con LM Studio: {e}")
            return f"[Error inesperado LM Studio: {str(e)}]"


---
File: /llm/ollama.py
---

import ollama
from typing import List, Dict, Optional, Any, AsyncIterator, Union
from .base import BaseLLMWrapper
from ..utils.logging_setup import logger

class OllamaWrapper(BaseLLMWrapper):
    """Wrapper para interactuar con modelos a través de la API de Ollama."""

    def __init__(self, model_name: str, host_url: Optional[str] = None, timeout: Optional[int] = None):
        self.model_name = model_name
        self.client = ollama.AsyncClient(host=host_url, timeout=timeout)
        logger.info(f"OllamaWrapper inicializado para modelo '{model_name}' en {host_url or 'default host'}")
        # Podríamos verificar si el modelo existe localmente aquí, pero requiere una llamada API
        # asyncio.run(self._check_model_exists()) # No hacer run aquí

    async def _check_model_exists(self):
         try:
             await self.client.show(self.model_name)
             logger.debug(f"Modelo Ollama '{self.model_name}' confirmado.")
             return True
         except ollama.ResponseError as e:
             if e.status_code == 404:
                 logger.error(f"Modelo Ollama '{self.model_name}' no encontrado. Asegúrate que esté descargado ('ollama pull {self.model_name}') o que el nombre sea correcto.")
             else:
                 logger.error(f"Error inesperado verificando modelo Ollama '{self.model_name}': {e}")
             return False
         except Exception as e:
              logger.error(f"Error de conexión con Ollama al verificar modelo: {e}")
              return False

    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        max_tokens: Optional[int] = None, # Ollama usa num_predict
        temperature: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ) -> Union[str, AsyncIterator[str]]:

        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if history:
            messages.extend(history)
        messages.append({"role": "user", "content": prompt})

        # Mapear parámetros comunes a Ollama 'options'
        options = {}
        if max_tokens is not None:
            options["num_predict"] = max_tokens
        if temperature is not None:
            options["temperature"] = temperature
        # Añadir otros kwargs directamente a options si son válidos para Ollama
        valid_ollama_options = {"mirostat", "mirostat_eta", "mirostat_tau", "num_ctx",
                                "num_gpu", "num_thread", "repeat_last_n", "repeat_penalty",
                                "seed", "stop", "tfs_z", "top_k", "top_p"}
        for k, v in kwargs.items():
            if k in valid_ollama_options:
                options[k] = v

        try:
            response_generator = await self.client.chat(
                model=self.model_name,
                messages=messages,
                stream=stream,
                options=options if options else None
            )

            if stream:
                async def stream_wrapper():
                    full_response_content = ""
                    async for chunk in response_generator:
                        content = chunk.get('message', {}).get('content')
                        if content:
                            full_response_content += content
                            yield content
                        # Opcional: manejar 'done' y 'error' si es necesario
                        if chunk.get('done') and chunk.get('error'):
                            logger.error(f"Error en stream de Ollama: {chunk['error']}")
                            # Podrías lanzar una excepción aquí o simplemente parar
                            break
                    logger.debug(f"Respuesta completa de Ollama (stream): {full_response_content[:100]}...")

                return stream_wrapper()
            else:
                # Respuesta no-stream
                full_response = response_generator # No es generador si stream=False
                content = full_response.get('message', {}).get('content', '')
                logger.debug(f"Respuesta completa de Ollama (no-stream): {content[:100]}...")
                return content

        except ollama.ResponseError as e:
            logger.error(f"Error de API de Ollama ({e.status_code}): {e.error}")
            # Considerar relanzar o devolver un error específico
            return f"[Error de Ollama: {e.error}]"
        except Exception as e:
            logger.error(f"Error inesperado al generar con Ollama: {e}")
            return f"[Error inesperado: {str(e)}]"


---
File: /llm/openai_api.py
---




---
File: /llm/vllm_api.py
---

# ares/llm/vllm_api.py

import asyncio
from typing import List, Dict, Any, AsyncIterator, Optional, Union, Tuple
import numpy as np
from tqdm.asyncio import tqdm 

# Importaciones de terceros
from openai import AsyncOpenAI, APIConnectionError, RateLimitError, APIStatusError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Importaciones del proyecto ARES
from .base import BaseLLMWrapper
from ..utils.logging_setup import logger
from ..utils.helpers import get_config_value

class VLLMWrapper(BaseLLMWrapper):
    """
    Wrapper para comunicarse con un servidor vLLM a través de su API compatible con OpenAI.
    Está diseñado para manejar tanto solicitudes individuales (para chat) como lotes masivos
    de solicitudes (para ingesta) de manera eficiente y robusta.
    """
    def __init__(self, ares_config: Dict[str, Any], model_identifier_for_api: str):
        """
        Inicializa el cliente API.

        Args:
            ares_config: La configuración global de ARES, de donde se leerá la URL del servidor.
            model_identifier_for_api: El nombre del modelo que se pasará en el campo 'model'
                                      de las solicitudes a la API.
        """
        vllm_server_config = get_config_value(ares_config, "vllm_server", {})
        host = vllm_server_config.get("host", "127.0.0.1")
        port = vllm_server_config.get("port", 8000)
        
        self.base_url = f"http://{host}:{port}/v1"
        api_key = vllm_server_config.get("api_key", "no-key-needed")
        
        self.model_identifier = model_identifier_for_api
        try:
            self.client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, max_retries=0) # Desactivar reintentos de openai, usamos tenacity
            logger.info(f"VLLMWrapper inicializado. Apuntando a API en {self.base_url} para el modelo '{self.model_identifier}'.")
        except Exception as e:
            logger.exception(f"Error inicializando el cliente AsyncOpenAI para VLLM: {e}")
            raise

    # La anotación @retry se aplicará a los métodos que hacen las llamadas de red.
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((APIConnectionError, RateLimitError, APIStatusError)),
        before_sleep=lambda retry_state: logger.warning(f"Reintentando llamada a VLLM API tras error: {retry_state.outcome.exception()}. Intento {retry_state.attempt_number}...")
    )
    async def _internal_generate_single(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int],
        temperature: Optional[float],
        stream: bool,
        **kwargs
    ) -> Union[str, AsyncIterator[str], Any]: # Any para el objeto de respuesta completo
        """Método interno para una única llamada a la API de chat, manejado por Tenacity."""

        # Parámetros que se pasarán directamente al método create de OpenAI
        completion_kwargs = {k: v for k, v in kwargs.items() if k in {
            "top_p", "stop", "logprobs", "top_logprobs"
        }}
        
        # Parámetro especial para controlar qué devuelve este método
        return_full_response = kwargs.get("return_full_response", False)
        
        if chat_template_kwargs := kwargs.get("chat_template_kwargs"):
            completion_kwargs["extra_body"] = {"chat_template_kwargs": chat_template_kwargs}

        response_gen = await self.client.chat.completions.create(
            model=self.model_identifier,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            **completion_kwargs
        )

        if stream:
            async def stream_wrapper():
                async for chunk in response_gen:
                    content = chunk.choices[0].delta.content
                    if content:
                        yield content
            return stream_wrapper()
        else:
            if return_full_response:
                return response_gen # Devuelve el objeto Completion completo
            else:
                return response_gen.choices[0].message.content.strip()

    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
        **kwargs
    ) -> Union[str, AsyncIterator[str]]:
        
        messages = []
        if system_prompt: messages.append({"role": "system", "content": system_prompt})
        if history: messages.extend(history)
        messages.append({"role": "user", "content": prompt})

        try:
            return await self._internal_generate_single(messages, max_tokens, temperature, stream, **kwargs)
        except Exception as e:
            logger.exception(f"Error final generando con VLLM API tras reintentos: {e}")
            error_msg = f"[Error en VLLM: {str(e)[:100]}]"
            if stream:
                async def error_stream(): yield error_msg
                return error_stream()
            return error_msg

    async def generate_batch(self, prompts: Union[List[str], List[List[Dict[str, str]]]]) -> List[str]:
        """
        Genera respuestas para un lote de prompts. Acepta prompts simples o conversaciones completas.
        """
        if not prompts: return []

        tasks = []
        for p in prompts:
            if isinstance(p, str):
                # Es un prompt de texto simple.
                task = self.generate(prompt=p, stream=False)
            elif isinstance(p, list):
                # Es una conversación (lista de mensajes).
                task = self.generate(prompt=p[-1]['content'], history=p[:-1], stream=False)
            else:
                logger.warning(f"Tipo de prompt inválido en lote: {type(p)}. Saltando.")
                # Añadir un resultado de error para mantener la correspondencia de índices.
                tasks.append(asyncio.sleep(0, result="[Error: Tipo de prompt inválido]"))
                continue
            tasks.append(task)
        
        # Ejecuta todas las llamadas a la API de forma concurrente.
        results = await tqdm.gather(*tasks, desc="Procesando Lote LLM", return_exceptions=True)
        
        # Procesa los resultados, manejando excepciones.
        final_outputs = []
        for i, res in enumerate(results):
            if isinstance(res, Exception):
                logger.error(f"Error en la solicitud de lote {i+1}/{len(prompts)}: {res}")
                final_outputs.append(f"[Error en vLLM batch: {str(res)[:100]}]")
            else:
                final_outputs.append(str(res))
        
        return final_outputs

    # <<< NUEVO MÉTODO PARA OBTENER DETALLES (LOGPROBS) >>>
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((APIConnectionError, RateLimitError, APIStatusError)),
        before_sleep=lambda retry_state: logger.warning(f"Reintentando llamada a VLLM API (con detalles) tras error: {retry_state.outcome.exception()}. Intento {retry_state.attempt_number}...")
    )
    async def _internal_generate_single_with_details(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """
        Método interno para una única llamada a la API que solicita detalles adicionales como logprobs.
        Manejado por Tenacity.
        """
        # Extraer parámetros de sampling y otros kwargs
        sampling_params = {
            "max_tokens": kwargs.get("max_tokens", 5),
            "temperature": kwargs.get("temperature", 0.1),
            "logprobs": kwargs.get("logprobs", False),
            "top_logprobs": kwargs.get("top_logprobs", None)
        }
        
        raw_completion = await self.client.chat.completions.create(
            model=self.model_identifier,
            messages=messages,
            **sampling_params
        )

        choice = raw_completion.choices[0]
        text_output = choice.message.content.strip() if choice.message.content else ""
        
        # Procesar logprobs si existen en la respuesta
        top_logprobs_processed = {}
        if choice.logprobs and choice.logprobs.content:
            for i, token_logprob in enumerate(choice.logprobs.content):
                # La clave del diccionario será el índice del token (0, 1, 2...)
                if token_logprob.top_logprobs:
                    top_logprobs_processed[i] = {
                        str(lp.token_id): lp.logprob for lp in token_logprob.top_logprobs
                    }
        
        return {
            "text": text_output,
            "top_logprobs": top_logprobs_processed,
            # Se pueden añadir otros detalles aquí en el futuro (ej. usage, finish_reason)
        }

    async def generate_with_details_batch(self, prompts: List[List[Dict[str, str]]], **kwargs) -> List[Dict[str, Any]]:
        """
        Genera respuestas para un lote de prompts y devuelve detalles, incluyendo logprobs.
        """
        if not prompts:
            return []

        tasks = [self._internal_generate_single_with_details(conv, **kwargs) for conv in prompts]
        
        results = await tqdm.gather(*tasks, desc="Procesando Lote LLM (con detalles)", return_exceptions=True)

        processed_outputs = []
        for i, res in enumerate(results):
            if isinstance(res, Exception):
                logger.error(f"Error en la solicitud de lote con detalles {i+1}/{len(prompts)}: {res}")
                processed_outputs.append({"text": "[Error]", "top_logprobs": {}})
            else:
                processed_outputs.append(res)
        
        return processed_outputs

    async def embed_batch(self, texts: List[str], model_name_override: Optional[str] = None) -> List[Optional[np.ndarray]]:
        """
        Genera embeddings para un lote de textos.
        """
        if not texts:
            return []
        
        try:
            target_model = model_name_override or self.model_identifier
            response = await self.client.embeddings.create(
                model=target_model,
                input=texts,
            )
            return [np.array(embedding.embedding, dtype=np.float32) for embedding in response.data]
        except Exception as e:
            logger.exception(f"Error generando embeddings en lote con VLLM API para modelo {target_model}: {e}")
            return [None] * len(texts)


---
File: /maintenance/index_optimizer.py
---

# ares/maintenance/index_optimizer.py
import asyncio
import time
from datetime import datetime, timedelta, timezone
from typing import Dict, Any, Optional
from ..storage.base import BaseVectorDB, BaseKVStore, BaseKnowledgeGraph
from ..storage.bm25.whoosh_bm25 import WhooshBM25Index # Asumimos Whoosh
from ..utils.logging_setup import logger

class IndexOptimizer:
    """Realiza tareas de mantenimiento y optimización en los almacenes."""

    def __init__(self,
                 config: Dict[str, Any],
                 vector_db: Optional[BaseVectorDB] = None,
                 bm25_index: Optional[WhooshBM25Index] = None,
                 kg_graph: Optional[BaseKnowledgeGraph] = None,
                 cache_store: Optional[BaseKVStore] = None,
                 description_store: Optional[BaseKVStore] = None): # Añadir para limpieza
        self.config = config.get("maintenance", {})
        self.vector_db = vector_db
        self.bm25_index = bm25_index
        self.kg_graph = kg_graph
        self.cache_store = cache_store
        self.description_store = description_store # Para limpiar resúmenes antiguos
        logger.info("IndexOptimizer inicializado.")

    async def _run_optimizer(self, component: Any, method_name: str, description: str):
        """Helper para ejecutar un método de optimización (sync o async)."""
        if not component:
            logger.debug(f"Componente para {description} no configurado, omitiendo optimización.")
            return
        if hasattr(component, method_name):
            logger.info(f"Optimizando {description}...")
            start_time = time.monotonic()
            try:
                optimize_method = getattr(component, method_name)
                if asyncio.iscoroutinefunction(optimize_method):
                     await optimize_method()
                else:
                     await asyncio.to_thread(optimize_method)
                duration = time.monotonic() - start_time
                logger.info(f"{description} optimizado en {duration:.2f} segundos.")
            except Exception as e:
                logger.error(f"Error optimizando {description}: {e}")
        else:
            logger.debug(f"{description} no tiene método '{method_name}'.")

    async def optimize_vector_db(self):
        """Optimiza la base de datos vectorial (si soporta el método)."""
        # Qdrant puede tener `optimize`, ChromaDB puede tener `compact`
        await self._run_optimizer(self.vector_db, 'optimize', 'VectorDB')
        # Podríamos intentar llamar a compact si optimize no existe
        # elif hasattr(self.vector_db, 'compact'): await self._run_optimizer(self.vector_db, 'compact', 'VectorDB (Compact)')

    async def optimize_bm25(self):
        """Optimiza el índice BM25 (Whoosh)."""
        if self.bm25_index and hasattr(self.bm25_index.ix, 'optimize'):
             await self._run_optimizer(self.bm25_index.ix, 'optimize', 'Índice BM25')

    async def optimize_kg(self):
        """Optimiza/guarda el grafo de conocimiento."""
        # Para NetworkX, guardar es suficiente
        await self._run_optimizer(self.kg_graph, '_save_graph', 'Knowledge Graph (Save)')
        # Si hay otro backend con optimize:
        # elif hasattr(self.kg_graph, 'optimize'): await self._run_optimizer(self.kg_graph, 'optimize', 'Knowledge Graph (Optimize)')

    async def clean_cache(self, older_than_days: int = 30):
        """Limpia entradas antiguas del caché LLM."""
        if not self.cache_store: return
        cutoff_ts = (datetime.now(timezone.utc) - timedelta(days=older_than_days)).timestamp()
        logger.info(f"Limpiando caché LLM anterior a {datetime.fromtimestamp(cutoff_ts, timezone.utc).strftime('%Y-%m-%d %H:%M:%S Z')}")

        deleted_count = 0
        try:
            # Necesita que KVStore soporte iteración y borrado, y metadatos de timestamp
            keys_to_delete = []
            async for key in self.cache_store.get_all_keys_iterator():
                 data = await self.cache_store.get(key)
                 if data and isinstance(data, dict) and data.get("timestamp", float('inf')) < cutoff_ts:
                      keys_to_delete.append(key)

            if keys_to_delete:
                 # KVStore idealmente tendría delete_batch
                 for key in keys_to_delete:
                     await self.cache_store.delete(key)
                 deleted_count = len(keys_to_delete)
                 logger.info(f"Limpieza de caché LLM: {deleted_count} entradas antiguas eliminadas.")
            else:
                 logger.info("Limpieza de caché LLM: No se encontraron entradas antiguas para eliminar.")

        except Exception as e:
             logger.error(f"Error durante limpieza de caché LLM: {e}")


    async def clean_old_summaries(self, keep_days: int = 90):
         """Limpia resúmenes diarios/semanales/mensuales antiguos."""
         if not self.description_store: return
         cutoff_ts = (datetime.now(timezone.utc) - timedelta(days=keep_days)).timestamp()
         logger.info(f"Limpiando resúmenes anteriores a {datetime.fromtimestamp(cutoff_ts, timezone.utc).strftime('%Y-%m-%d %H:%M:%S Z')}")

         deleted_count = 0
         try:
             # Requiere buscar/filtrar por metadata.summary_level y metadata.start_date
             keys_to_delete = []
             async for key in self.description_store.get_all_keys_iterator():
                  if key.startswith("daily_summary_") or key.startswith("weekly_summary_") or key.startswith("monthly_summary_"):
                       data = await self.description_store.get(key)
                       if data and isinstance(data, dict):
                            # Asumir que guardamos start_date como ISO string
                            start_date_str = data.get("metadata", {}).get("start_date")
                            if start_date_str:
                                 try:
                                      start_dt = datetime.fromisoformat(start_date_str.replace("Z", "+00:00"))
                                      if start_dt.timestamp() < cutoff_ts:
                                           keys_to_delete.append(key)
                                 except ValueError:
                                      logger.warning(f"Formato de fecha inválido en resumen {key}: {start_date_str}")

             if keys_to_delete:
                 for key in keys_to_delete: await self.description_store.delete(key)
                 deleted_count = len(keys_to_delete)
                 logger.info(f"Limpieza de resúmenes: {deleted_count} resúmenes antiguos eliminados.")
             else:
                 logger.info("Limpieza de resúmenes: No se encontraron resúmenes antiguos.")

         except Exception as e:
              logger.error(f"Error durante limpieza de resúmenes antiguos: {e}")


    async def run_scheduled_maintenance(self):
        """Ejecuta todas las tareas de mantenimiento programadas."""
        schedule = self.config.get("index_maintenance_schedule", "none").lower()
        run_now = False
        # Lógica simple de schedule (ejecutar si está configurado)
        if schedule in ["daily", "weekly", "monthly"]:
             # En una app real, se compararía con la fecha/hora actual
             run_now = True

        if run_now:
             logger.info(f"Ejecutando mantenimiento programado ({schedule})...")
             await self.optimize_vector_db()
             await self.optimize_bm25()
             await self.optimize_kg()
             # Limpiar caché y resúmenes con retenciones configurables
             cache_retention = self.config.get("cache_retention_days", 30)
             summary_retention = self.config.get("summary_retention_days", 90)
             await self.clean_cache(older_than_days=cache_retention)
             await self.clean_old_summaries(keep_days=summary_retention)
             logger.info("Mantenimiento programado completado.")
        else:
             logger.debug("Mantenimiento no programado para hoy.")


---
File: /maintenance/summarizer.py
---

# ares/maintenance/summarizer.py
import asyncio
from datetime import datetime, timedelta, timezone
import time
from typing import Dict, Any, Optional, List
from ..storage.base import BaseKVStore
from ..llm.base import BaseLLMWrapper
from ..core.datatypes import Description
from ..utils.logging_setup import logger
from ..utils.helpers import encode_string_by_tiktoken # Para limitar contexto

# Prompt para sumarizar descripciones diarias
DAILY_SUMMARY_PROMPT_TEMPLATE = """
Eres un asistente de síntesis experto. A continuación se presentan resúmenes objetivos y palabras clave extraídas de varias conversaciones mantenidas durante el día {date}. Tu tarea es crear un **Resumen Diario Consolidado** conciso (máximo 3-4 frases) que capture los temas y eventos más importantes del día. Enfócate en los hechos principales.

Descripciones del Día:
{daily_descriptions_formatted}

Resumen Consolidado del Día {date}:
"""

# Prompt para sumarizar resúmenes diarios en uno semanal/mensual
HIGHER_LEVEL_SUMMARY_PROMPT_TEMPLATE = """
Eres un asistente de síntesis experto. A continuación se presentan resúmenes diarios de conversaciones para el período {period}. Tu tarea es crear un **Resumen {level} Consolidado** muy conciso (máximo 2-3 frases) que capture las tendencias o temas más recurrentes y significativos del período.

Resúmenes Diarios:
{previous_summaries_formatted}

Resumen {level} Consolidado ({period}):
"""

class Summarizer:
    def __init__(self,
                 config: Dict[str, Any],
                 description_store: BaseKVStore,
                 summary_llm: Optional[BaseLLMWrapper]):
        self.config = config.get("maintenance", {})
        self.description_store = description_store
        self.summary_llm = summary_llm
        self.max_context_tokens = self.config.get("summary_llm_max_context_tokens", 7000) # Límite de tokens para contexto
        if not self.summary_llm:
             logger.warning("LLM de resumen no configurado. Sumarización automática deshabilitada.")
        else:
             logger.info("Summarizer inicializado.")

    async def _get_items_for_period(self, start_dt: datetime, end_dt: datetime, filter_key: str, filter_value: Optional[Any] = None) -> List[Dict]:
        """Recupera descripciones o resúmenes de un período."""
        logger.debug(f"Buscando items ({filter_key}={filter_value or 'any'}) entre {start_dt.isoformat()} y {end_dt.isoformat()}")
        try:
            # Construir filtro de timestamp y filtro adicional (ej. summary_level)
            filters = {
                "metadata.timestamp": { # Asumiendo que el timestamp está en metadata
                    "$gte": start_dt.isoformat(),
                    "$lt": end_dt.isoformat()
                }
            }
            if filter_key and filter_value is not None:
                 # Asumir que la clave está en metadata (ej. metadata.summary_level)
                 filters[f"metadata.{filter_key}"] = filter_value

            # Limitar para evitar sobrecargar, ajustar según necesidad
            items_data = await self.description_store.search_by_metadata(
                 filters=filters, limit=10000 # Límite muy alto, la BD debe ser eficiente
            )
            logger.debug(f"Encontrados {len(items_data)} items ({filter_key}={filter_value or 'any'}) para el período.")
            return items_data
        except NotImplementedError:
             logger.error("DescriptionStore no soporta búsqueda por metadatos. No se puede generar resumen.")
             return []
        except Exception as e:
             logger.exception(f"Error buscando items para resumen ({filter_key}): {e}")
             return []

    def _format_content_for_prompt(self, items: List[Dict], content_key: str = "objective_summary", keyword_key: str = "keywords") -> str:
        """Formatea una lista de descripciones o resúmenes para el prompt."""
        parts = []
        current_tokens = 0
        for data in items:
            summary = data.get(content_key, "")
            keywords = ", ".join(data.get(keyword_key, []))
            if summary:
                 line = f"- {summary} (Temas: {keywords if keywords else 'N/A'})"
                 line_tokens = len(encode_string_by_tiktoken(line)) # Contar tokens

                 if current_tokens + line_tokens <= self.max_context_tokens:
                      parts.append(line)
                      current_tokens += line_tokens + 1 # +1 for newline
                 else:
                      logger.warning(f"Contexto de descripciones/resúmenes truncado para resumen LLM ({current_tokens + line_tokens} > {self.max_context_tokens} tokens).")
                      break # Detenerse para no exceder el límite

        if not parts: return "No hay contenido relevante para resumir."
        return "\n".join(parts)

    async def create_summary(self, level: str, target_date: Optional[datetime] = None):
        """Genera un resumen para un nivel ('daily', 'weekly', 'monthly') y fecha dados."""
        if not self.summary_llm: return

        if target_date is None: target_date = datetime.now(timezone.utc)
        target_date = target_date.replace(hour=0, minute=0, second=0, microsecond=0) # Asegurar inicio del día UTC

        start_dt: datetime
        end_dt: datetime
        period_str: str
        summary_id_prefix: str
        source_data_key: str = "objective_summary" # Clave para obtener el texto base

        if level == 'daily':
             start_dt = target_date
             end_dt = start_dt + timedelta(days=1)
             date_str = start_dt.strftime("%Y-%m-%d")
             prompt_template = DAILY_SUMMARY_PROMPT_TEMPLATE
             period_str = date_str
             summary_id_prefix = "daily_summary_"
             base_items = await self._get_items_for_period(start_dt, end_dt, filter_key=None) # Obtener descripciones
             context_for_llm = self._format_content_for_prompt(base_items, content_key="objective_summary", keyword_key="keywords")
        elif level == 'weekly':
             start_dt = target_date - timedelta(days=target_date.weekday()) # Lunes de esa semana
             end_dt = start_dt + timedelta(weeks=1)
             period_str = f"Semana del {start_dt.strftime('%Y-%m-%d')}"
             prompt_template = HIGHER_LEVEL_SUMMARY_PROMPT_TEMPLATE
             summary_id_prefix = "weekly_summary_"
             base_items = await self._get_items_for_period(start_dt, end_dt, filter_key="summary_level", filter_value="daily") # Obtener resúmenes diarios
             context_for_llm = self._format_content_for_prompt(base_items, content_key="objective_summary", keyword_key=[]) # Usar resumen diario como texto
        elif level == 'monthly':
             start_dt = target_date.replace(day=1) # Inicio del mes
             next_month_year = start_dt.year + (start_dt.month // 12)
             next_month_month = start_dt.month % 12 + 1
             end_dt = datetime(next_month_year, next_month_month, 1, tzinfo=timezone.utc) # Inicio del siguiente mes
             period_str = start_dt.strftime("%Y-%m")
             prompt_template = HIGHER_LEVEL_SUMMARY_PROMPT_TEMPLATE
             summary_id_prefix = "monthly_summary_"
             base_items = await self._get_items_for_period(start_dt, end_dt, filter_key="summary_level", filter_value="daily") # Obtener resúmenes diarios
             context_for_llm = self._format_content_for_prompt(base_items, content_key="objective_summary", keyword_key=[]) # Usar resumen diario como texto
        else:
             logger.error(f"Nivel de resumen no soportado: {level}")
             return

        summary_id = f"{summary_id_prefix}{start_dt.strftime('%Y-%m-%d')}"
        logger.info(f"Intentando generar resumen {level} para: {period_str} (ID: {summary_id})")

        # 1. Validar si hay datos base
        if not base_items:
            logger.info(f"No hay datos base para resumen {level} de {period_str}.")
            return
        if context_for_llm == "No hay contenido relevante para resumir.":
            logger.info(f"No hay contenido formateado para LLM para resumen {level} de {period_str}.")
            return

        # 2. Llamar al LLM
        # Adaptar las claves del format según el template
        format_args = {
            "date": period_str,
            "period": period_str,
            "level": level.capitalize(),
            "daily_descriptions_formatted": context_for_llm if level == 'daily' else '',
            "previous_summaries_formatted": context_for_llm if level != 'daily' else ''
        }
        prompt = prompt_template.format(**format_args)

        try:
             summary_text = await self.summary_llm.generate(
                 prompt=prompt,
                 temperature=0.3,
                 max_tokens=self.config.get("summary_llm_max_output_tokens", 300),
                 stream=False
             )
             if not summary_text or not isinstance(summary_text, str):
                 raise ValueError("LLM devolvió resumen inválido.")
             summary_text = summary_text.strip()
        except Exception as e:
             logger.exception(f"Error LLM generando resumen {level} para {period_str}: {e}")
             return

        # 3. Almacenar Resumen Consolidado
        summary_metadata = {
            "summary_level": level,
            "start_date": start_dt.isoformat(), # Guardar como ISO string
            "end_date": end_dt.isoformat(),
            "source_ids": [item.get("window_id") or item.get("id") for item in base_items], # IDs de descripciones/resúmenes usados
            "timestamp": end_dt.isoformat() # Timestamp del final del período
        }
        summary_data = {
            "window_id": summary_id, # Usar ID de resumen como "window_id" para consistencia
            "objective_summary": summary_text,
            "subjective_interpretation": None,
            "entities": [], "relations": [], "keywords": [], # Podrían extraerse del resumen generado?
            "metadata": summary_metadata,
            "is_verified": True # Asumir verificado por ahora
        }

        try:
            await self.description_store.set(summary_id, summary_data)
            logger.info(f"Resumen {level} para {period_str} guardado con ID: {summary_id}")
        except Exception as e:
            logger.exception(f"Error guardando resumen {level} para {period_str} en DescriptionStore: {e}")


    async def run_scheduled_summaries(self):
        """Ejecuta la generación de resúmenes según la configuración."""
        if not self.summary_llm: return

        schedule = self.config.get("summary_schedule", "none").lower()
        now = datetime.now(timezone.utc)

        # Generar diario para ayer
        if schedule in ["daily", "weekly", "monthly"]: # Semanal/mensual también necesitan diario
             yesterday = now - timedelta(days=1)
             await self.create_summary(level='daily', target_date=yesterday)

        # Generar semanal para la semana pasada (si es Lunes hoy)
        if schedule in ["weekly", "monthly"] and now.weekday() == 0: # Es Lunes
             last_week_date = now - timedelta(weeks=1)
             await self.create_summary(level='weekly', target_date=last_week_date)

        # Generar mensual para el mes pasado (si es día 1 del mes)
        if schedule == "monthly" and now.day == 1:
             last_month_date = now.replace(day=1) - timedelta(days=1) # Último día del mes anterior
             await self.create_summary(level='monthly', target_date=last_month_date)


---
File: /retrieval/search/__init__.py
---




---
File: /retrieval/search/bm25_search.py
---

from typing import List, Tuple, Optional
from ...storage.bm25.whoosh_bm25 import WhooshBM25Index # Importar clase Whoosh
from ...storage.base import BaseKVStore # Para obtener texto si Whoosh no lo almacena completo
from ...core.datatypes import Chunk
from ...utils.logging_setup import logger

async def search_bm25(
    query: str,
    top_k: int,
    bm25_index: WhooshBM25Index, # Recibe instancia Whoosh
    chunk_store: BaseKVStore # Para obtener texto completo si es necesario
) -> List[Tuple[Chunk, float]]:
    """Realiza búsqueda BM25 usando Whoosh."""
    try:
        # Realizar búsqueda en Whoosh
        results_with_scores = await bm25_index.search(query, top_k) # [(doc_id, score), ...]

        chunk_ids = [doc_id for doc_id, _ in results_with_scores]
        if not chunk_ids:
            return []

        # Obtener datos completos de los chunks desde KVStore
        chunk_data_list = await chunk_store.get_batch(chunk_ids)

        # Mapear datos recuperados a los IDs
        chunk_map = {id: data for id, data in zip(chunk_ids, chunk_data_list)}

        final_results = []
        for doc_id, score in results_with_scores:
            data = chunk_map.get(doc_id)
            if data:
                # Reconstruir objeto Chunk
                chunk = Chunk(
                    id=doc_id,
                    text=data.get("text", ""),
                    metadata=data.get("metadata", {}),
                    token_count=data.get("token_count"),
                    embedding=None # BM25 no devuelve embedding
                )
                final_results.append((chunk, score))
            else:
                logger.warning(f"Chunk {doc_id} encontrado en BM25 pero no en KVStore.")

        logger.debug(f"Búsqueda BM25 (Whoosh) devolvió {len(final_results)} Chunks.")
        # Whoosh ya devuelve ordenado por score
        return final_results

    except Exception as e:
        logger.exception(f"Error durante la búsqueda BM25 (Whoosh): {e}")
        return []


---
File: /retrieval/search/bm25.py
---

# retrieval/search/bm25.py
from rank_bm25 import BM25Okapi
from typing import List, Dict, Tuple, Optional
from ..storage.base import BaseKVStore
from ..utils.logging_setup import logger
import asyncio

# Cache simple para el índice BM25
_bm25_index_cache: Optional[BM25Okapi] = None
_corpus_cache: Optional[Dict[str, str]] = None

async def build_bm25_index(chunk_store: BaseKVStore) -> Optional[BM25Okapi]:
    """Construye o reconstruye el índice BM25 desde el ChunkStore."""
    global _bm25_index_cache, _corpus_cache
    logger.info("Construyendo índice BM25...")
    corpus: Dict[str, str] = {}
    chunk_ids: List[str] = []
    try:
        # Iterar sobre todas las claves y obtener el texto
        # ¡Esto puede ser muy costoso para bases de datos grandes!
        # Idealmente, el KVStore tendría un método más eficiente o se haría offline.
        async for key in chunk_store.get_all_keys_iterator():
            chunk_data = await chunk_store.get(key)
            if chunk_data and 'text' in chunk_data:
                corpus[key] = chunk_data['text']
                chunk_ids.append(key)
            else:
                 logger.warning(f"Chunk {key} no encontrado o sin texto en KVStore durante indexación BM25.")


        if not corpus:
            logger.warning("No se encontraron documentos en ChunkStore para indexar con BM25.")
            _bm25_index_cache = None
            _corpus_cache = None
            return None

        tokenized_corpus = [corpus[doc_id].lower().split() for doc_id in chunk_ids] # Tokenización simple por espacio
        bm25 = BM25Okapi(tokenized_corpus)
        _bm25_index_cache = bm25
        _corpus_cache = corpus # Guardar corpus para mapeo de ID a texto
        logger.info(f"Índice BM25 construido con {len(corpus)} documentos.")
        return bm25
    except Exception as e:
        logger.exception(f"Error construyendo índice BM25: {e}")
        _bm25_index_cache = None
        _corpus_cache = None
        return None

async def search_bm25(query: str, top_k: int, chunk_store: BaseKVStore) -> List[Tuple[str, float]]:
    """Realiza una búsqueda BM25."""
    global _bm25_index_cache, _corpus_cache
    if _bm25_index_cache is None or _corpus_cache is None:
        # Intentar construir el índice si no existe (puede ser lento)
        await build_bm25_index(chunk_store)
        if _bm25_index_cache is None or _corpus_cache is None:
             logger.error("Índice BM25 no está disponible para la búsqueda.")
             return []

    tokenized_query = query.lower().split()
    try:
        # Obtener scores para todos los documentos en el índice
        # BM25Okapi espera que el corpus con el que se inicializó siga siendo válido
        doc_scores = _bm25_index_cache.get_scores(tokenized_query)

        # Mapear scores a IDs de chunk originales
        # Necesitamos las claves originales en el orden en que se indexaron. ¡Esto es frágil!
        # Es mejor si BM25Okapi pudiera devolver IDs o si guardamos el mapeo.
        # Asumiendo que _corpus_cache tiene el mapeo ID -> texto y el índice se construyó
        # en el orden de las claves de _corpus_cache.
        chunk_ids = list(_corpus_cache.keys())
        if len(doc_scores) != len(chunk_ids):
             logger.error(f"Discrepancia en tamaño de scores BM25 ({len(doc_scores)}) y corpus cacheado ({len(chunk_ids)}). Reconstruyendo índice.")
             # Forzar reconstrucción en el próximo intento o devolver vacío ahora.
             _bm25_index_cache = None
             _corpus_cache = None
             return []


        # Crear tuplas (chunk_id, score)
        results = list(zip(chunk_ids, doc_scores))

        # Ordenar por score descendente y tomar top_k
        results.sort(key=lambda item: item[1], reverse=True)

        return results[:top_k]

    except Exception as e:
        logger.exception(f"Error durante la búsqueda BM25: {e}")
        return []


---
File: /retrieval/search/description_search.py
---

# ares/retrieval/search/description_search.py
import asyncio
from typing import List, Tuple, Optional, Dict, Any
import numpy as np

from ...storage.base import BaseKVStore, BaseVectorDB
from ...core.datatypes import Description, Chunk # Chunk solo para hint en VectorDB
from ...ingestion.embedder import LateChunkingEmbedder
from ...utils.logging_setup import logger

async def search_descriptions(
    query: str,
    top_k: int,
    description_store: BaseKVStore, # Para obtener la descripción completa
    vector_db: BaseVectorDB, # Donde están los embeddings de las descripciones
    embedder: LateChunkingEmbedder,
    filters: Optional[Dict[str, Any]] = None
) -> List[Tuple[Description, float]]:
    """Busca en las descripciones almacenadas usando búsqueda vectorial."""
    logger.debug(f"SearchDescriptions: Iniciando para query='{query[:50]}...', k={top_k}, filters={filters}")
    results: List[Tuple[Description, float]] = []
    try:
        # --- 1. Embeber la Query ---
        query_embedding_list = await embedder.embed_text([query])
        if not query_embedding_list or query_embedding_list[0] is None:
            logger.error("SearchDescriptions: No se pudo generar embedding para query.")
            return []
        query_emb_np = query_embedding_list[0]

        # --- 2. Búsqueda Vectorial en Descripciones ---
        search_filters_for_vdb = {"type": "description"} # Filtrar por tipo
        if filters:
            logger.debug(f"SearchDescriptions: Aplicando filtros adicionales del plan: {filters}")
            search_filters_for_vdb.update(filters)
        logger.debug(f"SearchDescriptions: Filtro final para VectorDB: {search_filters_for_vdb}")

        # vector_db.search devuelve List[Chunk] donde el ID es el window_id
        # y metadata contiene el score
        vector_results: List[Chunk] = await vector_db.search(
            query_emb_np,
            top_k, # Usar el top_k solicitado para descripciones
            search_filters_for_vdb
        )
        
        logger.info(f"SearchDescriptions: VectorDB.search para query='{query[:30]}...' con k={top_k} y filtros={search_filters_for_vdb} devolvió {len(vector_results)} candidatos.") # <--- LOG IMPORTANTE
        for i, desc_candidate_chunk in enumerate(vector_results): # Loguear TODOS los que devuelve Qdrant
            logger.debug(
                f"  SearchDescriptions VDB Candidate {i}: ID={desc_candidate_chunk.id}, " # Este ID es el window_id
                f"Score={desc_candidate_chunk.metadata.get('similarity_score', -1.0)}, "
                f"PayloadType={desc_candidate_chunk.metadata.get('type')}" # Debería ser 'description'
            )

        if not vector_results:
            return []

        # --- 3. Recuperar Descripciones completas desde KVStore ---
        desc_ids_to_fetch = [chunk.id for chunk in vector_results] # ID del 'chunk' es el window_id
        scores_map = {chunk.id: chunk.metadata.get("similarity_score", 0.0) for chunk in vector_results}

        logger.debug(f"SearchDescriptions: Recuperando {len(desc_ids_to_fetch)} descripciones completas de DescriptionStore.")
        description_data_list = await description_store.get_batch(desc_ids_to_fetch)

        for desc_id_idx, desc_id_val in enumerate(desc_ids_to_fetch): # Usar enumerate para alinear con description_data_list
            data = description_data_list[desc_id_idx]
            if data and isinstance(data, dict):
                try:
                    # Reconstruir objeto Description
                    # Asumiendo que data ya tiene todos los campos necesarios para Description
                    description = Description(**data)
                    score = scores_map.get(desc_id_val, 0.0)
                    results.append((description, score))
                    logger.debug(f"  SearchDescriptions: Reconstruida Description ID={desc_id_val}, Score={score:.4f}, Summary='{description.objective_summary[:50]}...'")
                except TypeError as e:
                    logger.warning(f"Error de tipo al reconstruir Description {desc_id_val}: {e} - Datos: {data}")
                except Exception as e:
                    logger.warning(f"Error genérico reconstruyendo Description {desc_id_val}: {e}")
            else:
                logger.warning(f"Descripción {desc_id_val} encontrada en VectorDB pero no en DescriptionStore o datos inválidos.")

        # Ordenar por score
        results.sort(key=lambda item: item[1], reverse=True)
        logger.info(f"SearchDescriptions: Devolviendo {len(results)} descripciones procesadas.")

    except Exception as e:
        logger.exception(f"Error durante búsqueda vectorial en descripciones: {e}")

    return results


---
File: /retrieval/search/kg_search.py
---

# retrieval/search/kg_query.py
from typing import List, Tuple, Optional, Any, Dict, Set, Union
import itertools # Añadir import
import asyncio
import networkx as nx # Necesario para la lógica de caminos
# Usar interfaces base, no la implementación específica
from ...storage.base import BaseKnowledgeGraph, BaseKVStore
from ...core.datatypes import Chunk
from ...utils.logging_setup import logger
from ...utils.helpers import GRAPH_FIELD_SEP # Importar separador si se usa

async def search_kg(
    entities: List[str], # Entidades identificadas por el QueryPlanner
    kg_graph: BaseKnowledgeGraph, # Interfaz del grafo
    chunk_store: BaseKVStore, # Para obtener texto de chunks
    top_k: int,
    max_path_depth: int = 2, # Profundidad máxima para buscar caminos (1=vecinos, 2=vecinos de vecinos)
    min_path_score: float = 0.1 # Umbral para considerar un camino (placeholder)
) -> List[Tuple[Chunk, float]]:
    """
    Busca chunks relevantes en el KG encontrando caminos entre las entidades de la consulta.
    Implementación simplificada inspirada en PathRAG usando NetworkX.
    """
    logger.debug(f"Iniciando búsqueda KG para entidades: {entities}, profundidad: {max_path_depth}")
    if not entities or not isinstance(kg_graph, nx.DiGraph): # Asegurar que es la implementación NX
        logger.warning("Búsqueda KG requiere entidades y una implementación NetworkX.")
        return []

    # Asegurar que las entidades existan en el grafo
    existing_entities = [e for e in entities if await kg_graph.has_node(e)]
    if len(existing_entities) < 1: # Necesitamos al menos 1 entidad, idealmente 2 para caminos
        logger.debug("Pocas entidades válidas encontradas en el grafo para búsqueda KG.")
        # Podríamos buscar vecinos de las entidades existentes si len < 2
        if len(existing_entities) == 1:
             neighbors_data = await kg_graph.get_neighbors(existing_entities[0])
             related_nodes = {n['id'] for n in neighbors_data} | {existing_entities[0]}
             logger.debug(f"Buscando vecinos para entidad única: {existing_entities[0]}. Encontrados: {len(related_nodes)-1}")
        else:
             return [] # No hay entidades en el grafo
    else:
        related_nodes: Set[str] = set(existing_entities) # Nodos directamente relevantes

    # Encontrar caminos entre pares de entidades existentes (si hay >= 2)
    paths_found = []
    if len(existing_entities) >= 2:
        for start_node, end_node in itertools.combinations(existing_entities, 2):
            try:
                # Usar all_simple_paths para evitar ciclos y limitar profundidad
                paths = list(nx.all_simple_paths(
                    kg_graph.graph, # Acceder al grafo NX directamente
                    source=start_node,
                    target=end_node,
                    cutoff=max_path_depth
                ))
                if paths:
                    paths_found.extend(paths)
                    # Añadir todos los nodos de los caminos encontrados al conjunto relacionado
                    for path in paths:
                        related_nodes.update(path)
            except nx.NetworkXNoPath:
                continue # No hay camino entre este par
            except nx.NodeNotFound:
                logger.warning(f"Nodo no encontrado durante búsqueda de caminos: {start_node} o {end_node}")
                continue
            except Exception as e:
                 logger.exception(f"Error inesperado buscando caminos KG: {e}")

    logger.debug(f"Nodos relevantes iniciales + caminos: {len(related_nodes)}")

    # Recolectar source_ids (IDs de chunks) de nodos y aristas relevantes
    relevant_chunk_ids: Set[str] = set()

    # Obtener source_ids de los nodos relevantes
    node_tasks = [kg_graph.get_node(node_id) for node_id in related_nodes]
    node_results = await asyncio.gather(*node_tasks, return_exceptions=True)
    for node_data in node_results:
        if isinstance(node_data, dict) and 'source_id' in node_data:
            source_ids = node_data['source_id'].split(GRAPH_FIELD_SEP)
            relevant_chunk_ids.update(s_id for s_id in source_ids if s_id)

    # Obtener source_ids de las aristas en los caminos encontrados (si los hubo)
    edges_in_paths: Set[Tuple[str, str]] = set()
    for path in paths_found:
        for i in range(len(path) - 1):
            u, v = tuple(sorted((path[i], path[i+1]))) # Ordenar para consistencia
            edges_in_paths.add((u, v))

    edge_tasks = [kg_graph.get_edge(u, v) for u, v in edges_in_paths]
    edge_results = await asyncio.gather(*edge_tasks, return_exceptions=True)
    for edge_data in edge_results:
        if isinstance(edge_data, dict) and 'source_id' in edge_data:
            source_ids = edge_data['source_id'].split(GRAPH_FIELD_SEP)
            relevant_chunk_ids.update(s_id for s_id in source_ids if s_id)

    logger.debug(f"Total de IDs de chunks relevantes encontrados vía KG: {len(relevant_chunk_ids)}")

    # Obtener Chunks desde el ChunkStore
    if not relevant_chunk_ids:
        return []

    chunk_ids_list = list(relevant_chunk_ids)
    chunk_data_list = await chunk_store.get_batch(chunk_ids_list)

    # Crear objetos Chunk y asignar score (basado en si estaba en nodo directo o camino?)
    final_results: List[Tuple[Chunk, float]] = []
    kg_direct_score = 0.7 # Score si el chunk vino de una entidad directa
    kg_path_score = 0.6   # Score si vino de un nodo/arista en un camino

    for chunk_id, data in zip(chunk_ids_list, chunk_data_list):
        if data:
            chunk = Chunk(
                id=chunk_id,
                text=data.get("text", ""),
                metadata=data.get("metadata", {}),
                token_count=data.get("token_count"),
                embedding=None # KG no devuelve embedding
            )
            # Determinar score (simplificado)
            # Necesitaríamos saber qué nodos/aristas apuntaban a este chunk_id
            # Por ahora, damos un score base.
            score = kg_path_score # Asumir que vino de un camino por defecto
            # Podríamos refinar si guardamos el origen del chunk_id

            final_results.append((chunk, score))
        else:
            logger.warning(f"Chunk {chunk_id} referenciado por KG no encontrado en ChunkStore.")

    # Ordenar por score y limitar
    final_results.sort(key=lambda item: item[1], reverse=True)
    logger.debug(f"Búsqueda KG devolvió {len(final_results)} chunks (antes de top_k).")

    return final_results[:top_k]


---
File: /retrieval/search/vector_search.py
---

# ares/retrieval/search/vector_search.py
import numpy as np
from typing import List, Dict, Optional, Any, Tuple
from ...storage.base import BaseVectorDB, BaseKVStore # <--- AÑADIR BaseKVStore
from ...core.datatypes import Chunk
from ...ingestion.embedder import LateChunkingEmbedder
from ...utils.logging_setup import logger

async def search_vector(
    query: str,
    top_k: int,
    vector_db: BaseVectorDB,
    embedder: LateChunkingEmbedder,
    chunk_store: BaseKVStore,
    filters: Optional[Dict[str, Any]] = None # Filtros del plan
) -> List[Tuple[Chunk, float]]:
    """Realiza búsqueda vectorial y puebla el texto del chunk desde chunk_store."""
    try:
        query_embedding_list = await embedder.embed_text([query])
        if not query_embedding_list or query_embedding_list[0] is None:
             logger.error("No se pudo generar embedding para la consulta vectorial.")
             return []
        query_emb_np = query_embedding_list[0]
        logger.debug(f"Vector Search: Query='{query}', Query Embedding Shape={query_emb_np.shape if query_emb_np is not None else 'None'}")

        # ---- MODIFICACIÓN AQUÍ ----
        final_filters_for_vdb = {"type": "chunk"} # Por defecto, buscar solo chunks de texto
        if filters:
            # Fusionar con filtros del plan, asegurando que 'type' no se sobreescriba si ya está
            # o decidiendo una estrategia (ej. los filtros del plan tienen prioridad)
            current_type_filter = filters.pop("type", None) # Quitar 'type' de los filtros del plan
            final_filters_for_vdb.update(filters) # Añadir el resto de los filtros del plan
            if current_type_filter and current_type_filter != "chunk":
                logger.warning(f"Planner especificó un filtro de tipo '{current_type_filter}' para una búsqueda 'vector' (chunk). Se usará 'chunk'.")
            # Si quieres permitir que el planador sobreescriba el tipo (ej. para buscar todo)
            # else if current_type_filter: final_filters_for_vdb["type"] = current_type_filter

        logger.debug(f"Vector Search: Usando filtros para Qdrant: {final_filters_for_vdb}")
        # vector_db.search devuelve Chunks con metadata y embedding, PERO SIN TEXTO (según el análisis anterior)
        retrieved_items_from_vdb: List[Chunk] = await vector_db.search(query_emb_np, top_k, final_filters_for_vdb)
        # ---- FIN MODIFICACIÓN ----

        if not retrieved_items_from_vdb:
            logger.debug("Búsqueda vectorial en VectorDB no devolvió IDs/scores.")
            return []

        chunk_ids_to_fetch_text = [item.id for item in retrieved_items_from_vdb]
        scores_map = {item.id: item.metadata.get("similarity_score", 0.0) for item in retrieved_items_from_vdb}
        # Guardar los embeddings recuperados para no perderlos
        embeddings_map = {item.id: item.embedding for item in retrieved_items_from_vdb}
        # Guardar metadatos originales de VDB
        vdb_metadata_map = {item.id: item.metadata for item in retrieved_items_from_vdb}


        # Obtener datos completos (incluido texto) desde KVStore
        logger.debug(f"Recuperando texto para {len(chunk_ids_to_fetch_text)} chunks de KVStore.")
        chunk_data_list_from_kv = await chunk_store.get_batch(chunk_ids_to_fetch_text)

        results = []
        for i, chunk_id in enumerate(chunk_ids_to_fetch_text):
            kv_data = chunk_data_list_from_kv[i]
            if kv_data and isinstance(kv_data, dict) and kv_data.get("text"):
                # Reconstruir el Chunk con texto del KVStore y embedding/score del VectorDB
                # Usar metadatos del KVStore como base, y añadir score de VDB
                chunk_metadata_from_kv = kv_data.get("metadata", {})
                # Sobrescribir o añadir metadatos específicos de la búsqueda vectorial si es necesario
                chunk_metadata_from_kv.update(vdb_metadata_map.get(chunk_id, {})) # Fusionar metadatos de VDB
                chunk_metadata_from_kv["similarity_score"] = scores_map.get(chunk_id, 0.0) # Asegurar que el score esté

                rehydrated_chunk = Chunk(
                    id=chunk_id,
                    text=kv_data["text"],
                    metadata=chunk_metadata_from_kv,
                    token_count=kv_data.get("token_count"),
                    embedding=embeddings_map.get(chunk_id) # Usar embedding de VDB
                )
                results.append((rehydrated_chunk, scores_map.get(chunk_id, 0.0)))
                logger.debug(f"  Chunk {rehydrated_chunk.id} rehidratado con texto: '{rehydrated_chunk.text[:50]}...'")
            else:
                logger.warning(f"No se encontraron datos de texto para chunk {chunk_id} en KVStore o datos inválidos.")

        results.sort(key=lambda item: item[1], reverse=True)
        logger.debug(f"Búsqueda vectorial (search_vector) procesó y devolvió {len(results)} resultados con texto.")
        return results

    except Exception as e:
        logger.exception(f"Error durante la búsqueda vectorial: {e}")
        return []


---
File: /retrieval/__init__.py
---




---
File: /retrieval/base.py
---

# ares/retrieval/base.py
from abc import ABC, abstractmethod
from typing import List, Optional

# Se importa desde el núcleo de ARES para mantener una dependencia clara.
from ..core.datatypes import RetrievedItem

class BaseRerankerWrapper(ABC):
    """
    Interfaz abstracta para todos los modelos de reranking.

    Su propósito es tomar una lista de items recuperados y reordenarlos
    según la relevancia para una consulta dada, actualizando sus scores.
    Esta abstracción permite que diferentes implementaciones de rerankers
    (cross-encoders, modelos generativos como Qwen3, APIs externas, etc.)
    se integren de manera uniforme en el pipeline de recuperación de ARES.
    """
    
    @abstractmethod
    async def rerank(
        self, 
        query: str, 
        items: List[RetrievedItem], 
        top_n: Optional[int] = None
    ) -> List[RetrievedItem]:
        """
        Reordena una lista de items recuperados basándose en su relevancia para la consulta.

        Args:
            query: La consulta original del usuario, usada como referencia para la relevancia.
            items: La lista de objetos RetrievedItem obtenidos de la fase de recuperación inicial.
                   Cada item ya tiene un score preliminar.
            top_n: El número máximo de items a devolver después del reranking. Si es None,
                   se devuelven todos los items reordenados.

        Returns:
            Una nueva lista de RetrievedItem, reordenada por el nuevo score de relevancia
            (de mayor a menor) y posiblemente truncada a `top_n` elementos. El campo 'score'
            de cada RetrievedItem debe ser actualizado con el nuevo valor del reranker.
        """
        pass


---
File: /retrieval/manager.py
---

# ares/retrieval/manager.py
import asyncio
from typing import List, Dict, Optional, Any, Tuple, Union, Set
import numpy as np
# import itertools # No se usa itertools directamente en el código final

from ..core.datatypes import SearchPlan, Chunk, RetrievedItem, SearchOperation, CombineOperation, Description
from ..storage.base import BaseKVStore, BaseVectorDB, BaseKnowledgeGraph
# from ..storage.bm25.whoosh_bm25 import WhooshBM25Index # <<< NO USADO
from ..ingestion.embedder import LateChunkingEmbedder
from ..ingestion.embedder_sparse import FastEmbedSparseEmbedder # <<< NUEVO
from ..utils.logging_setup import logger
from ..utils.helpers import normalize_vector, get_config_value # compute_mdhash_id no se usa aquí

# Las funciones de búsqueda específicas ya no se importan, la lógica se mueve aquí
# from .search.bm25_search import search_bm25
# from .search.vector_search import search_vector
# from .search.kg_search import search_kg
# from .search.description_search import search_descriptions
from .search.kg_search import search_kg # Mantenemos KG por ahora

class RetrievalManager:
    """Ejecuta el plan de búsqueda estructurado, combina resultados y aplica lógica de continuidad."""

    def __init__(self,
                 config: Dict[str, Any],
                 chunk_store: BaseKVStore,
                 description_store: BaseKVStore,
                 vector_db: BaseVectorDB,
                 kg_graph: Optional[BaseKnowledgeGraph],
                 bm25_index: Optional[Any], # Mantenido por compatibilidad de firma, no se usará
                 embedder: LateChunkingEmbedder,
                 sparse_embedder_for_query: Optional[FastEmbedSparseEmbedder] # <<< NUEVO
                 ):
        self.config = config
        self.retrieval_cfg = get_config_value(config, 'retrieval', {})
        self.chunk_store = chunk_store
        self.description_store = description_store
        self.vector_db = vector_db
        self.kg_graph = kg_graph
        self.bm25_index = None # Forzar a None
        self.embedder = embedder
        self.sparse_embedder_for_query = sparse_embedder_for_query # <<< NUEVO
        
        self.include_neighbors = self.retrieval_cfg.get("include_neighbor_chunks", True)
        self.continuity_neighbor_score = self.retrieval_cfg.get("continuity_neighbor_score", 0.01)

        logger.info("RetrievalManager (Plan Executor) inicializado.")

    async def _execute_search_step(self, step: SearchOperation, plan: SearchPlan) -> List[Tuple[Union[Chunk, Description], float]]:
        """Ejecuta una única operación de búsqueda del plan."""
        op_type = step.op_type
        params = step.parameters
        
        k = params.get("k", 30)
        filters = params.get("filters")
        
        # Casos especiales que no usan Qdrant directamente por ahora
        if op_type == "kg_path_search":
            if not self.kg_graph or not get_config_value(self.config, "retrieval.use_kg_search", False):
                logger.debug("Búsqueda KG solicitada pero KG no disponible/deshabilitado por config. Saltando paso.")
                return []
            
            start_nodes = params.get("start_nodes", []) 
            if not start_nodes:
                logger.warning("Búsqueda KG: No se proporcionaron 'start_nodes'. Saltando.")
                return []
            max_depth_kg = params.get("max_depth", 2)
            return await search_kg(start_nodes, self.kg_graph, self.chunk_store, k, max_depth_kg, filters)
        
        # --- Lógica Unificada para Búsqueda Vectorial/Híbrida en Qdrant ---
        
        query_text_for_dense = getattr(plan, 'query_for_embedding', None) or params.get("query_text", plan.original_query)
        query_text_for_sparse = getattr(plan, 'query_for_bm25', None) or params.get("query_text", plan.original_query)

        dense_query_vector, sparse_query_vector = None, None

        logger.info(f"Ejecutando paso de búsqueda: {op_type}, k={k}")

        try:
            # Generar vectores densos y dispersos según el tipo de operación
            if op_type in ["vector", "hybrid", "description"]:
                logger.debug(f"Paso '{op_type}': Generando vector denso para query: '{query_text_for_dense[:50]}...'")
                dense_list = await self.embedder.embed_text([query_text_for_dense])
                dense_query_vector = dense_list[0] if dense_list else None

            if op_type in ["bm25", "hybrid"] and self.sparse_embedder_for_query:
                logger.debug(f"Paso '{op_type}': Generando vector disperso para query: '{query_text_for_sparse[:50]}...'")
                sparse_list = await self.sparse_embedder_for_query.embed_sparse_batch([query_text_for_sparse])
                sparse_query_vector = sparse_list[0] if sparse_list else None
            elif op_type in ["bm25", "hybrid"] and not self.sparse_embedder_for_query:
                logger.warning("Búsqueda dispersa/bm25 solicitada pero sparse_embedder no está disponible. Saltando componente dispersa.")

            if dense_query_vector is None and sparse_query_vector is None:
                logger.error(f"No se pudo generar ningún vector de consulta para el paso {op_type}. No se puede buscar.")
                return []

            # <<< LLAMAR AL NUEVO MÉTODO DE BÚSQUEDA DE QDRANT >>>
            # Para búsqueda de descripciones, añadimos un filtro de metadatos
            if op_type == "description":
                if not get_config_value(self.config, "retrieval.use_description_search", False):
                     logger.debug("Búsqueda en descripciones deshabilitada, saltando.")
                     return []
                if filters:
                    filters.append({"key": "metadata.type", "match": {"value": "description"}})
                else:
                    filters = [{"key": "metadata.type", "match": {"value": "description"}}]

            retrieved_chunks = await self.vector_db.search(
                dense_query_vector=dense_query_vector,
                sparse_query_vector=sparse_query_vector,
                top_k=k,
                filters=filters
            )
            
            # El método search devuelve Chunks con 'similarity_score' en metadata
            # Lo convertimos al formato esperado: (Chunk/Description, score)
            results = []
            for chunk in retrieved_chunks:
                score = chunk.metadata.get("similarity_score", 0.0)
                # Si el chunk es de una descripción, lo tratamos como un objeto Description
                if chunk.metadata.get("type") == "description":
                    # Este es un placeholder, el objeto Description real no está aquí.
                    # _combine_results lo manejará.
                    results.append((chunk, score)) 
                else:
                    results.append((chunk, score))
            return results

        except Exception as e:
            logger.exception(f"Error ejecutando paso de búsqueda '{op_type}': {e}")
            return []

    def _normalize_scores(self, results: List[Tuple[Any, float]]) -> List[Tuple[Any, float]]:
        """Normaliza scores a [0, 1]. Maneja casos sin scores numéricos válidos."""
        if not results: return []
        
        valid_scores = [score for _, score in results if isinstance(score, (int, float))]
        if not valid_scores:
            logger.warning("No se encontraron scores válidos para normalizar. Devolviendo resultados originales.")
            return results
        
        scores_np = np.array(valid_scores, dtype=float)
        min_score = np.min(scores_np)
        max_score = np.max(scores_np)

        if max_score == min_score:
            normalized_scores_values = [1.0] * len(valid_scores)
        else:
            normalized_scores_values = ((scores_np - min_score) / (max_score - min_score)).tolist()
        
        final_results_normalized = []
        norm_idx = 0
        for item, original_score in results:
            if isinstance(original_score, (int, float)):
                final_results_normalized.append((item, normalized_scores_values[norm_idx]))
                norm_idx += 1
            else:
                final_results_normalized.append((item, original_score))
        return final_results_normalized

    def _combine_results(self,
                         step_results: Dict[int, List[Tuple[Union[Chunk, Description], float]]],
                         plan: SearchPlan) -> List[RetrievedItem]:
        """Combina resultados de múltiples pasos usando el método definido en el plan."""
        combined_items_dict: Dict[str, RetrievedItem] = {}
        item_map: Dict[str, Union[Chunk, Description]] = {} 

        combine_op_config = next((step for step in plan.steps if isinstance(step, CombineOperation)), None)
        
        method = self.retrieval_cfg.get("combination_method", "rrf")
        rrf_k_param = self.retrieval_cfg.get("rrf_k", 60)

        if combine_op_config:
            method = combine_op_config.method
            rrf_k_param = combine_op_config.parameters.get("rrf_k", rrf_k_param)

        logger.debug(f"Combinando resultados de {len(step_results)} pasos usando método: {method} (RRF_K: {rrf_k_param})")

        processed_step_results = {}
        if method == "weighted_sum":
            for idx, results in step_results.items():
                processed_step_results[idx] = self._normalize_scores(results)
        else:
            processed_step_results = step_results

        for step_index, results_with_scores in processed_step_results.items():
             original_search_op = plan.steps[step_index]
             step_weight = 1.0
             if isinstance(original_search_op, SearchOperation):
                 step_weight = original_search_op.weight
             
             if method == "weighted_sum" and step_weight == 0:
                 logger.debug(f"Paso {step_index} con peso 0 en weighted_sum. Omitiendo.")
                 continue

             sorted_results = sorted(results_with_scores, key=lambda x: x[1], reverse=True)

             for rank, (item_obj, item_score) in enumerate(sorted_results):
                 # El objeto ahora siempre es un Chunk, pero puede representar una descripción
                 if not isinstance(item_obj, Chunk):
                     logger.warning(f"Item inesperado de tipo {type(item_obj)} en resultados del paso {step_index}. Saltando.")
                     continue
                 
                 item_id = item_obj.id
                 is_description = item_obj.metadata.get("type") == "description"
                 source_tag_for_metadata = f"step_{step_index}_{'description' if is_description else 'chunk'}"

                 # El objeto que se guarda en el mapa es siempre el Chunk
                 if item_id not in item_map:
                     item_map[item_id] = item_obj

                 retrieved_item = combined_items_dict.get(item_id)
                 if retrieved_item is None:
                     chunk_for_item = item_map[item_id] 
                     
                     if is_description:
                         # Creamos un RetrievedItem con un chunk "placeholder"
                         retrieved_item = RetrievedItem(
                             chunk=Chunk(
                                 id=chunk_for_item.id,
                                 text=f"[Placeholder for Description {chunk_for_item.metadata.get('window_id', item_id)}]",
                                 metadata={
                                     "type": "description_placeholder",
                                     "window_id": chunk_for_item.metadata.get("window_id", item_id),
                                     **chunk_for_item.metadata
                                 },
                                 embedding=None # El embedding de descripción no se usa más allá de la búsqueda
                             ),
                             score=0.0, source="combined", metadata={"retrieval_sources": []}
                         )
                     else:
                         retrieved_item = RetrievedItem(
                             chunk=chunk_for_item,
                             score=0.0, source="combined", metadata={"retrieval_sources": []}
                         )
                     combined_items_dict[item_id] = retrieved_item
                 
                 if method == "weighted_sum":
                      retrieved_item.score += item_score * step_weight 
                 elif method == "rrf":
                      retrieved_item.score += (1.0 / (rrf_k_param + rank + 1))
                 else: 
                      retrieved_item.score += item_score * step_weight

                 if source_tag_for_metadata not in retrieved_item.metadata["retrieval_sources"]:
                      retrieved_item.metadata["retrieval_sources"].append(source_tag_for_metadata)
                 
                 if retrieved_item.chunk.embedding is not None:
                      if retrieved_item.normalized_embedding is None: 
                           try:
                               retrieved_item.normalized_embedding = normalize_vector(retrieved_item.chunk.embedding)
                           except Exception as e_norm:
                               logger.error(f"Error normalizando embedding para chunk {retrieved_item.chunk.id}: {e_norm}")
        
        final_items = sorted(list(combined_items_dict.values()), key=lambda item: item.score, reverse=True)
        intermediate_k_results = self.retrieval_cfg.get("intermediate_top_k", 100)
        
        logger.debug(f"Resultados combinados: {len(final_items)} items únicos. Devolviendo top {intermediate_k_results}.")
        return final_items[:intermediate_k_results]

    async def _get_neighbor_chunk_ids(self, window_id: str, chunk_idx: int, direction: str, window_metadata_cache: Dict[str, Optional[Dict[str, Any]]]) -> Optional[str]:
        """Obtiene el ID del chunk vecino (prev/next) de una ventana, usando caché."""
        if window_id not in window_metadata_cache:
            try:
                window_meta_key = f"win_meta_{window_id}"
                window_meta_content = await self.chunk_store.get(window_meta_key)
                window_metadata_cache[window_id] = window_meta_content
            except Exception as e:
                logger.error(f"Error obteniendo metadata de ventana {window_id} para continuidad: {e}")
                window_metadata_cache[window_id] = None
                return None
        
        window_meta_content = window_metadata_cache[window_id]

        if not window_meta_content or "chunk_ids" not in window_meta_content or not isinstance(window_meta_content["chunk_ids"], list):
            if window_meta_content is not None: 
                 logger.warning(f"Metadatos inválidos o faltantes ('chunk_ids' list) para ventana {window_id} en continuidad.")
                 window_metadata_cache[window_id] = None
            return None
            
        chunk_ids_list = window_meta_content["chunk_ids"]
        target_idx = chunk_idx + (1 if direction == "next" else -1)

        if 0 <= target_idx < len(chunk_ids_list):
            return chunk_ids_list[target_idx]
        else:
            return None

    async def _apply_continuity_logic(self,
                                     items: List[RetrievedItem],
                                     original_chunk_ids: Set[str]) -> List[RetrievedItem]:
        """Añade chunks vecinos a los recuperados si están ausentes."""
        if not self.include_neighbors:
            logger.debug("Lógica de continuidad deshabilitada. Saltando.")
            return items

        final_item_map: Dict[str, RetrievedItem] = {
            item.chunk.id: item for item in items 
            if isinstance(item.chunk, Chunk) and item.chunk.metadata.get("type") != "description_placeholder"
        }
        
        ids_to_fetch: Set[str] = set()
        window_metadata_cache: Dict[str, Optional[Dict]] = {}
        checked_neighbors: Set[Tuple[str, int, str]] = set()

        logger.debug(f"Aplicando continuidad. Items iniciales (con chunk real para expansión): {len(final_item_map)}")

        for item in items:
            if not (isinstance(item.chunk, Chunk) and item.chunk.metadata.get("type") != "description_placeholder"):
                continue

            metadata = item.chunk.metadata
            window_id = metadata.get("window_id")
            chunk_idx = metadata.get("chunk_index_in_window")

            if window_id is None or chunk_idx is None:
                continue

            for direction in ["prev", "next"]:
                neighbor_check_key = (window_id, chunk_idx, direction)
                if neighbor_check_key in checked_neighbors:
                    continue 
                checked_neighbors.add(neighbor_check_key)

                neighbor_id = await self._get_neighbor_chunk_ids(window_id, chunk_idx, direction, window_metadata_cache)
                
                if neighbor_id and neighbor_id not in final_item_map:
                    ids_to_fetch.add(neighbor_id)

        if ids_to_fetch:
            logger.info(f"Continuity Logic: Fetching {len(ids_to_fetch)} unique neighbor chunks.")
            try:
                neighbor_data_list = await self.chunk_store.get_batch(list(ids_to_fetch))
            except Exception as e:
                logger.error(f"Error en get_batch para chunks vecinos: {e}")
                neighbor_data_list = [None] * len(ids_to_fetch)
            
            neighbor_data_map = {id_: data for id_, data in zip(list(ids_to_fetch), neighbor_data_list) if data and isinstance(data, dict)}

            valid_neighbors_info: List[Tuple[str, Dict[str, Any]]] = []
            texts_to_embed_neighbors: List[str] = []
            ids_for_neighbor_embeddings: List[str] = []

            for chunk_id_neighbor in ids_to_fetch:
                data_neighbor = neighbor_data_map.get(chunk_id_neighbor)
                if data_neighbor and data_neighbor.get("text") is not None:
                     valid_neighbors_info.append((chunk_id_neighbor, data_neighbor))
                     texts_to_embed_neighbors.append(data_neighbor.get("text", ""))
                     ids_for_neighbor_embeddings.append(chunk_id_neighbor)
                else:
                     logger.warning(f"Datos inválidos o faltantes para chunk vecino {chunk_id_neighbor}. No se añadirá.")
            
            neighbor_embeddings_map: Dict[str, Optional[np.ndarray]] = {}
            if texts_to_embed_neighbors:
                 logger.debug(f"Embedding {len(texts_to_embed_neighbors)} neighbor texts for continuity.")
                 try:
                      embeddings_list_neighbors = await self.embedder.embed_text(texts_to_embed_neighbors)
                      if len(embeddings_list_neighbors) == len(ids_for_neighbor_embeddings):
                           neighbor_embeddings_map = dict(zip(ids_for_neighbor_embeddings, embeddings_list_neighbors))
                      else:
                           logger.error("Discrepancia en el número de embeddings y IDs para vecinos de continuidad.")
                 except Exception as e_emb_neigh:
                      logger.error(f"Error embedding neighbor chunks for continuity: {e_emb_neigh}")

            for chunk_id_neighbor, data_neighbor in valid_neighbors_info:
                embedding_neighbor = neighbor_embeddings_map.get(chunk_id_neighbor)
                norm_emb_neighbor = normalize_vector(embedding_neighbor) if embedding_neighbor is not None else None
                
                metadata_neighbor = data_neighbor.get("metadata", {})
                metadata_neighbor["continuity_added"] = True

                chunk_neighbor = Chunk(
                    id=chunk_id_neighbor,
                    text=data_neighbor.get("text", ""),
                    metadata=metadata_neighbor,
                    token_count=data_neighbor.get("token_count"),
                    embedding=embedding_neighbor
                )
                
                final_item_map[chunk_id_neighbor] = RetrievedItem(
                    chunk=chunk_neighbor,
                    score=self.continuity_neighbor_score,
                    source="continuity",
                    normalized_embedding=norm_emb_neighbor,
                    metadata={"retrieval_sources": ["continuity"]}
                )
                logger.debug(f"Añadido vecino {chunk_id_neighbor} a los resultados con score {self.continuity_neighbor_score}.")
        else:
            logger.debug("Continuity Logic: No new neighbor chunks to fetch or add.")

        return list(final_item_map.values())

    async def execute_plan_and_retrieve(self, plan: SearchPlan) -> List[RetrievedItem]:
         """Ejecuta un plan de búsqueda completo y devuelve los items combinados y con continuidad."""
         logger.info(f"Ejecutando plan de búsqueda para: '{plan.original_query[:50]}...'")
         logger.debug(f"Plan completo: {plan.model_dump_json(indent=2)}")

         step_results: Dict[int, List[Tuple[Union[Chunk, Description], float]]] = {}
         search_tasks = []
         search_indices = []

         if not plan.steps:
              logger.info("Plan de búsqueda no contiene pasos. Devolviendo lista vacía.")
              return []

         for i, step in enumerate(plan.steps):
              if isinstance(step, SearchOperation):
                   logger.info(f"Plan: Agendando paso de búsqueda {i} - Tipo: {step.op_type}, Peso: {step.weight}, Params: {step.parameters}")
                   search_tasks.append(self._execute_search_step(step, plan))
                   search_indices.append(i)
              elif isinstance(step, CombineOperation):
                   logger.info(f"Plan: Paso de combinación {i} encontrado (método: {step.method}, params: {step.parameters})")
         
         if not search_tasks:
              logger.warning("El plan no contiene pasos de búsqueda ejecutables. Devolviendo lista vacía.")
              return []

         search_outputs = await asyncio.gather(*search_tasks, return_exceptions=True)

         valid_results_count = 0
         for i, output in enumerate(search_outputs):
             step_idx = search_indices[i]
             op_type_log = "Unknown"
             if step_idx < len(plan.steps) and isinstance(plan.steps[step_idx], SearchOperation):
                 op_type_log = plan.steps[step_idx].op_type

             if isinstance(output, Exception):
                  logger.error(f"Error en el paso de búsqueda {step_idx} (Tipo: {op_type_log}): {output}")
                  step_results[step_idx] = []
             elif output is None:
                  logger.warning(f"El paso de búsqueda {step_idx} (Tipo: {op_type_log}) retornó None. Tratando como sin resultados.")
                  step_results[step_idx] = []
             else:
                  logger.info(f"Paso de búsqueda {step_idx} (Tipo: {op_type_log}) completado. Resultados: {len(output)}")
                  step_results[step_idx] = output
                  if output:
                      valid_results_count +=1
        
         if valid_results_count == 0 and not any(isinstance(step, CombineOperation) for step in plan.steps):
             logger.warning("Ningún paso de búsqueda produjo resultados válidos. Devolviendo lista vacía.")

         combined_items = self._combine_results(step_results, plan)
         logger.info(f"Combinación inicial produjo {len(combined_items)} items únicos.")

         original_real_chunk_ids = {
             item.chunk.id for item in combined_items 
             if isinstance(item.chunk, Chunk) and item.chunk.metadata.get("type") != "description_placeholder"
         }

         final_items_with_continuity = await self._apply_continuity_logic(combined_items, original_real_chunk_ids)
         logger.info(f"Lógica de continuidad aplicada. Total items: {len(final_items_with_continuity)}.")

         final_items_with_continuity.sort(key=lambda x: x.score, reverse=True)

         if final_items_with_continuity:
              top_n_log = min(3, len(final_items_with_continuity))
              logger.info(f"Top {top_n_log} items finales (de {len(final_items_with_continuity)}):")
              for i, item in enumerate(final_items_with_continuity[:top_n_log]):
                   chunk_type_info = "Chunk"
                   if item.chunk.metadata.get("type") == "description_placeholder":
                       chunk_type_info = f"Placeholder (Desc {item.chunk.metadata.get('window_id', item.chunk.id)})"
                   elif item.chunk.metadata.get("continuity_added"):
                       chunk_type_info = "Chunk (Continuity)"
                   
                   source_info = item.metadata.get('retrieval_sources', [])
                   if not source_info and item.source:
                       source_info = [item.source]

                   logger.info(f"  {i+1}. ID: {item.chunk.id}, Score: {item.score:.4f}, Type: {chunk_type_info}, Sources: {source_info}, Text: '{item.chunk.text[:80]}...'")
         else:
              logger.info("No se recuperaron items finales después de todos los pasos.")
              
         return final_items_with_continuity


---
File: /retrieval/planner.py
---

# retrieval/planner.py
import json
import re
from typing import List, Dict, Optional, Any, Union # Asegurar Union

from ..llm.base import BaseLLMWrapper
from ..core.datatypes import SearchPlan, SubQuery # SearchOperation, CombineOperation ya no se usan aquí directamente para crear el plan
from ..utils.logging_setup import logger
from ..utils.helpers import log_or_save_raw_json, get_config_value # get_config_value movido a helpers

# PLANNER_SYSTEM_PROMPT_INITIAL (para la primera pasada o modo Lite)
# Este prompt pide al LLM que descomponga la query si es compleja.
PLANNER_SYSTEM_PROMPT_INITIAL = """
Eres un planificador experto para ARES. Analiza la 'Consulta Usuario' y el 'Contexto Reciente'.
Tu tarea es:
1.  Si la consulta es compleja y se beneficiaría de múltiples perspectivas o requiere información de diferentes facetas, genera una lista de 2 a 4 SUB-CONSULTAS específicas y concisas. Cada sub-consulta debe ser autocontenida y enfocarse en un aspecto de la consulta original.
2.  Si la consulta es simple y directa, devuelve una lista con UNA ÚNICA sub-consulta que sea idéntica o una ligera reformulación de la consulta original.

Contexto Reciente:
{recent_history}
------
Consulta Usuario:
{user_query}
------

Formato de Salida: Una lista JSON de objetos. Cada objeto representa una sub-consulta.
Ejemplo para consulta COMPLEJA ("Compara los efectos del cambio climático en la agricultura y los océanos y menciona soluciones"):
```json
[
  {{"id": "sq1", "text": "¿Cuáles son los impactos específicos del cambio climático en la agricultura?"}},
  {{"id": "sq2", "text": "¿Cómo afecta el cambio climático a los ecosistemas oceánicos y niveles del mar?"}},
  {{"id": "sq3", "text": "¿Qué soluciones se proponen para mitigar los efectos del cambio climático en agricultura y océanos?"}}
]
```
Ejemplo para consulta SIMPLE ("¿Qué es ARES?"):
```json
[
  {{"id": "sq1", "text": "¿Qué es el sistema ARES y cuáles son sus características principales?"}}
]
```
Responde ÚNICAMENTE con la lista JSON válida. No incluyas explicaciones ni texto adicional fuera del bloque JSON.
"""

# PLANNER_SYSTEM_PROMPT_REFINEMENT (para modo Iterativo, pasadas > 1)
# Este prompt recibe la query original, sub-consultas previas y sus resultados,
# y decide si responder, refinar o parar.
PLANNER_SYSTEM_PROMPT_REFINEMENT = """
Eres un planificador y razonador experto para ARES. Has estado trabajando en una 'Consulta Original del Usuario'.
Se te proporcionan las 'Sub-Consultas Anteriores Ejecutadas' y el 'Contexto Acumulado de sus Resultados'.
Tu tarea es decidir el siguiente paso:
1.  **RESPONDER:** Si crees que el 'Contexto Acumulado' es suficiente para responder completamente la 'Consulta Original', indica que estás listo para responder.
2.  **REFINAR (NUEVAS SUB-CONSULTAS):** Si el contexto es insuficiente o necesitas más detalles/facetas, genera una NUEVA lista de 1 a 3 sub-consultas específicas para obtener la información faltante. No repitas sub-consultas que ya dieron buenos resultados.
3.  **SIN RESPUESTA:** Si después de los intentos consideras que no se puede encontrar una respuesta satisfactoria con la información disponible o la consulta es inherentemente no respondible por el sistema, indica esto.

Consulta Original del Usuario:
{original_query}
------
Historial de Sub-Consultas y Contexto Acumulado:
{sub_queries_and_results_history}
------

Formato de Salida: Un ÚNICO objeto JSON con una de las siguientes estructuras:

Opción 1: Listo para Responder
```json
{{
  "decision": "final_answer_ready",
  "summary_of_findings_for_responder": "Un breve resumen de los hallazgos clave del contexto acumulado que el LLM final debe usar para construir la respuesta a la consulta original."
}}
```

Opción 2: Necesita Refinamiento (Nuevas Sub-Consultas)
```json
{{
  "decision": "needs_refinement",
  "new_sub_queries": [
    {{"id": "sq_next1", "text": "Nueva sub-consulta específica..."}},
    {{"id": "sq_next2", "text": "Otra nueva sub-consulta..."}}
  ],
  "reasoning_for_refinement": "Breve explicación de por qué se necesita más información y qué se espera de las nuevas sub-consultas."
}}
```

Opción 3: No se Puede Responder
```json
{{
  "decision": "complete_no_answer",
  "reasoning": "Explicación de por qué no se puede responder (ej. información no encontrada después de varios intentos)."
}}
```
Responde ÚNICAMENTE con el objeto JSON válido.
"""

class QueryPlanner:
    def __init__(self, llm_wrapper: BaseLLMWrapper, config: Optional[Dict[str, Any]] = None):
        self.llm = llm_wrapper
        self.config = config if config else {}
        # El prompt se seleccionará dinámicamente en generate_plan
        logger.info("QueryPlanner (SubQuery & Iterative-Capable) inicializado.")

    def _format_history(self, history: Optional[List[Dict[str, str]]]) -> str:
        if not history:
            return "No hay historial reciente."
        max_history_len = 1000 # Caracteres
        formatted = "\n".join([f"{msg['role']}: {msg['content']}" for msg in history[-6:]]) # Limitar a últimos 3 turnos (6 mensajes)
        if len(formatted) > max_history_len:
             start_index = len(formatted) - max_history_len
             formatted = "..." + formatted[start_index:]
        return formatted

    def _parse_llm_response_for_plan(self, llm_output: str, original_query: str, is_refinement_call: bool) -> Optional[SearchPlan]:
        logger.debug(f"Respuesta cruda del LLM Planner ({'Refinement' if is_refinement_call else 'Initial'}): {llm_output}")
        
        json_str_extracted_for_parsing = None
        try:
            # Lógica robusta de extracción de JSON (Markdown o plain)
            # Intenta primero con ```json ... ```
            match_markdown_json = re.search(r"```json\s*([\{\[])[\s\S]*?([\}\]])\s*```", llm_output, re.DOTALL)
            if match_markdown_json:
                # Reconstruir el JSON completo del grupo de captura
                # Group 0 es todo el match, incluyendo ```json y ```
                # Group 1 es el primer bracket ([ o {)
                # Group 2 es el último bracket (] o })
                # Es más seguro tomar desde el inicio del group 1 hasta el final del group 2 dentro del string original
                # o simplemente limpiar el group(0)
                json_str_raw = match_markdown_json.group(0).replace("```json", "").replace("```", "").strip()
            else: # Intentar encontrar JSON sin markdown
                # Buscar el primer '{' o '[' y el último '}' o ']' que formen un bloque coherente
                first_bracket_match = re.search(r"[\{\[]", llm_output)
                if first_bracket_match:
                    start_idx = first_bracket_match.start()
                    # Determinar el tipo de bracket de cierre esperado
                    open_bracket = llm_output[start_idx]
                    close_bracket = '}' if open_bracket == '{' else ']'
                    
                    # Buscar el último bracket de cierre correspondiente
                    # Esto es simplista y puede fallar con JSONs anidados incorrectamente formateados fuera de un bloque principal
                    # Pero es un intento razonable para JSON "desnudo"
                    # Para encontrar el último, buscamos desde el final del string
                    # end_idx_rev es el índice desde el final del string
                    last_bracket_rev_match = re.search(re.escape(close_bracket), llm_output[::-1])
                    if last_bracket_rev_match:
                        end_idx = len(llm_output) - last_bracket_rev_match.start()
                        json_str_raw = llm_output[start_idx:end_idx].strip()
                    else:
                        json_str_raw = None # No se encontró un bracket de cierre
                else:
                    json_str_raw = None # No se encontró un bracket de inicio
            
            if not json_str_raw:
                log_or_save_raw_json(llm_output, f"Planner_{'Refine' if is_refinement_call else 'Initial'}_NoJSONBrackets", query_text=original_query, base_working_dir=get_config_value(self.config, "general.working_dir", "."))
                logger.error("Planner LLM no devolvió JSON reconocible (ni Markdown ni plain).")
                return None
            
            # Limpiar comentarios (// y /* */)
            json_str_cleaned = re.sub(r"//.*", "", json_str_raw)
            json_str_cleaned = re.sub(r"/\*.*?\*/", "", json_str_cleaned, flags=re.DOTALL).strip()
            json_str_extracted_for_parsing = json_str_cleaned

            if not json_str_extracted_for_parsing:
                log_or_save_raw_json(llm_output, f"Planner_{'Refine' if is_refinement_call else 'Initial'}_EmptyAfterClean", query_text=original_query, base_working_dir=get_config_value(self.config, "general.working_dir", "."))
                logger.warning("JSON extraído del Planner quedó vacío después de limpiar comentarios.")
                return None

            log_or_save_raw_json(json_str_extracted_for_parsing, f"Planner_{'Refine' if is_refinement_call else 'Initial'}_PreParse", query_text=original_query, base_working_dir=get_config_value(self.config, "general.working_dir", "."))
            
            parsed_data = json.loads(json_str_extracted_for_parsing)

            if is_refinement_call: # Espera un objeto con "decision"
                if not isinstance(parsed_data, dict) or "decision" not in parsed_data:
                    logger.error(f"Respuesta de refinamiento de Planner no tiene 'decision' o no es un dict: {parsed_data}")
                    return None
                
                decision = parsed_data["decision"]
                reasoning = parsed_data.get("reasoning") or parsed_data.get("reasoning_for_refinement")

                if decision == "final_answer_ready":
                    return SearchPlan(
                        original_query=original_query,
                        status="final_answer_ready",
                        final_answer_from_planner=parsed_data.get("summary_of_findings_for_responder"), # Puede ser None
                        reasoning=reasoning
                    )
                elif decision == "needs_refinement":
                    new_sq_list = parsed_data.get("new_sub_queries", [])
                    sub_queries = [SubQuery(id=sq.get("id", f"sq_ref{i}"), text=sq["text"]) for i, sq in enumerate(new_sq_list) if isinstance(sq, dict) and "text" in sq and sq["text"].strip()]
                    if not sub_queries: 
                        logger.warning("Refinement pidió nuevas sub-queries pero no se proporcionaron válidas. Considerado como 'complete_no_answer'.")
                        return SearchPlan(original_query=original_query, status="complete_no_answer", reasoning=reasoning or "Planner pidió refinar pero no dio subconsultas.")
                    return SearchPlan(original_query=original_query, status="needs_refinement", sub_queries=sub_queries, reasoning=reasoning)
                elif decision == "complete_no_answer":
                    return SearchPlan(original_query=original_query, status="complete_no_answer", reasoning=reasoning)
                else:
                    logger.error(f"Decisión desconocida del Planner de refinamiento: {decision}")
                    return SearchPlan(original_query=original_query, status="complete_no_answer", reasoning=f"Decisión desconocida del planner: {decision}")
            
            else: # Llamada inicial, espera una lista de sub-consultas
                if not isinstance(parsed_data, list):
                    logger.error(f"Respuesta inicial de Planner no es una lista: {parsed_data}")
                    # Fallback a usar la query original si la estructura es incorrecta pero hay texto
                    if isinstance(parsed_data, dict) and parsed_data.get("text"): # Si por error mandó un objeto en vez de lista de objetos
                         sub_queries = [SubQuery(id="sq_fallback_struct", text=parsed_data["text"])]
                    elif isinstance(parsed_data, dict) and parsed_data.get("sub_queries") and isinstance(parsed_data.get("sub_queries"),list): # Si mandó el formato de refinamiento por error
                         initial_sq_list = parsed_data.get("sub_queries", [])
                         sub_queries = [SubQuery(id=sq.get("id", f"sq_init_err{i}"), text=sq["text"]) for i, sq in enumerate(initial_sq_list) if isinstance(sq, dict) and "text" in sq and sq["text"].strip()]
                         if not sub_queries:
                            sub_queries = [SubQuery(id="sq_fallback_orig", text=original_query)]
                    else:
                        logger.warning("Planner inicial no generó estructura de lista. Usando query original como sub-consulta única.")
                        sub_queries = [SubQuery(id="sq_fallback_orig", text=original_query)]

                    return SearchPlan(original_query=original_query, sub_queries=sub_queries, status="pending_execution")

                sub_queries = [SubQuery(id=sq.get("id", f"sq_init{i}"), text=sq["text"]) for i, sq in enumerate(parsed_data) if isinstance(sq, dict) and "text" in sq and sq["text"].strip()]
                if not sub_queries:
                    logger.warning("Planner inicial no generó sub-consultas válidas (lista vacía o con textos vacíos). Usando query original como sub-consulta única.")
                    sub_queries = [SubQuery(id="sq_fallback_empty", text=original_query)]
                
                # Para el modo "Lite", el status será "pending_execution" y AresCore construirá el plan de operaciones
                return SearchPlan(original_query=original_query, sub_queries=sub_queries, status="pending_execution")

        except json.JSONDecodeError as e:
            logger.error(f"Error parseando JSON del Planner LLM: {e}. JSON intentado: '{str(json_str_extracted_for_parsing)[:500]}...'")
            # Fallback a un plan simple si es llamada inicial, o error si es refinamiento
            if not is_refinement_call:
                return SearchPlan(original_query=original_query, sub_queries=[SubQuery(id="sq_json_err_fallback", text=original_query)], status="pending_execution")
            else:
                return SearchPlan(original_query=original_query, status="complete_no_answer", reasoning=f"Error decodificando JSON del planner: {str(e)[:100]}")

        except Exception as e:
            logger.exception(f"Error inesperado creando SearchPlan: {e}. Respuesta LLM: {llm_output[:200]}...")
            if not is_refinement_call:
                return SearchPlan(original_query=original_query, sub_queries=[SubQuery(id="sq_unk_err_fallback", text=original_query)], status="pending_execution")
            else:
                return SearchPlan(original_query=original_query, status="complete_no_answer", reasoning=f"Excepción inesperada en planner: {str(e)[:100]}")

    async def generate_plan(self, 
                            query: str, 
                            recent_history: Optional[List[Dict[str, str]]] = None,
                            # Para modo iterativo:
                            is_refinement_call: bool = False,
                            previous_sub_queries_and_results: Optional[str] = None # Historial formateado de sub-consultas y sus resultados
                           ) -> Optional[SearchPlan]:
        
        history_str = self._format_history(recent_history)
        llm_call_type_for_log = 'refinement' if is_refinement_call else 'initial'
        
        if is_refinement_call:
            if not previous_sub_queries_and_results:
                logger.error("Llamada de refinamiento a Planner sin historial de sub-consultas previas. No se puede planificar.")
                return SearchPlan(original_query=query, status="complete_no_answer", reasoning="Error interno: falta historial para refinamiento.")
            
            try:
                prompt = PLANNER_SYSTEM_PROMPT_REFINEMENT.format(
                    original_query=query,
                    sub_queries_and_results_history=previous_sub_queries_and_results
                )
            except KeyError as e:
                logger.error(f"Error formateando prompt de refinamiento del Planner: falta la clave {e}. Usando prompt básico.")
                # Esto es problemático para refinamiento, podría ser mejor devolver un error.
                return SearchPlan(original_query=query, status="complete_no_answer", reasoning=f"Error interno: prompt de refinamiento incompleto ({e}).")
            
            max_tokens_planner = int(get_config_value(self.config, "planner.max_tokens_refinement", 700))
        else: # Llamada inicial
            try:
                prompt = PLANNER_SYSTEM_PROMPT_INITIAL.format(
                    recent_history=history_str,
                    user_query=query
                )
            except KeyError as e:
                logger.error(f"Error formateando prompt inicial del Planner: falta la clave {e}. Usando prompt básico.")
                # Fallback a un prompt muy simple si el formateo falla
                prompt = f"Analiza la siguiente consulta de usuario y genera sub-consultas si es compleja, o reformúlala si es simple. Consulta: {query}"

            max_tokens_planner = int(get_config_value(self.config, "planner.max_tokens_initial", 500))
        
        planner_temperature = float(get_config_value(self.config, "planner.temperature", 0.1))

        try:
            llm_response = await self.llm.generate(
                prompt=prompt, 
                stream=False, 
                temperature=planner_temperature, 
                max_tokens=max_tokens_planner
            )
            
            if not llm_response or not isinstance(llm_response, str) or not llm_response.strip():
                logger.error(f"Planner LLM devolvió una respuesta vacía o inválida (call_type: {llm_call_type_for_log}).")
                # Fallback a un plan simple si es llamada inicial, o error si es refinamiento
                if not is_refinement_call:
                    return SearchPlan(original_query=query, sub_queries=[SubQuery(id="sq_empty_llm_fallback", text=query)], status="pending_execution")
                else:
                    return SearchPlan(original_query=query, status="complete_no_answer", reasoning="Planner LLM devolvió respuesta vacía en refinamiento.")

            return self._parse_llm_response_for_plan(llm_response, query, is_refinement_call)

        except Exception as e: 
            logger.exception(f"Error fatal durante la generación del plan de búsqueda (call_type: {llm_call_type_for_log}): {e}")
            # Fallback a un plan simple si es llamada inicial, o error si es refinamiento
            if not is_refinement_call:
                return SearchPlan(original_query=query, sub_queries=[SubQuery(id="sq_ex_fallback", text=query)], status="pending_execution")
            else:
                return SearchPlan(original_query=query, status="complete_no_answer", reasoning=f"Excepción en planner durante refinamiento: {str(e)[:100]}")


---
File: /retrieval/reranker_qwen.py
---

# ares/retrieval/reranker_qwen.py

from typing import List, Tuple, Optional, Dict, Any
import numpy as np
import math
import asyncio

from .base import BaseRerankerWrapper 
from ..llm.base import BaseLLMWrapper
from ..core.datatypes import RetrievedItem
from ..utils.logging_setup import logger

class QwenRerankerWrapper(BaseRerankerWrapper):
    """
    Wrapper para los modelos de reranking de Qwen.
    Calcula un score de relevancia basado en los logprobs de los tokens 'yes' y 'no'.
    """
    def __init__(self, llm_wrapper: BaseLLMWrapper, tokenizer: Any):
        """
        Args:
            llm_wrapper: Una instancia de un LLMWrapper para comunicarse con el modelo.
            tokenizer: El tokenizer del modelo reranker, necesario para obtener los token IDs.
        """
        self.llm = llm_wrapper
        self.tokenizer = tokenizer
        
        try:
            # Aunque la nueva lógica usa tokens de texto, mantenemos esto por si se revierte
            # o para comprobaciones iniciales.
            self.yes_token_id = self.tokenizer.encode("yes", add_special_tokens=False)[0]
            self.no_token_id = self.tokenizer.encode("no", add_special_tokens=False)[0]
            logger.info(f"QwenRerankerWrapper inicializado. 'yes' token ID: {self.yes_token_id}, 'no' token ID: {self.no_token_id}")
        except Exception as e:
            logger.exception(f"No se pudieron obtener los token IDs para 'yes'/'no'. Reranker podría no funcionar. Error: {e}")
            self.yes_token_id = -1 
            self.no_token_id = -1

    def _format_prompt(self, query: str, doc: str) -> List[Dict[str, str]]:
        """Formatea el prompt según la plantilla de Qwen para tareas de reranking."""
        return [
            {"role": "system", "content": "Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\"."},
            {"role": "user", "content": f"<Instruct>: Given a web search query, retrieve relevant passages that answer the query\n\n<Query>: {query}\n\n<Document>: {doc}"}
        ]

    async def rerank(self, query: str, items: List[RetrievedItem], top_n: Optional[int] = None) -> List[RetrievedItem]:
        if not items or self.yes_token_id == -1:
            return items
            
        if self.yes_token_id == -1 or self.no_token_id == -1:
            logger.warning("Token IDs para 'yes'/'no' no fueron inicializados. Saltando reranking.")
            return items

        prompts_for_batch = [self._format_prompt(query, item.chunk.text) for item in items]
        
        # <<< INICIO DE LA MODIFICACIÓN >>>
        tasks = []
        for conv in prompts_for_batch:
            # Llamamos a 'generate' pidiendo explícitamente los logprobs
            task = self.llm.generate(
                prompt=conv[-1]['content'],
                history=conv[:-1],
                max_tokens=2,      # Suficiente para obtener 'yes' o 'no'
                logprobs=True,     # ¡Solicitar logprobs!
                top_logprobs=10,   # Obtener un top-N para asegurar que 'yes'/'no' estén
                return_full_response=True # ¡Pedir el objeto de respuesta completo!
            )
            tasks.append(task)
        
        outputs = await asyncio.gather(*tasks, return_exceptions=True)
        # <<< FIN DE LA MODIFICACIÓN >>>
        
        new_scores = []
        for output in outputs:
            # Si gather capturó una excepción para esta tarea o la respuesta no es válida
            if isinstance(output, Exception) or not hasattr(output, 'choices') or not output.choices:
                logger.error(f"Error en una de las llamadas de reranking: {output}")
                new_scores.append(0.0)
                continue
            
            try:
                choice = output.choices[0]
                logprob_content = choice.logprobs.content if choice.logprobs else []

                logprob_dict = {}
                if logprob_content and logprob_content[0].top_logprobs:
                    # Los logprobs del primer token generado
                    first_token_logprobs = logprob_content[0].top_logprobs
                    # El token puede ser string o int dependiendo de la versión/backend.
                    # Convertir a string para una búsqueda consistente.
                    logprob_dict = {str(lp.token): lp.logprob for lp in first_token_logprobs}
                else:
                    logger.warning("La respuesta del modelo no contenía logprobs.")

                # Ahora usamos los tokens string, que es más robusto
                logit_yes = logprob_dict.get("yes", -100.0)
                logit_no = logprob_dict.get("no", -100.0)
                
                exp_yes = math.exp(logit_yes)
                exp_no = math.exp(logit_no)
                score = exp_yes / (exp_yes + exp_no) if (exp_yes + exp_no) > 0 else 0.0
                new_scores.append(score)

            except (AttributeError, IndexError, TypeError) as e:
                logger.error(f"Error procesando la salida del modelo para reranking: {e}. Output: {output}")
                new_scores.append(0.0) # Score bajo si la estructura de la respuesta no es la esperada

        if len(new_scores) != len(items):
            logger.error("Discrepancia entre el número de scores calculados y el número de items. Saltando reranking.")
            return items

        for item, score in zip(items, new_scores):
            item.score = score
        
        reranked_items = sorted(items, key=lambda x: x.score, reverse=True)
        
        return reranked_items[:top_n] if top_n is not None else reranked_items


---
File: /retrieval/reranker.py
---

# ares/retrieval/reranker.py
import torch # Asegurar import si no estaba
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from typing import List, Tuple, Optional
from ..core.datatypes import Chunk, RetrievedItem
from ..utils.logging_setup import logger

_reranker_model_cache = {}

class JinaRerankerWrapper:
    """Wrapper para Jina Reranker V2 Multilingual."""
    def __init__(self, model_name_or_path: str = "jinaai/jina-reranker-v2-base-multilingual", device: str = "cpu"):
        self.model_name = model_name_or_path
        self.device = device
        self._load_model()
        # ---- CORRECCIÓN ----
        logger.info(f"JinaRerankerWrapper inicializado con modelo '{self.model_name}' en dispositivo '{self.device}'")

    def _load_model(self):
        cache_key = (self.model_name, self.device) # Usar tupla como clave de caché
        if cache_key in _reranker_model_cache: # Comprobar usando la clave de tupla
            self.model, self.tokenizer = _reranker_model_cache[cache_key]
            self.model.to(self.device) # Asegurar que esté en el dispositivo correcto
            logger.debug(f"Reranker '{self.model_name}' cargado desde caché y movido a '{self.device}'.")
            return

        logger.info(f"Cargando reranker '{self.model_name}' en '{self.device}'...")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
            self.model = AutoModelForSequenceClassification.from_pretrained(
                self.model_name,
                torch_dtype="auto",
                trust_remote_code=True
            )
            self.model.to(self.device)
            self.model.eval()
            _reranker_model_cache[cache_key] = (self.model, self.tokenizer) # Usar tupla como clave
            logger.info(f"Reranker '{self.model_name}' cargado y añadido a caché.")
        except Exception as e:
            logger.error(f"Error cargando reranker '{self.model_name}': {e}")
            raise
    
    @torch.no_grad()
    def rerank(self, query: str, items: List[RetrievedItem], top_n: Optional[int] = None) -> List[RetrievedItem]:
        if not items:
            return []
        
        # Filtrar items que no tienen un chunk o cuyo chunk no tiene texto
        valid_items_for_pairing = [item for item in items if hasattr(item, 'chunk') and hasattr(item.chunk, 'text') and item.chunk.text]
        if not valid_items_for_pairing:
            logger.warning("Reranker: No hay items válidos con texto para rerankear.")
            return items # Devolver original si no hay nada válido

        pairs = [[query, item.chunk.text] for item in valid_items_for_pairing]

        try:
             inputs = self.tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(self.device)
             scores_tensor = self.model(**inputs, return_dict=True).logits.view(-1).float() # Mantener en GPU si es posible
             
             # Crear un mapa de ID de item original a nuevo score
             item_scores_map = {}
             for item, score in zip(valid_items_for_pairing, scores_tensor.cpu().numpy()): # Mover a CPU para numpy
                  item_scores_map[id(item)] = float(score) # Usar id(item) como clave única temporal

             # Actualizar scores en la lista original de items
             for item in items:
                 # Si el item fue procesado por el reranker, actualizar su score
                 if id(item) in item_scores_map:
                     item.score = item_scores_map[id(item)]
                 # Si no fue procesado (ej. no tenía texto), su score original se mantiene o se le asigna uno bajo
                 # Esto es importante si `items` contenía más elementos que `valid_items_for_pairing`
                 elif not (hasattr(item, 'chunk') and hasattr(item.chunk, 'text') and item.chunk.text):
                      item.score = -float('inf') # O un score muy bajo para que quede al final

             reranked_items = sorted(items, key=lambda x: x.score, reverse=True)

             if top_n is not None:
                  reranked_items = reranked_items[:top_n]

             logger.debug(f"Reranking completado. Top {len(reranked_items)} items seleccionados.")
             return reranked_items

        except Exception as e:
             logger.exception(f"Error durante reranking con Jina Reranker: {e}")
             return items


---
File: /retrieval/rse.py
---

# --- START OF NEW FILE ares/retrieval/rse.py ---
import numpy as np
from typing import List, Dict, Tuple, Optional, Any
import asyncio # Aunque RSE es principalmente síncrono, el método público puede ser async

from ..core.datatypes import RetrievedItem, Chunk
from ..storage.base import BaseKVStore # Para obtener chunks "sandwiched"
from ..utils.logging_setup import logger
from ..utils.helpers import compute_mdhash_id, get_config_value

# Parámetros por defecto para RSE (pueden ser sobrescritos por config)
RSE_DEFAULT_MAX_LENGTH_SEGMENT = 15
RSE_DEFAULT_OVERALL_MAX_LENGTH_CHUNKS = 30
RSE_DEFAULT_MINIMUM_VALUE_SEGMENT = 0.5
RSE_DEFAULT_IRRELEVANT_CHUNK_PENALTY = 0.18 # Valor de dsRAG, ajustar
RSE_DEFAULT_DECAY_RATE = 30 # Valor de dsRAG, ajustar
RSE_DEFAULT_TOP_K_FOR_DOC_SELECTION = 10
RSE_DEFAULT_CHUNK_LENGTH_ADJUSTMENT = True
RSE_DEFAULT_REFERENCE_CHUNK_LENGTH = 700 # Caracteres

class RelevantSegmentExtractor:
    def __init__(self, config: Dict[str, Any], chunk_store: BaseKVStore):
        rse_cfg = get_config_value(config, "retrieval.rse_params", {})
        
        self.chunk_store = chunk_store # Para obtener texto de chunks no recuperados
        self.max_length_segment = rse_cfg.get("max_length_segment", RSE_DEFAULT_MAX_LENGTH_SEGMENT)
        self.overall_max_length_chunks = rse_cfg.get("overall_max_length_chunks", RSE_DEFAULT_OVERALL_MAX_LENGTH_CHUNKS)
        self.minimum_value_segment = rse_cfg.get("minimum_value_segment", RSE_DEFAULT_MINIMUM_VALUE_SEGMENT)
        self.irrelevant_chunk_penalty = rse_cfg.get("irrelevant_chunk_penalty", RSE_DEFAULT_IRRELEVANT_CHUNK_PENALTY)
        self.decay_rate = rse_cfg.get("decay_rate", RSE_DEFAULT_DECAY_RATE)
        self.top_k_for_doc_selection = rse_cfg.get("top_k_for_doc_selection", RSE_DEFAULT_TOP_K_FOR_DOC_SELECTION)
        self.chunk_length_adjustment = rse_cfg.get("chunk_length_adjustment", RSE_DEFAULT_CHUNK_LENGTH_ADJUSTMENT)
        self.reference_chunk_length = rse_cfg.get("reference_chunk_length", RSE_DEFAULT_REFERENCE_CHUNK_LENGTH)
        logger.info("RelevantSegmentExtractor inicializado.")

    def _get_chunk_value(self, rank: int, absolute_relevance_value: float) -> float:
        """Calcula el valor de un chunk basado en su rank y score de relevancia."""
        # Similar a dsRAG: exp(-rank / decay_rate) * relevance_score - penalty
        # La transformación beta CDF de dsRAG para absolute_relevance_value no se incluye aquí por simplicidad inicial.
        # Se asume que el score del reranker ya está en una escala razonable.
        return np.exp(-rank / self.decay_rate) * absolute_relevance_value - self.irrelevant_chunk_penalty

    def _adjust_for_chunk_length(self, chunk_value: float, chunk_text_length: int) -> float:
        """Ajusta el valor del chunk basado en su longitud."""
        if not self.chunk_length_adjustment or self.reference_chunk_length <= 0:
            return chunk_value
        
        # Lógica dsRAG: solo beneficia a chunks más largos que la referencia, no penaliza cortos.
        # bounded_chunk_length = max(chunk_text_length, self.reference_chunk_length)
        # adjustment_factor = bounded_chunk_length / self.reference_chunk_length
        
        # Lógica alternativa: factor proporcional a la longitud, normalizado por referencia.
        # Esto puede beneficiar más a chunks muy largos.
        adjustment_factor = chunk_text_length / self.reference_chunk_length
        # Podríamos acotarlo para evitar factores extremos, ej. np.clip(adjustment_factor, 0.5, 2.0)
        return chunk_value * adjustment_factor

    async def _get_full_window_chunk_sequence(self, window_id: str, retrieved_items_map: Dict[int, RetrievedItem]) -> List[RetrievedItem]:
        """
        Obtiene la secuencia completa de chunks para una ventana, rellenando los no recuperados desde el KVStore.
        """
        if not window_id: return []

        # Obtener metadatos de la ventana para saber todos los chunk_ids y su orden
        window_meta_key = f"win_meta_{window_id}"
        window_meta = await self.chunk_store.get(window_meta_key)
        if not window_meta or "chunk_ids" not in window_meta:
            logger.warning(f"RSE: No se encontró metadata completa (chunk_ids) para ventana {window_id}. No se pueden rellenar huecos.")
            # Devolver solo los recuperados, ordenados
            return sorted(retrieved_items_map.values(), key=lambda item: item.chunk.metadata.get("chunk_index_in_window", float('inf')))

        all_chunk_ids_in_window: List[str] = window_meta["chunk_ids"]
        full_sequence: List[Optional[RetrievedItem]] = [None] * len(all_chunk_ids_in_window)

        # Colocar los chunks ya recuperados en sus posiciones
        for chunk_idx_in_window, item in retrieved_items_map.items():
            if 0 <= chunk_idx_in_window < len(full_sequence):
                full_sequence[chunk_idx_in_window] = item
        
        # Identificar y cargar chunks faltantes ("sandwiched")
        ids_to_fetch_from_kv: List[str] = []
        indices_to_fill: List[int] = []

        for i, chunk_id_expected in enumerate(all_chunk_ids_in_window):
            if full_sequence[i] is None: # Este chunk no fue recuperado/rerankeado
                ids_to_fetch_from_kv.append(chunk_id_expected)
                indices_to_fill.append(i)
        
        if ids_to_fetch_from_kv:
            logger.debug(f"RSE: Rellenando {len(ids_to_fetch_from_kv)} chunks no recuperados para ventana {window_id} desde KVStore.")
            kv_data_list = await self.chunk_store.get_batch(ids_to_fetch_from_kv)
            for original_idx_in_list, kv_data in enumerate(kv_data_list):
                target_sequence_idx = indices_to_fill[original_idx_in_list]
                chunk_id_filled = ids_to_fetch_from_kv[original_idx_in_list]
                if kv_data and isinstance(kv_data, dict) and kv_data.get("text") is not None:
                    filled_chunk = Chunk(
                        id=chunk_id_filled,
                        text=kv_data["text"],
                        metadata=kv_data.get("metadata", {}), # Crucial que contenga chunk_index_in_window
                        token_count=kv_data.get("token_count")
                        # Embedding será None, y score será bajo (o solo la penalización)
                    )
                    # Asignar un score muy bajo o nulo, ya que no fue recuperado por relevancia
                    full_sequence[target_sequence_idx] = RetrievedItem(chunk=filled_chunk, score=-1.0, source="rse_filled") 
                else:
                    logger.warning(f"RSE: No se pudo obtener texto para chunk {chunk_id_filled} (índice {target_sequence_idx} en ventana {window_id}).")
        
        return [item for item in full_sequence if item is not None] # Devolver solo los items válidos


    async def extract_segments(self, reranked_items: List[RetrievedItem], original_query_for_reranker: str) -> List[RetrievedItem]:
        if not reranked_items:
            return []

        # 1. Agrupar items rerankeados por window_id y prepararlos
        window_to_reranked_items_map: Dict[str, Dict[int, RetrievedItem]] = {} # window_id -> {chunk_idx_in_window: RetrievedItem}
        
        for rank, item in enumerate(reranked_items):
            window_id = item.chunk.metadata.get("window_id")
            chunk_idx = item.chunk.metadata.get("chunk_index_in_window")
            if window_id is None or chunk_idx is None:
                logger.warning(f"RSE: Item {item.chunk.id} sin window_id o chunk_index_in_window. Omitiendo de RSE.")
                continue
            
            if window_id not in window_to_reranked_items_map:
                window_to_reranked_items_map[window_id] = {}
            
            # Guardar el item con su rank global para el cálculo de _get_chunk_value
            item.metadata["rse_original_reranker_rank"] = rank 
            window_to_reranked_items_map[window_id][chunk_idx] = item

        # 2. Para cada ventana, obtener la secuencia completa de chunks y calcular "valores"
        all_segments_across_windows: List[Tuple[float, List[RetrievedItem]]] = [] # (segment_score, list_of_items_in_segment)

        # Seleccionar los top_k_for_doc_selection documentos (ventanas) basado en el mejor score de chunk rerankeado
        sorted_window_ids = sorted(
            window_to_reranked_items_map.keys(),
            key=lambda wid: max(it.score for it in window_to_reranked_items_map[wid].values() if it.score is not None) 
                            if window_to_reranked_items_map[wid] else -float('inf'),
            reverse=True
        )[:self.top_k_for_doc_selection]
        
        for window_id in sorted_window_ids:
            retrieved_items_for_window_map = window_to_reranked_items_map[window_id]
            # Obtener la secuencia completa, rellenando huecos desde KVStore
            window_chunk_sequence: List[RetrievedItem] = await self._get_full_window_chunk_sequence(window_id, retrieved_items_for_window_map)
            if not window_chunk_sequence: continue

            chunk_values = []
            for item in window_chunk_sequence:
                rank = item.metadata.get("rse_original_reranker_rank", len(reranked_items)) # Si es rellenado, rank alto
                # Si score es -1.0 (rellenado), absolute_relevance_value será cercano a 0 o negativo.
                # dsRAG usa el similarity score del reranker (transformado por beta CDF).
                # Aquí usamos el score del item, que ya podría ser el del reranker.
                # Si item.score es -1.0 (rellenado), su valor será muy negativo.
                abs_relevance = item.score if item.score != -1.0 else 0.001 # Pequeño valor para evitar -inf si rank es alto
                
                value = self._get_chunk_value(rank, abs_relevance)
                if self.chunk_length_adjustment:
                    value = self._adjust_for_chunk_length(value, len(item.chunk.text))
                chunk_values.append(value)
            
            # 3. Encontrar los mejores segmentos para esta ventana (lógica de dsrag.rse.get_best_segments)
            current_window_best_segments_indices: List[Tuple[int, int]] = [] # (start_idx_in_sequence, end_idx_in_sequence)
            current_window_segment_scores: List[float] = []
            
            # Implementación simplificada de get_best_segments para una sola ventana
            # (sin el bucle por query que tiene dsRAG, ya que aquí tenemos una "relevancia general" por ventana)
            temp_total_segment_len_chunks = 0 # Para controlar overall_max_length_chunks globalmente más tarde

            # Bucle para encontrar múltiples segmentos no solapados en la ventana actual
            while True: # Se romperá cuando no se encuentren más segmentos válidos
                best_segment_in_this_pass = None # (value, start, end)
                
                for start in range(len(window_chunk_sequence)):
                    for end in range(start + 1, min(start + self.max_length_segment + 1, len(window_chunk_sequence) + 1)):
                        # Validar que no solape con los ya seleccionados *en esta ventana*
                        overlaps_existing_in_window = any(start < seg_end and end > seg_start for seg_start, seg_end in current_window_best_segments_indices)
                        if overlaps_existing_in_window: continue
                        
                        # Validar que la longitud total no exceda el presupuesto global (esto se refinará después)
                        # segment_len_candidate = end - start
                        # if temp_total_segment_len_chunks + segment_len_candidate > self.overall_max_length_chunks:
                        #     continue

                        segment_value = sum(chunk_values[start:end])
                        if segment_value >= self.minimum_value_segment:
                            if best_segment_in_this_pass is None or segment_value > best_segment_in_this_pass[0]:
                                best_segment_in_this_pass = (segment_value, start, end)
                
                if best_segment_in_this_pass:
                    val, s_idx, e_idx = best_segment_in_this_pass
                    current_window_best_segments_indices.append((s_idx, e_idx))
                    current_window_segment_scores.append(val)
                    # temp_total_segment_len_chunks += (e_idx - s_idx) # Esto se hará globalmente
                else:
                    break # No más segmentos válidos en esta ventana para esta pasada
            
            for (s_idx, e_idx), seg_score in zip(current_window_best_segments_indices, current_window_segment_scores):
                all_segments_across_windows.append((seg_score, window_chunk_sequence[s_idx:e_idx]))

        # 4. Ordenar todos los segmentos de todas las ventanas por score y aplicar overall_max_length_chunks
        all_segments_across_windows.sort(key=lambda x: x[0], reverse=True)
        
        final_rse_items: List[RetrievedItem] = []
        current_total_chunks_in_final_segments = 0
        for seg_score, segment_chunk_list in all_segments_across_windows:
            if not segment_chunk_list: continue
            
            num_chunks_in_this_segment = len(segment_chunk_list)
            if current_total_chunks_in_final_segments + num_chunks_in_this_segment > self.overall_max_length_chunks:
                if not final_rse_items: # Si es el primer segmento y ya es demasiado largo, tomarlo truncado si es posible
                    segment_chunk_list = segment_chunk_list[:self.overall_max_length_chunks]
                    if not segment_chunk_list: continue
                    num_chunks_in_this_segment = len(segment_chunk_list)
                else: # Ya tenemos otros segmentos, no podemos añadir este completo
                    continue 
            
            current_total_chunks_in_final_segments += num_chunks_in_this_segment

            segment_text = "\n\n".join([item.chunk.text for item in segment_chunk_list]).strip()
            first_item_in_segment = segment_chunk_list[0]
            
            segment_id = f"rse-seg-{first_item_in_segment.chunk.metadata.get('window_id', 'unk')}-{first_item_in_segment.chunk.metadata.get('chunk_index_in_window', 0)}-{compute_mdhash_id(segment_text)[:6]}"
            
            # Heredar metadatos del primer chunk, y añadir info de RSE
            segment_base_metadata = first_item_in_segment.chunk.metadata.copy()
            segment_metadata = {
                **segment_base_metadata, # Copia todos los metadatos del primer chunk del segmento
                "rse_segment": True,
                "rse_num_chunks_in_segment": num_chunks_in_this_segment,
                "rse_original_chunk_ids": [item.chunk.id for item in segment_chunk_list],
                "rse_segment_score": seg_score,
                "type": "rse_segment" # Distinguir este tipo de chunk
            }
            # Eliminar claves que no aplican al segmento como un todo
            segment_metadata.pop("chunk_index_in_window", None)
            segment_metadata.pop("chunk_index_in_lsc_section", None)
            segment_metadata.pop("start_char_in_lsc_section", None)
            segment_metadata.pop("end_char_in_lsc_section", None)
            
            # Calcular embedding promediado para el segmento (para MMR posterior)
            segment_embeddings = [item.chunk.embedding for item in segment_chunk_list if item.chunk.embedding is not None]
            avg_segment_embedding = None
            if segment_embeddings:
                avg_segment_embedding = np.mean(np.array(segment_embeddings), axis=0).astype(np.float32)
            
            # Crear el Chunk y RetrievedItem para el segmento
            segment_chunk_obj = Chunk(
                id=segment_id, 
                text=segment_text, 
                metadata=segment_metadata, 
                embedding=avg_segment_embedding, # Guardar el embedding promediado
                token_count=sum(item.chunk.token_count for item in segment_chunk_list if item.chunk.token_count) # Sumar token counts
            )
            
            rse_item = RetrievedItem(
                chunk=segment_chunk_obj,
                score=seg_score, # Usar el score calculado por RSE
                source="rse_segment",
                metadata={"retrieval_sources": ["rse"]},
                normalized_embedding=avg_segment_embedding / np.linalg.norm(avg_segment_embedding) if avg_segment_embedding is not None else None
            )
            final_rse_items.append(rse_item)

            if current_total_chunks_in_final_segments >= self.overall_max_length_chunks:
                break
        
        # No es necesario re-ordenar `final_rse_items` aquí si `all_segments_across_windows` ya estaba ordenado.
        logger.info(f"RSE extrajo {len(final_rse_items)} segmentos finales, total_chunks_in_segments={current_total_chunks_in_final_segments}.")
        return final_rse_items

# --- END OF NEW FILE ares/retrieval/rse.py ---


---
File: /retrieval/selector.py
---

# ares/retrieval/selector.py
import numpy as np
from typing import List, Optional # Asegurar Optional si no estaba
from ..core.datatypes import Chunk, RetrievedItem
from ..utils.helpers import cosine_similarity, normalize_vector # Asegurar normalize_vector
from ..utils.logging_setup import logger

async def select_diverse_chunks_mmr(
    query_embedding_normalized: np.ndarray,
    reranked_items: List[RetrievedItem],
    lambda_mult: float = 0.5,
    k: int = 10
) -> List[Chunk]:
    if not reranked_items or k <= 0:
        return []

    candidates = [item for item in reranked_items if item.normalized_embedding is not None and isinstance(item.chunk, Chunk)]
    if not candidates:
         logger.warning("MMR: No hay items con embeddings válidos. Devolviendo Top K por score reranked.")
         return [item.chunk for item in reranked_items[:k] if isinstance(item.chunk, Chunk)]

    k = min(k, len(candidates))

    selected_indices_in_candidates: List[int] = []
    candidate_indices = list(range(len(candidates)))
    
    # Asumir que los embeddings ya están normalizados y son 1D
    candidate_embeddings_norm = np.array([c.normalized_embedding for c in candidates]) # Esto será (num_candidates, dim)

    if candidate_indices:
        # El primer item seleccionado se basa en el score del reranker, no en MMR.
        # El primer item en `candidates` ya es el más relevante según reranker.
        # Si reranked_items ya está ordenado, el primer candidato es el mejor.
        first_candidate_idx_in_original_list = 0 # Asumir que candidates mantiene el orden de reranked_items
        selected_indices_in_candidates.append(first_candidate_idx_in_original_list)
        # Remover el índice del candidato seleccionado de la lista de candidatos disponibles
        if first_candidate_idx_in_original_list in candidate_indices:
            candidate_indices.remove(first_candidate_idx_in_original_list)


    while len(selected_indices_in_candidates) < k and candidate_indices:
        best_score = -np.inf
        best_idx_to_add = -1

        # Embeddings de los ya seleccionados
        # Esto es un array de forma (num_seleccionados, dimension_embedding)
        selected_embeddings_matrix = candidate_embeddings_norm[selected_indices_in_candidates]

        for idx_in_candidates_list in candidate_indices: # Iterar sobre los índices restantes de la lista 'candidates'
            current_candidate_embedding = candidate_embeddings_norm[idx_in_candidates_list] # Vector 1D

            # Relevancia (similitud con query)
            relevance_score = cosine_similarity(query_embedding_normalized, current_candidate_embedding)

            # Diversidad (max similitud con ya seleccionados)
            if selected_embeddings_matrix.size > 0:
                # Calcular similitud del candidato actual con TODOS los ya seleccionados
                # El resultado de np.dot será un array 1D de similitudes
                sim_to_selected_array = np.dot(selected_embeddings_matrix, current_candidate_embedding.T)
                # No necesitamos llamar a nuestra cosine_similarity aquí si los vectores ya están normalizados
                # Si no están normalizados, necesitaríamos un bucle:
                # sim_to_selected_array = np.array([cosine_similarity(s_emb, current_candidate_embedding) for s_emb in selected_embeddings_matrix])
                
                max_similarity_to_selected = np.max(sim_to_selected_array) if sim_to_selected_array.size > 0 else 0.0
            else:
                max_similarity_to_selected = 0.0

            mmr_score = lambda_mult * relevance_score - (1 - lambda_mult) * max_similarity_to_selected

            if mmr_score > best_score:
                best_score = mmr_score
                best_idx_to_add = idx_in_candidates_list

        if best_idx_to_add != -1:
             selected_indices_in_candidates.append(best_idx_to_add)
             candidate_indices.remove(best_idx_to_add)
        else:
             # No se encontraron más candidatos que mejoren el score (puede pasar si lambda_mult es bajo y todos son muy similares)
             break 

    final_chunks = [candidates[idx].chunk for idx in selected_indices_in_candidates]
    logger.debug(f"Selección MMR completada. {len(final_chunks)}/{k} chunks seleccionados.")
    return final_chunks

# Implementación de Knapsack (más compleja, opcional):
# async def select_diverse_chunks_knapsack(...) -> List[Chunk]: ...


---
File: /storage/cache/__init__.py
---




---
File: /storage/cache/kv_store_cache.py
---




---
File: /storage/knowledge_graph/__init__.py
---




---
File: /storage/knowledge_graph/base.py
---




---
File: /storage/knowledge_graph/networkx_impl.py
---

import networkx as nx
import os
import pickle
from typing import Dict, List, Optional, Any, Tuple
from ..base import BaseKnowledgeGraph
from ...utils.logging_setup import logger

class NetworkXKnowledgeGraph(BaseKnowledgeGraph):
    """Implementación de KnowledgeGraph usando NetworkX y persistencia pickle."""

    def __init__(self, graph_path: str):
        self.graph_path = graph_path
        self._load_graph()
        logger.info(f"NetworkXKnowledgeGraph inicializado. Grafo cargado desde {graph_path} (Nodos: {self.graph.number_of_nodes()}, Aristas: {self.graph.number_of_edges()})")

    def _load_graph(self):
        """Carga el grafo desde un archivo pickle."""
        if os.path.exists(self.graph_path):
            try:
                with open(self.graph_path, 'rb') as f:
                    self.graph: nx.DiGraph = pickle.load(f) # Usar DiGraph para relaciones dirigidas
                    logger.debug(f"Grafo cargado desde {self.graph_path}")
            except Exception as e:
                logger.error(f"Error cargando grafo desde {self.graph_path}: {e}. Creando grafo nuevo.")
                self.graph = nx.DiGraph()
        else:
            logger.info(f"Archivo de grafo no encontrado en {self.graph_path}. Creando grafo nuevo.")
            self.graph = nx.DiGraph()

    def _save_graph(self):
        """Guarda el grafo en un archivo pickle."""
        try:
            # Asegurar que el directorio exista
            os.makedirs(os.path.dirname(self.graph_path), exist_ok=True)
            with open(self.graph_path, 'wb') as f:
                pickle.dump(self.graph, f)
            logger.debug(f"Grafo guardado en {self.graph_path}")
        except Exception as e:
            logger.error(f"Error guardando grafo en {self.graph_path}: {e}")

    async def add_node(self, node_id: str, node_type: str, properties: Dict[str, Any]) -> None:
        # Usar update para añadir/actualizar propiedades sin sobreescribir el nodo entero
        if self.graph.has_node(node_id):
             self.graph.nodes[node_id].update(properties)
             self.graph.nodes[node_id]['type'] = node_type # Asegurar que el tipo se actualice
        else:
             self.graph.add_node(node_id, type=node_type, **properties)
        # self._save_graph() # Guardar puede ser costoso, mejor al final

    async def add_edge(self, source_id: str, target_id: str, label: str, properties: Dict[str, Any]) -> None:
        # Asegurarse que los nodos existan (NetworkX los crea si no existen, pero es buena práctica verificar)
        if not self.graph.has_node(source_id):
             await self.add_node(source_id, "UNKNOWN", {}) # Añadir nodo con tipo default
             logger.warning(f"Nodo fuente '{source_id}' no existía, añadido con tipo UNKNOWN.")
        if not self.graph.has_node(target_id):
             await self.add_node(target_id, "UNKNOWN", {})
             logger.warning(f"Nodo destino '{target_id}' no existía, añadido con tipo UNKNOWN.")

        # Usar update para añadir/actualizar propiedades
        if self.graph.has_edge(source_id, target_id):
             self.graph.edges[source_id, target_id].update(properties)
             self.graph.edges[source_id, target_id]['label'] = label # Asegurar etiqueta
        else:
             self.graph.add_edge(source_id, target_id, label=label, **properties)
        # self._save_graph()

    async def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        if self.graph.has_node(node_id):
            # Devolver una copia para evitar modificaciones externas accidentales
            return dict(self.graph.nodes[node_id])
        return None

    async def get_neighbors(self, node_id: str, relation_label: Optional[str] = None) -> List[Dict[str, Any]]:
        neighbors_data = []
        if self.graph.has_node(node_id):
            # Considerar tanto sucesores como predecesores en DiGraph
            successors = list(self.graph.successors(node_id))
            predecessors = list(self.graph.predecessors(node_id))
            all_neighbors = set(successors) | set(predecessors)

            for neighbor_id in all_neighbors:
                edge_data_fwd = self.graph.get_edge_data(node_id, neighbor_id)
                edge_data_bwd = self.graph.get_edge_data(neighbor_id, node_id)

                edge_data = edge_data_fwd or edge_data_bwd # Priorizar fwd si ambos existen?
                if edge_data:
                     if relation_label is None or edge_data.get('label') == relation_label:
                          node_data = await self.get_node(neighbor_id)
                          if node_data:
                               node_data['id'] = neighbor_id # Añadir ID al diccionario
                               neighbors_data.append(node_data)
        return neighbors_data

    async def find_paths(self, start_node_id: str, end_node_id: str, max_depth: int = 3) -> List[List[Dict[str, Any]]]:
        paths_data = []
        if self.graph.has_node(start_node_id) and self.graph.has_node(end_node_id):
             # Usar all_simple_paths para encontrar caminos sin ciclos
             # cutoff limita la profundidad
             simple_paths = nx.all_simple_paths(self.graph, source=start_node_id, target=end_node_id, cutoff=max_depth)
             for path_nodes in simple_paths:
                  path_details = []
                  for node_id in path_nodes:
                       node_data = await self.get_node(node_id)
                       if node_data:
                           node_data['id'] = node_id
                           path_details.append(node_data)
                  if len(path_details) == len(path_nodes): # Asegurar que todos los nodos se encontraron
                       paths_data.append(path_details)
        return paths_data

    async def update_node_properties(self, node_id: str, properties: Dict[str, Any]) -> None:
        if self.graph.has_node(node_id):
            self.graph.nodes[node_id].update(properties)
            # self._save_graph()
        else:
            logger.warning(f"Intento de actualizar propiedades de nodo inexistente: {node_id}")

    async def delete_node(self, node_id: str) -> None:
        if self.graph.has_node(node_id):
            self.graph.remove_node(node_id)
            # self._save_graph()
            logger.info(f"Nodo '{node_id}' eliminado del grafo.")
        else:
            logger.warning(f"Intento de eliminar nodo inexistente: {node_id}")

    # Callbacks para guardar el estado
    async def index_done_callback(self):
        self._save_graph()

    async def query_done_callback(self):
        pass # No es necesario guardar después de consultar


---
File: /storage/knowledge_graph/pathrag_networkx.py
---

# ares/storage/knowledge_graph/pathrag_networkx.py
import networkx as nx
import os
import pickle
import asyncio
from typing import Dict, List, Optional, Any, Tuple, Set
import itertools # Añadir import para combinations
from ..base import BaseKnowledgeGraph
from ...core.datatypes import Chunk # Necesario para tipo de retorno de search_kg
from ...utils.logging_setup import logger
from ...utils.pathrag_logic import find_relevant_paths # Importar lógica PathRAG (puede ser placeholder)
from ...utils.helpers import GRAPH_FIELD_SEP

class PathRAGLogicKnowledgeGraph(BaseKnowledgeGraph):
    """Implementación KG con interfaz PathRAG sobre NetworkX (Funcionalidad básica)."""

    def __init__(self, graph_path: str):
        self.graph_path = graph_path
        self._load_graph()
        logger.info(f"PathRAGLogicKnowledgeGraph inicializado. Grafo: {graph_path} (Nodos: {self.graph.number_of_nodes()}, Aristas: {self.graph.number_of_edges()})")

    def _load_graph(self):
        if os.path.exists(self.graph_path):
            try:
                with open(self.graph_path, 'rb') as f:
                    self.graph: nx.DiGraph = pickle.load(f)
            except Exception as e:
                logger.error(f"Error cargando grafo {self.graph_path}: {e}. Creando nuevo."); self.graph = nx.DiGraph()
        else: logger.info(f"Grafo no encontrado en {self.graph_path}. Creando nuevo."); self.graph = nx.DiGraph()

    def _save_graph(self):
        try:
            os.makedirs(os.path.dirname(self.graph_path), exist_ok=True)
            with open(self.graph_path, 'wb') as f: pickle.dump(self.graph, f)
            logger.debug(f"Grafo guardado en {self.graph_path}")
        except Exception as e: logger.error(f"Error guardando grafo {self.graph_path}: {e}")

    async def add_node(self, node_id: str, node_type: str, properties: Dict[str, Any]) -> None:
        await asyncio.to_thread(self._add_node_sync, node_id, node_type, properties)

    def _add_node_sync(self, node_id: str, node_type: str, properties: Dict[str, Any]):
        if self.graph.has_node(node_id):
            existing_sources = set(self.graph.nodes[node_id].get("source_id", "").split(GRAPH_FIELD_SEP))
            new_sources_val = properties.get("source_id", "")
            new_sources = set(new_sources_val.split(GRAPH_FIELD_SEP) if isinstance(new_sources_val, str) else new_sources_val)
            properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, existing_sources | new_sources))
            self.graph.nodes[node_id].update(properties)
            self.graph.nodes[node_id]['type'] = node_type
        else:
            if isinstance(properties.get("source_id"), (list, set)):
                 properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, properties["source_id"]))
            self.graph.add_node(node_id, type=node_type, **properties)

    async def add_edge(self, source_id: str, target_id: str, label: str, properties: Dict[str, Any]) -> None:
        await asyncio.to_thread(self._add_edge_sync, source_id, target_id, label, properties)

    def _add_edge_sync(self, source_id: str, target_id: str, label: str, properties: Dict[str, Any]):
        if not self.graph.has_node(source_id): self._add_node_sync(source_id, "UNKNOWN", {"source_id": properties.get("source_id","")})
        if not self.graph.has_node(target_id): self._add_node_sync(target_id, "UNKNOWN", {"source_id": properties.get("source_id","")})

        edge_data = self.graph.get_edge_data(source_id, target_id)
        if edge_data and edge_data.get('label') == label:
            existing_sources = set(edge_data.get("source_id", "").split(GRAPH_FIELD_SEP))
            new_sources_val = properties.get("source_id", "")
            new_sources = set(new_sources_val.split(GRAPH_FIELD_SEP) if isinstance(new_sources_val, str) else new_sources_val)
            properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, existing_sources | new_sources))
            self.graph.edges[source_id, target_id].update(properties)
        else:
            if isinstance(properties.get("source_id"), (list, set)):
                 properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, properties["source_id"]))
            self.graph.add_edge(source_id, target_id, label=label, **properties)

    async def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        return await asyncio.to_thread(self._get_node_sync, node_id)

    def _get_node_sync(self, node_id: str) -> Optional[Dict[str, Any]]:
        if self.graph.has_node(node_id):
            return dict(self.graph.nodes[node_id])
        return None

    async def get_edge(self, source_id: str, target_id: str, label: Optional[str] = None) -> Optional[Dict[str, Any]]:
         return await asyncio.to_thread(self._get_edge_sync, source_id, target_id, label)

    def _get_edge_sync(self, source_id: str, target_id: str, label: Optional[str] = None) -> Optional[Dict[str, Any]]:
        if self.graph.has_edge(source_id, target_id):
             edge_data = dict(self.graph.get_edge_data(source_id, target_id))
             if label is None or edge_data.get('label') == label:
                 return edge_data
        return None

    async def get_neighbors(self, node_id: str, relation_label: Optional[str] = None) -> List[Dict[str, Any]]:
         return await asyncio.to_thread(self._get_neighbors_sync, node_id, relation_label)

    def _get_neighbors_sync(self, node_id: str, relation_label: Optional[str] = None) -> List[Dict[str, Any]]:
        neighbors_data = []
        if self.graph.has_node(node_id):
            # Usar set para evitar duplicados si hay aristas en ambas direcciones
            neighbor_ids = set(self.graph.successors(node_id)) | set(self.graph.predecessors(node_id))
            for neighbor_id in neighbor_ids:
                edge_data = self._get_edge_sync(node_id, neighbor_id, relation_label) or \
                            self._get_edge_sync(neighbor_id, node_id, relation_label)
                if edge_data: # Si existe relación (y coincide label si se dio)
                     node_data = self._get_node_sync(neighbor_id)
                     if node_data:
                          node_data['id'] = neighbor_id
                          neighbors_data.append(node_data)
        return neighbors_data

    async def find_paths(self, start_node_id: str, end_node_id: Optional[str] = None, max_depth: int = 2, **kwargs) -> List[Dict[str, Any]]:
        """Llama a la lógica de PathRAG."""
        # Pasar kwargs adicionales como relation_weights, score_threshold, etc.
        return await find_relevant_paths(
            graph=self.graph, # Pasar el grafo NetworkX
            start_node=start_node_id,
            end_node=end_node_id,
            max_depth=max_depth,
            **kwargs # Pasar otros parámetros a la función lógica
        )

    async def has_node(self, node_id: str) -> bool:
         return await asyncio.to_thread(self.graph.has_node, node_id)

    async def update_node_properties(self, node_id: str, properties: Dict[str, Any]) -> None:
         await asyncio.to_thread(self._update_node_properties_sync, node_id, properties)

    def _update_node_properties_sync(self, node_id: str, properties: Dict[str, Any]):
         if self.graph.has_node(node_id):
            if "source_id" in properties:
                 existing_sources = set(self.graph.nodes[node_id].get("source_id", "").split(GRAPH_FIELD_SEP))
                 new_sources_val = properties["source_id"]
                 new_sources = set(new_sources_val.split(GRAPH_FIELD_SEP) if isinstance(new_sources_val, str) else new_sources_val)
                 properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, existing_sources | new_sources))
            self.graph.nodes[node_id].update(properties)
         else: logger.warning(f"Intento update nodo inexistente: {node_id}")

    async def update_edge_properties(self, source_id: str, target_id: str, label: str, properties: Dict[str, Any]) -> None:
        await asyncio.to_thread(self._update_edge_properties_sync, source_id, target_id, label, properties)

    def _update_edge_properties_sync(self, source_id: str, target_id: str, label: str, properties: Dict[str, Any]):
         if self.graph.has_edge(source_id, target_id):
             edge_data = self.graph.get_edge_data(source_id, target_id)
             if edge_data.get('label') == label:
                  if "source_id" in properties:
                       existing_sources = set(edge_data.get("source_id", "").split(GRAPH_FIELD_SEP))
                       new_sources_val = properties["source_id"]
                       new_sources = set(new_sources_val.split(GRAPH_FIELD_SEP) if isinstance(new_sources_val, str) else new_sources_val)
                       properties["source_id"] = GRAPH_FIELD_SEP.join(filter(None, existing_sources | new_sources))
                  self.graph.edges[source_id, target_id].update(properties)
             else: logger.warning(f"No se encontró arista con label '{label}' entre {source_id} y {target_id}.")
         else: logger.warning(f"Intento update arista inexistente: {source_id} -> {target_id}")

    async def delete_node(self, node_id: str) -> None:
        await asyncio.to_thread(self._delete_node_sync, node_id)

    def _delete_node_sync(self, node_id: str):
         if self.graph.has_node(node_id):
             self.graph.remove_node(node_id)
             logger.info(f"Nodo '{node_id}' eliminado.")
         else: logger.warning(f"Intento delete nodo inexistente: {node_id}")

    # Callbacks
    async def index_done_callback(self): self._save_graph()
    async def query_done_callback(self): pass # No es necesario guardar después de consultar


---
File: /storage/knowledge_graph/pathrag_wrapper.py
---




---
File: /storage/kv_store/__init__.py
---




---
File: /storage/kv_store/base.py
---




---
File: /storage/kv_store/duckdb.py
---




---
File: /storage/kv_store/sqlite.py
---

import sqlite3
import json
import os
from typing import Dict, List, Optional, Any, AsyncIterator
from ..base import BaseKVStore
from ...utils.logging_setup import logger # Asegúrate que logging_setup esté creado

# Nota: Esta implementación es síncrona por simplicidad con SQLite.
# Para producción asíncrona real, se usarían librerías como aiosqlite
# o se ejecutarían operaciones síncronas en un ThreadPoolExecutor.
# Por ahora, simularemos async con métodos síncronos.

class SQLiteKVStore(BaseKVStore):
    """Implementación de KVStore usando SQLite."""

    def __init__(self, db_path: str):
        self.db_path = db_path
        self._initialize_db()
        logger.info(f"SQLiteKVStore inicializado en {db_path}")

    def _initialize_db(self):
        """Crea la tabla si no existe."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS kv_store (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            )
        """)
        # Añadir índice para búsqueda por metadata si es necesario (ejemplo)
        # cursor.execute("CREATE INDEX IF NOT EXISTS idx_metadata_json ON kv_store(json_extract(value, '$.metadata_field'))")
        conn.commit()
        conn.close()

    def _get_connection(self):
        """Obtiene una conexión a la base de datos."""
        return sqlite3.connect(self.db_path)

    async def get(self, key: str) -> Optional[Dict[str, Any]]:
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT value FROM kv_store WHERE key = ?", (key,))
        row = cursor.fetchone()
        conn.close()
        if row:
            try:
                return json.loads(row[0])
            except json.JSONDecodeError:
                logger.error(f"Error decodificando JSON para la clave {key}")
                return None # O manejar de otra forma
        return None

    async def set(self, key: str, value: Dict[str, Any]) -> None:
        conn = self._get_connection()
        cursor = conn.cursor()
        try:
            value_str = json.dumps(value)
            # Usar INSERT OR REPLACE para comportamiento upsert
            cursor.execute("INSERT OR REPLACE INTO kv_store (key, value) VALUES (?, ?)", (key, value_str))
            conn.commit()
        except json.JSONDecodeError:
             logger.error(f"Error codificando JSON para la clave {key}")
             # Considerar lanzar una excepción o no insertar
        finally:
            conn.close()

    async def delete(self, key: str) -> None:
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute("DELETE FROM kv_store WHERE key = ?", (key,))
        conn.commit()
        conn.close()

    async def get_batch(self, keys: List[str]) -> List[Optional[Dict[str, Any]]]:
        # SQLite no tiene un get_batch nativo eficiente como algunas NoSQL.
        # Se puede hacer con WHERE key IN (...) pero puede ser lento para muchas claves.
        # Por simplicidad, iteramos get por ahora.
        # Para optimizar: usar WHERE key IN (?, ?, ...) o ejecutar en executor.
        results = []
        for key in keys:
            results.append(await self.get(key))
        return results

    async def set_batch(self, items: Dict[str, Dict[str, Any]]) -> None:
        conn = self._get_connection()
        cursor = conn.cursor()
        data_to_insert = []
        for key, value in items.items():
            try:
                value_str = json.dumps(value)
                data_to_insert.append((key, value_str))
            except json.JSONDecodeError:
                logger.error(f"Error codificando JSON para la clave {key} en batch set")
                # Omitir esta clave o manejar el error

        if data_to_insert:
            # Usar INSERT OR REPLACE para comportamiento upsert en batch
            cursor.executemany("INSERT OR REPLACE INTO kv_store (key, value) VALUES (?, ?)", data_to_insert)
            conn.commit()
        conn.close()

    async def exists(self, key: str) -> bool:
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT 1 FROM kv_store WHERE key = ?", (key,))
        exists = cursor.fetchone() is not None
        conn.close()
        return exists

    async def get_all_keys_iterator(self) -> AsyncIterator[str]:
        # Nota: Esto carga todas las claves en memoria primero, no es un iterador real de DB.
        # Para un iterador real, se necesitaría gestión de cursor/paginación.
        conn = self._get_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT key FROM kv_store")
        keys = [row[0] for row in cursor.fetchall()]
        conn.close()
        for key in keys:
            yield key # Simula async iterator

    # Ejemplo básico de búsqueda por metadatos (requiere que 'value' sea JSON)
    async def search_by_metadata(self, filters: Dict[str, Any], limit: int = 100) -> List[Dict[str, Any]]:
        conn = self._get_connection()
        cursor = conn.cursor()
        where_clauses = []
        params = []
        for field, value in filters.items():
             # Asumiendo que los campos de filtro están en el primer nivel del JSON 'value'
             where_clauses.append(f"json_extract(value, '$.{field}') = ?")
             params.append(value)

        if not where_clauses:
             return []

        query = f"SELECT value FROM kv_store WHERE {' AND '.join(where_clauses)} LIMIT ?"
        params.append(limit)

        cursor.execute(query, tuple(params))
        rows = cursor.fetchall()
        conn.close()

        results = []
        for row in rows:
            try:
                results.append(json.loads(row[0]))
            except json.JSONDecodeError:
                logger.error(f"Error decodificando JSON durante búsqueda por metadatos")
        return results

    # Necesario para el caché
    async def index_done_callback(self):
        pass # SQLite escribe inmediatamente, no hay caché explícito que vaciar

    async def query_done_callback(self):
        pass # No aplica a SQLite KV


---
File: /storage/kv_store/test_sqlite_kv_store.py
---

import pytest
import os
from typing import Dict, Any
from ..ares.storage.kv_store.sqlite import SQLiteKVStore # Ajustar

# pytest-asyncio marca tests como asíncronos
pytestmark = pytest.mark.asyncio

async def test_sqlite_kv_init(test_working_dir):
    db_path = os.path.join(test_working_dir, "init_test.db")
    store = SQLiteKVStore(db_path=db_path)
    assert os.path.exists(db_path)
    # Verificar que la tabla existe (puede requerir acceso directo a sqlite3)
    import sqlite3
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='kv_store'")
    assert cursor.fetchone() is not None
    conn.close()

async def test_sqlite_kv_set_get(sample_sqlite_kv_store: SQLiteKVStore):
    key = "test_key_1"
    value = {"a": 1, "b": "hello"}
    await sample_sqlite_kv_store.set(key, value)
    retrieved = await sample_sqlite_kv_store.get(key)
    assert retrieved == value

async def test_sqlite_kv_get_nonexistent(sample_sqlite_kv_store: SQLiteKVStore):
    retrieved = await sample_sqlite_kv_store.get("nonexistent_key")
    assert retrieved is None

async def test_sqlite_kv_overwrite(sample_sqlite_kv_store: SQLiteKVStore):
    key = "overwrite_key"
    value1 = {"version": 1}
    value2 = {"version": 2}
    await sample_sqlite_kv_store.set(key, value1)
    await sample_sqlite_kv_store.set(key, value2)
    retrieved = await sample_sqlite_kv_store.get(key)
    assert retrieved == value2

async def test_sqlite_kv_delete(sample_sqlite_kv_store: SQLiteKVStore):
    key = "delete_key"
    value = {"data": "to_delete"}
    await sample_sqlite_kv_store.set(key, value)
    assert await sample_sqlite_kv_store.get(key) == value
    await sample_sqlite_kv_store.delete(key)
    assert await sample_sqlite_kv_store.get(key) is None

async def test_sqlite_kv_exists(sample_sqlite_kv_store: SQLiteKVStore):
    key = "exists_key"
    value = {"data": "exists"}
    assert not await sample_sqlite_kv_store.exists(key)
    await sample_sqlite_kv_store.set(key, value)
    assert await sample_sqlite_kv_store.exists(key)
    await sample_sqlite_kv_store.delete(key)
    assert not await sample_sqlite_kv_store.exists(key)

async def test_sqlite_kv_batch_set_get(sample_sqlite_kv_store: SQLiteKVStore):
    items = {
        "batch_key_1": {"val": 1},
        "batch_key_2": {"val": "two"},
        "batch_key_3": {"val": [1, 2]},
    }
    await sample_sqlite_kv_store.set_batch(items)

    keys_to_get = ["batch_key_1", "batch_key_2", "nonexistent"]
    retrieved_batch = await sample_sqlite_kv_store.get_batch(keys_to_get)

    assert len(retrieved_batch) == 3
    assert retrieved_batch[0] == {"val": 1}
    assert retrieved_batch[1] == {"val": "two"}
    assert retrieved_batch[2] is None

async def test_sqlite_kv_get_all_keys(sample_sqlite_kv_store: SQLiteKVStore):
    items = { f"key_{i}": {"n": i} for i in range(5) }
    await sample_sqlite_kv_store.set_batch(items)

    retrieved_keys = set()
    async for key in sample_sqlite_kv_store.get_all_keys_iterator():
        retrieved_keys.add(key)

    assert retrieved_keys == set(items.keys())

# Añadir tests para search_by_metadata si se implementa la lógica de indexación JSON


---
File: /storage/vector_db/__init__.py
---




---
File: /storage/vector_db/base.py
---




---
File: /storage/vector_db/chroma_local.py
---




---
File: /storage/vector_db/qdrant_local.py
---

# ares/storage/vector_db/qdrant_local.py

import qdrant_client
from qdrant_client.http import models as rest
import numpy as np
import asyncio
import uuid
from typing import List, Dict, Optional, Any, Tuple

from ..base import BaseVectorDB
from ...core.datatypes import Chunk
from ...utils.logging_setup import logger

def _build_qdrant_filter(filters: Optional[Dict[str, Any]]) -> Optional[rest.Filter]:
    """Convierte un diccionario de filtros a un objeto Filter de Qdrant."""
    if not filters:
        return None
    
    must_conditions = []
    should_conditions = [] # Para filtros OR si se implementan

    for key, value in filters.items():

        # ---- CORRECCIÓN AQUÍ ----
        # Nuestros payloads son planos, no tienen una sub-clave "metadata" anidada.
        # Los campos como "type", "window_id" están en el nivel superior del payload.
        field_key = key 
        # Original: field_key = key if '.' in key else f"metadata.{key}"
        # ---- FIN CORRECCIÓN ----

        if isinstance(value, dict) and any(op.startswith('$') for op in value.keys()):
            # Manejar operadores como $gte, $lt, $in, $ne
            for op, op_val in value.items():
                if op == "$eq":
                    must_conditions.append(rest.FieldCondition(key=field_key, match=rest.MatchValue(value=op_val)))
                elif op == "$ne":
                    must_conditions.append(
                        rest.Filter(must_not=[rest.FieldCondition(key=field_key, match=rest.MatchValue(value=op_val))])
                    )
                elif op == "$in" and isinstance(op_val, list):
                     # Qdrant >=1.7 prefiere any para $in
                     should_in = [rest.FieldCondition(key=field_key, match=rest.MatchValue(value=item)) for item in op_val]
                     if should_in: must_conditions.append(rest.Filter(should=should_in))
                elif op == "$nin" and isinstance(op_val, list):
                    must_not_in = [rest.FieldCondition(key=field_key, match=rest.MatchValue(value=item)) for item in op_val]
                    if must_not_in: must_conditions.append(rest.Filter(must_not=must_not_in))
                elif op in ["$gt", "$gte", "$lt", "$lte"]:
                    range_args = {op[1:]: op_val} # gt, gte, lt, lte
                    must_conditions.append(rest.FieldCondition(key=field_key, range=rest.Range(**range_args)))
                else:
                    logger.warning(f"Operador de filtro no soportado '{op}' en _build_qdrant_filter")

        elif isinstance(value, list): # $in implícito para listas simples
            should_in = [rest.FieldCondition(key=field_key, match=rest.MatchValue(value=item)) for item in value]
            if should_in: must_conditions.append(rest.Filter(should=should_in))
        else: # Igualdad simple
            must_conditions.append(rest.FieldCondition(key=field_key, match=rest.MatchValue(value=value)))

    return rest.Filter(must=must_conditions) if must_conditions else None


class QdrantLocalVectorDB(BaseVectorDB):
    def __init__(self, collection_name: str, path: str, embedding_dim: int, sparse_vector_name: str = "bm25_sparse", recreate_on_conflict: bool = False):
        self.collection_name = collection_name.replace("-", "_").lower()
        self.path = path
        self.embedding_dim = embedding_dim
        self.sparse_vector_name = sparse_vector_name
        self.client = qdrant_client.QdrantClient(path=self.path)
        self._ensure_collection_exists(recreate_on_conflict)
        logger.info(f"QdrantLocalVectorDB (Híbrido) conectado a la colección='{self.collection_name}' en {path}")

    def _ensure_collection_exists(self, recreate_on_conflict: bool):
        """Crea la colección con soporte híbrido si no existe."""
        try:
            collection_info = self.client.get_collection(collection_name=self.collection_name)
            # Verificar si la configuración es compatible (simplificado)
            has_sparse = self.sparse_vector_name in (collection_info.sparse_vectors or {})
            if not has_sparse and recreate_on_conflict:
                logger.warning(f"La colección '{self.collection_name}' existe pero sin soporte para vectores dispersos. Recreando debido a 'recreate_on_conflict=True'.")
                self.client.delete_collection(collection_name=self.collection_name)
                raise RuntimeError("Colección recreada.") # Forzar la creación
            elif not has_sparse:
                 raise ValueError(f"La colección '{self.collection_name}' existe pero no fue creada para vectores dispersos ('{self.sparse_vector_name}'). Habilite 'recreate_on_conflict=True' o borre el directorio de la VDB.")
            
            logger.debug(f"Colección '{self.collection_name}' ya existe y es compatible.")

        except Exception:
            logger.info(f"Creando nueva colección híbrida '{self.collection_name}'.")
            
            # <<< INICIO DE LA CORRECCIÓN SIMPLIFICADA >>>
            # Al crear los parámetros del vector disperso, no especificamos el índice.
            # Qdrant usará el valor por defecto, que es el correcto para este caso.
            sparse_vector_params = rest.SparseVectorParams() # <--- SIN ARGUMENTOS

            self.client.create_collection(
                collection_name=self.collection_name,
                vectors_config={
                    "": rest.VectorParams(size=self.embedding_dim, distance=rest.Distance.COSINE),
                },
                sparse_vectors_config={
                    self.sparse_vector_name: sparse_vector_params
                }
            )
            # <<< FIN DE LA CORRECCIÓN >>>

    async def add_vectors(self, chunks: List[Chunk], sparse_vectors_map: Optional[Dict[str, Dict]] = None) -> None:
        """Añade puntos con vectores densos y, opcionalmente, dispersos."""
        points_to_upsert = []
        for chunk in chunks:
            if chunk.embedding is None: continue
            point_id = self._generate_point_id(chunk.id)
            payload = chunk.metadata.copy()
            payload["original_id"] = chunk.id
            payload["text_preview"] = chunk.text[:250]

            vectors_dict = {"": chunk.embedding.tolist()}
            if sparse_vectors_map and chunk.id in sparse_vectors_map and sparse_vectors_map[chunk.id] is not None:
                vectors_dict[self.sparse_vector_name] = sparse_vectors_map[chunk.id]

            points_to_upsert.append(
                rest.PointStruct(id=point_id, vector=vectors_dict, payload=payload)
            )
        
        if points_to_upsert:
            await asyncio.to_thread(self.client.upsert, collection_name=self.collection_name, points=points_to_upsert, wait=True)
            logger.info(f"Upserted {len(points_to_upsert)} puntos híbridos en '{self.collection_name}'.")

    async def search(self, dense_query_vector: Optional[np.ndarray], sparse_query_vector: Optional[Dict], top_k: int, filters: Optional[Dict[str, Any]] = None) -> List[Chunk]:
        """Realiza búsqueda híbrida, densa o dispersa."""
        if dense_query_vector is None and sparse_query_vector is None: return []

        qdrant_filter = _build_qdrant_filter(filters)
        
        # Qdrant fusiona resultados de múltiples queries en una búsqueda por lotes
        search_requests = []
        if dense_query_vector is not None:
            search_requests.append(rest.SearchRequest(vector=dense_query_vector.tolist(), limit=top_k, filter=qdrant_filter, with_payload=True, with_vector=True))
        if sparse_query_vector is not None:
            search_requests.append(rest.SearchRequest(vector=rest.SparseVector(**sparse_query_vector), using=self.sparse_vector_name, limit=top_k, filter=qdrant_filter, with_payload=True, with_vector=True))

        if not search_requests: return []

        batch_results = await asyncio.to_thread(self.client.search_batch, collection_name=self.collection_name, requests=search_requests)

        # La fusión y re-ranking (RRF) se hará en el RetrievalManager.
        # Aquí, simplemente devolvemos todos los resultados con sus scores.
        # Es crucial que el RetrievalManager pueda diferenciar el origen de cada resultado.
        
        results_map: Dict[str, Chunk] = {}
        for result_set in batch_results:
            for hit in result_set:
                payload = hit.payload or {}
                chunk_id = payload.get("original_id", hit.id)
                
                if chunk_id in results_map:
                    # Si ya vimos este chunk (ej. apareció en búsqueda densa y dispersa),
                    # guardamos el score más alto. El RetrievalManager se encargará del RRF.
                    results_map[chunk_id].metadata["similarity_score"] = max(results_map[chunk_id].metadata["similarity_score"], hit.score)
                else:
                    vector_dense = hit.vector.get("") if isinstance(hit.vector, dict) else hit.vector
                    results_map[chunk_id] = Chunk(
                        id=chunk_id,
                        text=payload.get("text_preview", ""),
                        metadata={**payload, "similarity_score": hit.score},
                        embedding=np.array(vector_dense) if vector_dense else None
                    )
        
        return list(results_map.values())

    def _generate_point_id(self, internal_id: str) -> str:
         """Genera un UUID consistente para Qdrant basado en un ID interno."""
         return str(uuid.uuid5(uuid.NAMESPACE_DNS, internal_id))

    # Opcional: Añadir un método para obtener chunks por ID si es necesario para MMR
    # async def get_chunks_by_ids(self, ids: List[str]) -> List[Optional[Chunk]]:
    #     # Recuperar puntos por ID de Qdrant
    #     point_ids = [self._generate_point_id(id) for id in ids]
    #     try:
    #         response = self.client.retrieve(
    #             collection_name=self.collection_name,
    #             points=point_ids,
    #             with_payload=True,
    #             with_vectors=True # Necesitamos los vectores/embeddings
    #         )
    #         chunk_map = {p.payload.get('chunk_id', p.id): p for p in response}
    #         results = []
    #         for id in ids:
    #              point = chunk_map.get(id)
    #              if point:
    #                   payload = point.payload or {}
    #                   metadata = payload.get("metadata", {})
    #                   chunk = Chunk(
    #                        id=payload.get("chunk_id", point.id),
    #                        text=payload.get("text", ""),
    #                        metadata=metadata,
    #                        embedding=np.array(point.vector, dtype=np.float32) if point.vector else None,
    #                        token_count=payload.get("token_count")
    #                   )
    #                   results.append(chunk)
    #              else:
    #                   results.append(None) # Chunk no encontrado
    #         return results
    #     except Exception as e:
    #          logger.exception(f"Error recuperando chunks por ID en Qdrant: {e}")
    #          return [None] * len(ids)

    async def delete_vectors(self, ids: List[str]) -> None:
        """Elimina vectores basados en IDs originales (chunk_id o window_id)."""
        if not ids: return
        point_ids = [self._generate_point_id(id) for id in ids]
        try:
            await asyncio.to_thread(
                self.client.delete,
                collection_name=self.collection_name,
                points_selector=rest.PointIdsList(points=point_ids),
                wait=True
            )
            logger.info(f"Eliminados {len(point_ids)} puntos de '{self.collection_name}'.")
        except Exception as e:
            logger.error(f"Error durante delete en Qdrant: {e}")

    async def get_vector_count(self) -> int:
        """Obtiene el número de vectores en la colección."""
        try:
             count_result = await asyncio.to_thread(
                  self.client.count, collection_name=self.collection_name, exact=True # Pedir conteo exacto
             )
             return count_result.count
        except Exception as e:
             if "not found" in str(e).lower() or ("status_code" in dir(e) and e.status_code == 404):
                  return 0
             logger.error(f"Error obteniendo count de Qdrant: {e}")
             return -1

    async def optimize(self):
        """Solicita a Qdrant optimizar los segmentos de la colección (si está soportado/implementado)."""
        logger.info(f"Solicitando optimización para la colección Qdrant '{self.collection_name}'...")
        try:
             # El método 'optimize' no existe directamente en el cliente Python estándar hasta donde sé.
             # La optimización suele ser automática o configurada en el servidor Qdrant.
             # Sin embargo, podemos forzar un flush si es útil.
             await asyncio.to_thread(self.client.http.collections_api.update_collection_aliases, collection_name=self.collection_name, wait=True)
             # await asyncio.to_thread(self.client.flush, collection_name=self.collection_name) # Flush puede existir en algunas versiones/modos
             logger.info(f"Flush/Wait completado para la colección Qdrant '{self.collection_name}'. La optimización real depende del servidor.")
        except AttributeError:
             logger.warning(f"El cliente Qdrant actual no tiene método 'flush' o similar para forzar operaciones.")
        except Exception as e:
             logger.error(f"Error durante la solicitud de flush/wait para optimización de Qdrant: {e}")

    # Callbacks (Sin cambios)
    async def index_done_callback(self):
         # Forzar flush al final puede ser útil en algunos escenarios locales
         # await asyncio.to_thread(self.client.flush, collection_name=self.collection_name)
         pass
    async def query_done_callback(self): pass
    async def close(self):
         if hasattr(self.client, 'close'):
              logger.info("Cerrando cliente Qdrant...")
              await asyncio.to_thread(self.client.close)
              logger.info("Cliente Qdrant cerrado.")


---
File: /storage/__init__.py
---




---
File: /storage/base.py
---

from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Any, Sequence
import numpy as np
from ..core.datatypes import Chunk # Asegúrate que la importación relativa sea correcta

class BaseKVStore(ABC):
    """Interfaz abstracta para almacenamiento Key-Value."""
    @abstractmethod
    async def get(self, key: str) -> Optional[Dict[str, Any]]:
        pass

    @abstractmethod
    async def set(self, key: str, value: Dict[str, Any]) -> None:
        pass

    @abstractmethod
    async def delete(self, key: str) -> None:
        pass

    @abstractmethod
    async def get_batch(self, keys: List[str]) -> List[Optional[Dict[str, Any]]]:
        pass

    @abstractmethod
    async def set_batch(self, items: Dict[str, Dict[str, Any]]) -> None:
        pass

    @abstractmethod
    async def exists(self, key: str) -> bool:
        pass

    @abstractmethod
    async def get_all_keys_iterator(self): # Devuelve un iterador/generador
        pass

    # Método opcional para búsqueda por metadatos si el backend lo soporta
    async def search_by_metadata(self, filters: Dict[str, Any], limit: int = 100) -> List[Dict[str, Any]]:
        raise NotImplementedError("Metadata search not implemented for this backend.")


class BaseVectorDB(ABC):
    """Interfaz abstracta para bases de datos vectoriales."""
    @abstractmethod
    async def add_vectors(self, chunks: List[Chunk]) -> List[str]: # Devuelve IDs
        pass

    @abstractmethod
    async def search(self, query_embedding: np.ndarray, top_k: int, filters: Optional[Dict[str, Any]] = None) -> List[Chunk]:
        pass

    @abstractmethod
    async def delete_vectors(self, ids: List[str]) -> None:
        pass

    @abstractmethod
    async def get_vector_count(self) -> int:
        pass


class BaseKnowledgeGraph(ABC):
    """Interfaz abstracta para almacenamiento de grafos de conocimiento."""
    @abstractmethod
    async def add_node(self, node_id: str, node_type: str, properties: Dict[str, Any]) -> None:
        pass

    @abstractmethod
    async def add_edge(self, source_id: str, target_id: str, label: str, properties: Dict[str, Any]) -> None:
        pass

    @abstractmethod
    async def get_node(self, node_id: str) -> Optional[Dict[str, Any]]:
        pass

    @abstractmethod
    async def get_neighbors(self, node_id: str, relation_label: Optional[str] = None) -> List[Dict[str, Any]]:
        pass

    # Métodos adaptados/inspirados en PathRAG podrían ir aquí
    @abstractmethod
    async def find_paths(self, start_node_id: str, end_node_id: str, max_depth: int = 3) -> List[List[Dict[str, Any]]]:
        pass

    @abstractmethod
    async def update_node_properties(self, node_id: str, properties: Dict[str, Any]) -> None:
        pass

    @abstractmethod
    async def delete_node(self, node_id: str) -> None:
        pass


---
File: /tests/llm/test_ollama_wrapper.py
---

import pytest
import ollama # Necesita ser instalado para probar
from ..ares.llm.ollama import OllamaWrapper # Ajustar

pytestmark = pytest.mark.asyncio

# Mock de la respuesta de Ollama para evitar llamadas reales
@pytest.fixture
def mock_ollama_chat(mocker):
    # Mock del método chat asíncrono
    async def async_chat_mock(*args, **kwargs):
        if kwargs.get("stream", False):
            async def stream_gen():
                yield {'message': {'content': 'Hola'}, 'done': False}
                yield {'message': {'content': ' mundo!'}, 'done': False}
                yield {'done': True} # Simular finalización
            return stream_gen()
        else:
            return {'message': {'content': 'Hola mundo!'}}

    # Mock del cliente asíncrono
    mock_client = mocker.AsyncMock(spec=ollama.AsyncClient)
    mock_client.chat = async_chat_mock # Asignar nuestro mock asíncrono

    # Mockear la inicialización del cliente dentro del wrapper
    mocker.patch('ollama.AsyncClient', return_value=mock_client)
    return mock_client


async def test_ollama_wrapper_init(mock_ollama_chat):
    wrapper = OllamaWrapper(model_name="test_model", host_url="http://mockhost:11434")
    assert wrapper.model_name == "test_model"
    # Verificar que se llamó a ollama.AsyncClient con los argumentos correctos
    ollama.AsyncClient.assert_called_once_with(host="http://mockhost:11434", timeout=None)

async def test_ollama_generate_no_stream(mock_ollama_chat):
    wrapper = OllamaWrapper(model_name="test_model")
    response = await wrapper.generate("Dime hola")
    assert response == "Hola mundo!"
    # Verificar que client.chat fue llamado
    mock_ollama_chat.chat.assert_called_once()
    call_args = mock_ollama_chat.chat.call_args
    assert call_args.kwargs['model'] == "test_model"
    assert call_args.kwargs['messages'] == [{"role": "user", "content": "Dime hola"}]
    assert not call_args.kwargs['stream']

async def test_ollama_generate_with_history_system(mock_ollama_chat):
    wrapper = OllamaWrapper(model_name="test_model")
    history = [{"role": "user", "content": "Pregunta anterior"}, {"role": "assistant", "content": "Respuesta anterior"}]
    system = "Eres un bot"
    response = await wrapper.generate("Siguiente pregunta", system_prompt=system, history=history)
    assert response == "Hola mundo!"
    mock_ollama_chat.chat.assert_called_once()
    call_args = mock_ollama_chat.chat.call_args
    expected_messages = [
        {"role": "system", "content": "Eres un bot"},
        {"role": "user", "content": "Pregunta anterior"},
        {"role": "assistant", "content": "Respuesta anterior"},
        {"role": "user", "content": "Siguiente pregunta"},
    ]
    assert call_args.kwargs['messages'] == expected_messages

async def test_ollama_generate_stream(mock_ollama_chat):
    wrapper = OllamaWrapper(model_name="test_model")
    response_stream = await wrapper.generate("Dime hola en stream", stream=True)
    chunks = [chunk async for chunk in response_stream]
    assert chunks == ['Hola', ' mundo!']
    mock_ollama_chat.chat.assert_called_once()
    call_args = mock_ollama_chat.chat.call_args
    assert call_args.kwargs['stream'] is True

async def test_ollama_generate_with_options(mock_ollama_chat):
     wrapper = OllamaWrapper(model_name="test_model")
     await wrapper.generate("Test", temperature=0.5, max_tokens=100, top_k=40) # max_tokens -> num_predict
     mock_ollama_chat.chat.assert_called_once()
     call_args = mock_ollama_chat.chat.call_args
     assert call_args.kwargs['options'] == {'temperature': 0.5, 'num_predict': 100, 'top_k': 40}

# Añadir tests para manejo de errores si es necesario


---
File: /tests/conftest.py
---

import pytest
import os
import shutil
from ..ares.storage.kv_store.sqlite import SQLiteKVStore # Ajustar importación según estructura final

@pytest.fixture(scope="function") # 'function' scope recrea el fixture para cada test
def test_working_dir(tmp_path):
    """Crea un directorio temporal para las pruebas."""
    ares_data_dir = tmp_path / "ares_data"
    ares_data_dir.mkdir()
    print(f"\nTest working directory: {ares_data_dir}")
    yield str(ares_data_dir)
    # Limpieza (opcional, tmp_path generalmente se limpia solo)
    # shutil.rmtree(ares_data_dir)

@pytest.fixture
def sample_sqlite_kv_store(test_working_dir):
    """Fixture para un KVStore SQLite limpio para cada test."""
    db_path = os.path.join(test_working_dir, "test_kv.db")
    store = SQLiteKVStore(db_path=db_path)
    yield store
    # Opcional: eliminar db después del test si es necesario
    if os.path.exists(db_path):
         os.remove(db_path)

# Añadir fixtures similares para Qdrant, NetworkX si es necesario inicializarlos/limpiarlos


---
File: /tests/test_normalizer.py
---

import pytest
from ..ares.ingestion.normalizer import Normalizer # Ajustar importación

def test_normalize_whitespace():
    assert Normalizer.normalize_whitespace("  Mucho   espacio  \n\t aquí  ") == "Mucho espacio aquí"
    assert Normalizer.normalize_whitespace("Sin cambios") == "Sin cambios"
    assert Normalizer.normalize_whitespace("") == ""

def test_remove_control_characters():
    assert Normalizer.remove_control_characters("Texto\x00con\x1fcontroles\x7f") == "Textoconcontroles"
    assert Normalizer.remove_control_characters("Texto con\nsaltos\ty tabs") == "Texto con\nsaltos\ty tabs"
    assert Normalizer.remove_control_characters("Normal") == "Normal"

def test_normalize_integration():
    assert Normalizer.normalize("  Texto\x00 \n  con\t todo  ") == "Texto con todo"
    assert Normalizer.normalize(" Ya limpio ") == "Ya limpio"
    assert Normalizer.normalize(None) == "" # Manejo de None
    assert Normalizer.normalize(123) == ""  # Manejo de no-string


---
File: /utils/__init__.py
---




---
File: /utils/helpers.py
---

# ares/utils/helpers.py
import os 
import re
import hashlib
import json
import numpy as np
from typing import Optional, List, Any, Tuple, Union, Dict, Callable, Type
import time
import importlib
from dataclasses import dataclass, field
from ..storage.base import BaseKVStore
from .logging_setup import logger # Usar nuestro logger
from datetime import datetime # Para timestamps en nombres de archivo

_json_debug_dir = None

def _get_json_debug_dir(base_working_dir: str) -> str:
    global _json_debug_dir
    if _json_debug_dir is None:
        _json_debug_dir = os.path.join(base_working_dir, "json_debug_output")
        os.makedirs(_json_debug_dir, exist_ok=True)
        logger.info(f"Directorio de depuración JSON: {_json_debug_dir}")
    return _json_debug_dir

def log_or_save_raw_json(
    json_string: Optional[str],
    component_name: str,
    window_id: Optional[str] = None, # Para DescriptionGenerator y Verifier
    query_text: Optional[str] = None, # Para QueryPlanner
    base_working_dir: Optional[str] = "./ares_data", # Necesario para crear el directorio
    save_to_file: bool = True # Controla si se guarda a archivo
):
    """
    Loguea una cadena JSON cruda y opcionalmente la guarda en un archivo para depuración.
    """
    if not json_string:
        logger.debug(f"[{component_name}] No hay cadena JSON cruda para loguear/guardar.")
        return

    log_message = f"[{component_name}] JSON CRUDO EXTRAÍDO PARA DEPURACIÓN:\n---\n{json_string}\n---"
    logger.debug(log_message) # Loguear siempre a nivel DEBUG

    if save_to_file and base_working_dir:
        try:
            debug_dir = _get_json_debug_dir(base_working_dir)
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            identifier = ""
            if window_id:
                identifier = f"win_{window_id}"
            elif query_text:
                # Tomar los primeros N caracteres de la query para el nombre del archivo, limpiando caracteres no válidos
                safe_query_prefix = re.sub(r'[^\w_.)( -]', '', query_text[:30]).strip().replace(" ", "_")
                identifier = f"query_{safe_query_prefix}"
            else:
                identifier = "unknown"

            filename = f"{component_name}_{identifier}_{timestamp_str}.json"
            filepath = os.path.join(debug_dir, filename)

            with open(filepath, "w", encoding="utf-8") as f:
                f.write(json_string)
            logger.info(f"[{component_name}] JSON crudo guardado para depuración en: {filepath}")
        except Exception as e:
            logger.error(f"[{component_name}] Error guardando JSON crudo para depuración: {e}")

# --- Constantes ---
GRAPH_FIELD_SEP = "<|>" # Separador para listas en campos de grafo (source_id)

# --- Hashing ---
def compute_mdhash_id(content: Union[str, bytes], prefix: str = "") -> str:
    """Genera un ID hash MD5 para un contenido, con un prefijo opcional."""
    if isinstance(content, str):
         content_bytes = content.encode('utf-8', errors='replace')
    elif isinstance(content, bytes):
         content_bytes = content
    else:
         content_bytes = str(content).encode('utf-8', errors='replace')
    return prefix + hashlib.md5(content_bytes).hexdigest()

# --- Decodificación Segura ---
def safe_unicode_decode(content_bytes: bytes) -> str:
    """Intenta decodificar bytes, manejando escapes unicode incorrectos."""
    try:
        return content_bytes.decode('utf-8')
    except UnicodeDecodeError:
        logger.warning("UnicodeDecodeError encontrado, intentando decodificación alternativa.")
        try:
            # Intentar reparar escapes unicode mal formados (ej. \uXXXX)
            content_str = content_bytes.decode('latin-1') # Leer byte a byte
            unicode_escape_pattern = re.compile(r"\\u([0-9a-fA-F]{4})")
            def replace_unicode_escape(match):
                try: return chr(int(match.group(1), 16))
                except ValueError: return match.group(0) # Devolver original si inválido
            processed_str = unicode_escape_pattern.sub(replace_unicode_escape, content_str)
            # Re-codificar y decodificar en UTF-8 ignorando errores residuales
            return processed_str.encode('utf-8', 'ignore').decode('utf-8', 'ignore')
        except Exception as e:
             logger.warning(f"Fallback de decodificación unicode falló: {e}. Usando 'replace'.")
             return content_bytes.decode('utf-8', errors='replace')

def get_config_value(config: Dict, key_path: str, default: Any = None) -> Any:
    """
    Obtiene un valor de la configuración usando una ruta de claves separadas por puntos.
    Ejemplo: get_config_value(config, "retrieval.planner_llm_config.model_name")

    Args:
        config: El diccionario de configuración completo.
        key_path: La ruta de claves separadas por puntos (ej. "section.subsection.key").
        default: El valor a devolver si la clave no se encuentra.

    Returns:
        El valor encontrado o el valor por defecto.
    """
    keys = key_path.split('.')
    value = config
    try:
        for key in keys:
            if isinstance(value, dict):
                value = value[key]
            else:
                # Si intentamos acceder a una clave en algo que no es un dict, fallamos
                return default
        return value
    except (KeyError, TypeError, IndexError):
        # Capturar KeyError (clave no encontrada), TypeError (intentar indexar no dict),
        # IndexError (potencialmente si se usa split en algo inesperado)
        return default
# --- Fin de la función get_config_value ---



# --- Vectores ---
def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    """Calcula la similitud coseno entre dos vectores numpy (asume no normalizados)."""
    if v1.shape != v2.shape:
        logger.error(f"Dimensiones de vectores no coinciden: {v1.shape} vs {v2.shape}")
        return 0.0
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    # Asegurar que sean float32 para evitar problemas de precisión en dot
    dot_product = np.dot(v1.astype(np.float32), v2.astype(np.float32))
    similarity = dot_product / (norm1 * norm2)
    return float(np.clip(similarity, -1.0, 1.0))

def normalize_vector(v: Optional[np.ndarray]) -> Optional[np.ndarray]:
    """Normaliza un vector numpy a longitud unitaria, maneja None."""
    if v is None: return None
    norm = np.linalg.norm(v)
    if norm == 0: return v
    return (v / norm).astype(np.float32) # Devolver siempre float32


# --- Carga Dinámica ---
def get_callable_from_string(import_string: str) -> Callable:
    """Importa y devuelve un callable (clase o función) desde un string."""
    try:
        module_name, class_name = import_string.rsplit('.', 1)
        module = importlib.import_module(module_name)
        return getattr(module, class_name)
    except (ImportError, AttributeError, ValueError) as e:
        logger.error(f"No se pudo cargar el callable '{import_string}': {e}")
        raise ImportError(f"No se pudo cargar {import_string}") from e


# --- Caché (Simple) ---
# (Puede moverse a un módulo dedicado si crece)
@dataclass
class CacheData:
    args_hash: str
    content: Union[str, Dict, List]
    prompt: Optional[str] = None
    mode: str = "default"
    timestamp: float = field(default_factory=time.time)

async def get_from_cache(kv_store: Optional['BaseKVStore'], cache_key: str) -> Optional[CacheData]:
    if kv_store is None: return None
    cached_item = await kv_store.get(cache_key)
    if cached_item:
        try:
             # Asumir que el kv_store devuelve dicts
             return CacheData(**cached_item)
        except Exception as e:
             logger.warning(f"Error reconstruyendo CacheData desde KVStore {cache_key}: {e}")
             # await kv_store.delete(cache_key) # Opcional: eliminar entrada corrupta
             return None
    return None

async def save_to_cache(kv_store: Optional['BaseKVStore'], cache_key: str, data: CacheData):
    if kv_store is None: return
    # Convertir a dict antes de guardar (asegura serialización JSON si KV es JSON)
    try:
        # Convertir numpy arrays si existen (ej. en embeddings cacheados)
        dict_data = data.__dict__.copy()
        for key, value in dict_data.items():
             if isinstance(value, np.ndarray):
                  dict_data[key] = value.tolist() # Convertir a lista para JSON
        await kv_store.set(cache_key, dict_data)
    except Exception as e:
         logger.error(f"Error guardando en caché para key {cache_key}: {e}")


# --- Otros Helpers ---
# Añadir aquí funciones como contar tokens (si se usa fuera de clases específicas),
# manejo de tiempo, etc.

# Ejemplo de helper para contar tokens (si se usa globalmente)
# --- Contar/Codificar Tokens ---
_token_encoder = None
_encoder_lock = None # Para seguridad en entornos multihilo/async al inicializar

async def _init_encoder(model: str = "cl100k_base"): # Debe ser async
    global _token_encoder, _encoder_lock
    if _encoder_lock is None:
         try:
              import asyncio
              _encoder_lock = asyncio.Lock()
         except ImportError:
              import threading
              _encoder_lock = threading.Lock()

    # Usar el lock apropiado (async o thread)
    if isinstance(_encoder_lock, asyncio.Lock):
        async with _encoder_lock:
            if _token_encoder is None:
                # ... (lógica de inicialización de tiktoken como la tenías)
                try:
                    import tiktoken
                    _token_encoder = tiktoken.get_encoding(model)
                    logger.debug(f"Encoder tiktoken '{model}' cargado globalmente.")
                except ImportError:
                    logger.warning("Tiktoken no instalado. Codificación/Conteo usará fallback split().")
                    _token_encoder = "split"
                except Exception as e:
                    logger.error(f"Error cargando encoder tiktoken {model}: {e}. Usando fallback split().")
                    _token_encoder = "split"
    else: # threading.Lock
        with _encoder_lock:
            if _token_encoder is None:
                # ... (misma lógica de inicialización)
                try:
                    import tiktoken
                    _token_encoder = tiktoken.get_encoding(model)
                    logger.debug(f"Encoder tiktoken '{model}' cargado globalmente.")
                except ImportError:
                    logger.warning("Tiktoken no instalado. Codificación/Conteo usará fallback split().")
                    _token_encoder = "split"
                except Exception as e:
                    logger.error(f"Error cargando encoder tiktoken {model}: {e}. Usando fallback split().")
                    _token_encoder = "split"

# --- Nueva función ---
def encode_string_by_tiktoken(text: str, model: str = "cl100k_base") -> List[int]:
    """
    Codifica un string en una lista de IDs de tokens usando tiktoken.
    Proporciona fallback si tiktoken no está disponible o falla.
    NOTA: Esta versión es SÍNCRONA para simplificar su uso directo donde se necesita.
    La inicialización del encoder es manejada de forma asíncrona si es necesario.
    """
    global _token_encoder
    if _token_encoder is None or _token_encoder == "split":
         # Intenta inicializar si aún no se ha hecho (puede que no sea async aquí, es una limitación)
         # O simplemente acepta el fallback si no se inicializó antes
         if _token_encoder is None:
              logger.warning("Llamando a encode_string_by_tiktoken sin inicialización previa del encoder. Usando fallback split().")
         # Fallback muy simple: IDs basados en posición (¡no útil para conteo real!)
         # Para el conteo en PromptBuilder, len() es suficiente.
         # Aquí devolvemos algo para que el tipo coincida, aunque no sea semántico.
         # Para el uso en PromptBuilder, solo necesitamos len(), así que esto es menos crítico.
         # Si se necesitara para otra cosa, el fallback debería ser más robusto.
         # return list(range(len(text.split()))) # Fallback simple si no hay tiktoken
         # CORRECCIÓN: PromptBuilder solo necesita len(), no los IDs reales.
         # Devolvemos una lista con longitud correcta basada en split para que len() funcione.
         return [0] * len(text.split()) # Fallback para len()


    if not isinstance(text, str):
         text = str(text) # Intentar convertir a string

    try:
        # Asegurar que _token_encoder sea el objeto encoder, no "split"
        if hasattr(_token_encoder, 'encode'):
             return _token_encoder.encode(text, disallowed_special=())
        else:
             logger.error("Encoder tiktoken no está correctamente inicializado. Usando fallback split().")
             return [0] * len(text.split())
    except Exception as e:
         logger.error(f"Error codificando texto con tiktoken: {e}. Usando fallback.")
         return [0] * len(text.split())
# --- Fin nueva función ---

def count_tokens_global(text: str, model: str = "cl100k_base") -> int:
    """Cuenta tokens usando tiktoken (usa el encoder cacheado)."""
    # Esta función ahora puede simplemente usar la longitud de la salida de encode_string_by_tiktoken
    return len(encode_string_by_tiktoken(text, model))


---
File: /utils/logging_setup.py
---

# utils/logging_setup.py
import logging
import sys
import os
from logging.handlers import RotatingFileHandler
from typing import Optional # <--- AÑADIR ESTA LÍNEA

# Definir un logger específico para ARES
logger = logging.getLogger("ares")

def setup_logging(log_level_str: str = "INFO", log_file: Optional[str] = None, max_bytes: int = 10*1024*1024, backup_count: int = 3):
    """Configura el logging para consola y archivo rotativo opcional."""
    try:
        log_level = getattr(logging, log_level_str.upper())
    except AttributeError:
        print(f"WARN: Nivel de log '{log_level_str}' inválido. Usando INFO por defecto.", file=sys.stderr)
        log_level = logging.INFO

    # Configurar formato
    log_format = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )

    logger.setLevel(log_level)

    # Evitar añadir handlers múltiples si se llama varias veces
    if logger.hasHandlers():
        logger.handlers.clear()

    # Handler para consola
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_format)
    # Permitir que el handler de consola muestre logs de nivel INFO o superior,
    # independientemente del nivel raíz (útil si raíz es DEBUG pero consola solo INFO)
    console_handler.setLevel(max(log_level, logging.INFO))
    logger.addHandler(console_handler)

    # Handler para archivo rotativo (si se especifica)
    if log_file:
        try:
            # Asegurar que el directorio del log exista
            log_dir = os.path.dirname(log_file)
            if log_dir and not os.path.exists(log_dir):
                os.makedirs(log_dir, exist_ok=True)

            # Usar RotatingFileHandler
            file_handler = RotatingFileHandler(
                log_file,
                maxBytes=max_bytes,
                backupCount=backup_count,
                encoding='utf-8'
            )
            file_handler.setFormatter(log_format)
            # El handler de archivo siempre respeta el nivel raíz del logger
            file_handler.setLevel(log_level)
            logger.addHandler(file_handler)
            print(f"INFO: Logging configurado para archivo: {log_file} (Nivel: {log_level_str})")
        except Exception as e:
            print(f"ERROR: No se pudo configurar el logging a archivo {log_file}: {e}", file=sys.stderr)

    # Configurar logging de librerías externas (ejemplo)
    logging.getLogger("urllib3").setLevel(logging.WARNING)
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("qdrant_client").setLevel(logging.INFO) # O WARNING si es muy verboso
    logging.getLogger("chromadb").setLevel(logging.WARNING)
    logging.getLogger("networkx").setLevel(logging.INFO)

    logger.info(f"Logging ARES inicializado a nivel {log_level_str}")


---
File: /utils/pathrag_logic.py
---

# ares/utils/pathrag_logic.py
from typing import List, Dict, Any, Optional
import networkx as nx # Mantener si se usa para type hints en el futuro

# Importar el logger de forma segura para evitar problemas de importación circular
try:
    from .logging_setup import logger
except ImportError:
    import logging
    logger = logging.getLogger(__name__)
    logger.addHandler(logging.NullHandler()) # Evitar warning si no hay handlers configurados


async def find_relevant_paths(
    graph: nx.DiGraph, # O el tipo de grafo apropiado si cambia
    start_node: str,
    end_node: Optional[str] = None,
    max_depth: int = 2,
    **kwargs
) -> List[Dict[str, Any]]: # El tipo de retorno debe coincidir con lo esperado por pathrag_networkx.py
    """
    Placeholder para la lógica de búsqueda de caminos PathRAG.
    IMPORTANTE: Esta es una implementación vacía porque la funcionalidad KG/PathRAG
    está incompleta o desactivada por defecto en la configuración.
    Devuelve una lista vacía para evitar errores durante la ejecución.
    """
    # Loguear una advertencia si esta función llega a ser llamada
    # (lo cual no debería ocurrir si use_kg_search=false).
    logger.warning("Llamada a la función placeholder find_relevant_paths. "
                   "La lógica KG/PathRAG no está completamente implementada o está desactivada.")
    return []

# Puedes añadir otras funciones placeholder relacionadas con PathRAG aquí si son necesarias
# para satisfacer otras importaciones en el futuro.


---
File: /vllm_server/manager.py
---

# ares/vllm_server/manager.py

import asyncio
import sys
import os
import json
from typing import List, Dict, Optional, Any
from tqdm.asyncio import tqdm

# Dependencias del proyecto ARES
from ..utils.logging_setup import logger
from ..utils.helpers import get_config_value

class VLLMServerManager:
    """
    Gestiona el ciclo de vida de un proceso 'vllm serve' como un subproceso.
    Esta versión final está diseñada para ser robusta, manejar correctamente
    la configuración del TOML y proporcionar una retroalimentación clara.
    """
    def __init__(self, config: Dict[str, Any]):
        self.config: Dict[str, Any] = get_config_value(config, "vllm_server", {})
        self.process: Optional[asyncio.subprocess.Process] = None
        self._monitoring_task: Optional[asyncio.Task] = None
        self.server_ready = asyncio.Event()
        logger.info("VLLMServerManager inicializado.")

    def _build_command(self) -> List[str]:
        """
        Construye la lista de argumentos para 'vllm serve' a partir de la config.
        Maneja correctamente argumentos simples, booleanos y complejos (JSON).
        """
        if not self.config.get("enabled", False) or not self.config.get("model"):
            return []

        command = [sys.executable, "-u", "-m", "vllm.entrypoints.openai.api_server"]
        
        for key, value in self.config.items():
            # Claves de control internas del manager, no son flags de vLLM
            if key in ["enabled", "vllm_extra_args"]:
                continue
            
            flag = f"--{key.replace('_', '-')}"
            
            if isinstance(value, bool):
                # Para flags booleanos, solo los añadimos si son `true`
                if value:
                    command.append(flag)
            elif isinstance(value, (dict, list)):
                # Para argumentos complejos, los serializamos como un string JSON
                # y los envolvemos en comillas para la shell (aunque `exec` no lo necesita, es buena práctica)
                command.append(flag)
                command.append(json.dumps(value))
            else:
                # Para argumentos simples (string, int, float)
                command.append(flag)
                command.append(str(value))
        
        # Manejar los argumentos de vllm_extra_args de la misma manera
        extra_args = self.config.get("vllm_extra_args", {})
        if isinstance(extra_args, dict):
            for key, value in extra_args.items():
                flag = f"--{key.replace('_', '-')}"
                if isinstance(value, bool):
                    if value: command.append(flag)
                elif isinstance(value, (dict, list)):
                    command.append(flag)
                    command.append(json.dumps(value))
                else:
                    command.append(flag)
                    command.append(str(value))
        
        logger.info(f"Comando vLLM construido: {' '.join(command)}")
        return command

    async def _monitor_stream(self, stream: asyncio.StreamReader, prefix: str):
        """Lee y loguea un stream (stdout/stderr) del subproceso."""
        ready_message = "Uvicorn running on"
        while not stream.at_eof():
            try:
                line = await stream.readline()
                if not line: break
                decoded_line = line.decode(errors='ignore').strip()
                if decoded_line:
                    logger.info(f"[{prefix}] {decoded_line}")
                    if ready_message in decoded_line and not self.server_ready.is_set():
                        logger.info("Mensaje de 'listo' del servidor vLLM detectado.")
                        self.server_ready.set()
            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error leyendo el stream {prefix}: {e}")
                break

    async def start_server(self, timeout: int = 600):
        """Lanza el servidor vLLM y espera a que esté listo."""
        command = self._build_command()
        if not command: return
        if self.process and self.process.returncode is None: return

        logger.info("Iniciando el servidor vLLM como un subproceso...")
        try:
            env = os.environ.copy()
            # Forzar FlashInfer para máximo rendimiento
            env["VLLM_ATTENTION_BACKEND"] = "FLASHINFER"

            env["VLLM_USE_V1"] = "0"
            logger.info("Forzando el uso del motor vLLM V0 para mayor estabilidad.")

            self.process = await asyncio.create_subprocess_exec(
                *command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )
            
            stdout_task = asyncio.create_task(self._monitor_stream(self.process.stdout, "VLLM_OUT"))
            stderr_task = asyncio.create_task(self._monitor_stream(self.process.stderr, "VLLM_ERR"))
            self._monitoring_task = asyncio.gather(stdout_task, stderr_task)

            logger.info(f"Esperando a que el servidor vLLM esté listo (timeout: {timeout}s)...")
            logger.info("NOTA: El primer arranque, con la compilación de kernels, puede tardar varios minutos.")
            
            # Esperar a que el evento 'server_ready' se active por la tarea de monitoreo
            await asyncio.wait_for(self.server_ready.wait(), timeout=timeout)
            
            # Una vez listo, verificar que el proceso no haya muerto
            if self.process.returncode is not None:
                raise RuntimeError(f"El proceso del servidor vLLM terminó prematuramente con código {self.process.returncode} después de estar listo.")

            url = f"http://{self.config.get('host', '127.0.0.1')}:{self.config.get('port', 8000)}"
            logger.info(f"Servidor vLLM autogestionado confirmado como listo en {url}")

        except asyncio.TimeoutError:
            logger.error(f"El servidor vLLM no estuvo listo en el tiempo de espera de {timeout} segundos.")
            await self.stop_server()
            raise RuntimeError(f"Timeout esperando al servidor vLLM.")
        except Exception as e:
            logger.exception("Fallo catastrófico al intentar iniciar el servidor vLLM.")
            await self.stop_server()
            raise RuntimeError("No se pudo iniciar el servidor vLLM.") from e

    async def stop_server(self):
        """Detiene el servidor vLLM y las tareas de monitoreo de forma segura."""
        if self._monitoring_task and not self._monitoring_task.done():
            self._monitoring_task.cancel()
        
        if self.process and self.process.returncode is None:
            logger.info("Enviando señal de terminación (SIGTERM) al servidor vLLM...")
            try:
                self.process.terminate()
                await asyncio.wait_for(self.process.wait(), timeout=15)
                logger.info("Servidor vLLM terminado correctamente.")
            except asyncio.TimeoutError:
                logger.warning("El servidor vLLM no respondió a SIGTERM. Forzando detención (SIGKILL)...")
                self.process.kill()
                await self.process.wait()
            finally:
                self.process = None


---
File: /main_document_test_ALICE.py
---

# ares/main_document_test_ALICE.py
import asyncio
import os
import sys
from typing import List, Dict, Tuple, Optional, Any
import time
from datetime import datetime, timezone

# --- Inicio de la corrección sys.path ---
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root_parent = os.path.dirname(script_dir)
if project_root_parent not in sys.path:
     sys.path.insert(0, project_root_parent)
# --- Fin de la corrección sys.path ---

from ares.core.orchestrator import AresCore
from ares.utils.logging_setup import logger, setup_logging # Importar setup_logging también
from ares.core.config_loader import get_config_value # Para working_dir

# --- Configuración para esta prueba ---
# Asegúrate que el config_path aquí sea el mismo que usarías para `python -m ares.main`
# o define uno específico si es necesario.
CONFIG_PATH: Optional[str] = None # Usa el default_config.toml de AresCore
# ------------------------------------

async def process_large_text(ares_core: AresCore, document_id: str, large_text_content: str, document_metadata: Dict):
    """
    Procesa un texto largo dividiéndolo en "ventanas" si excede un límite,
    y luego ingesta cada ventana.
    """
    # Un límite arbitrario para simular la división en ventanas.
    # En un sistema real, esto podría ser el context_window del LLM de descripción.
    # O, para Late Chunking puro, podría ser el max_length del embedder si no se quieren
    # embeddings que crucen "documentos" muy grandes.
    # Por ahora, usemos un límite de tokens para el texto de la ventana.
    # Jina V3 maneja hasta 8192 tokens. Podríamos usar un valor un poco menor.
    MAX_WINDOW_TOKENS = get_config_value(ares_core.config, "ingestion.max_window_tokens_for_description", 7800)
    
    # Usar el tokenizer del embedder para un conteo de tokens más preciso
    # (asumiendo que ares_core.embedder.tokenizer está disponible)
    try:
        tokenizer = ares_core.embedder.tokenizer
        def count_tokens_func(text):
            return len(tokenizer.encode(text, add_special_tokens=False))
    except Exception:
        logger.warning("No se pudo acceder al tokenizer del embedder, usando split() para contar tokens en main_document_test.")
        def count_tokens_func(text):
            return len(text.split())


    current_token_count = 0
    current_window_text = ""
    window_counter = 0
    total_ingested_successfully = 0
    total_failed = 0

    # Simular división por párrafos o una heurística simple para no cortar a mitad de palabra
    # En la práctica, Chonkie o un pre-chunker a nivel de documento se encargaría de esto.
    # Aquí, vamos a simplificar y asumir que `large_text_content` es una sola gran cadena.
    # Y la dividiremos basada en conteo de tokens.

    words = large_text_content.split(' ') # División simple por palabras
    
    for word_idx, word in enumerate(words):
        word_with_space = word + " "
        word_token_count = count_tokens_func(word_with_space)

        if current_token_count + word_token_count > MAX_WINDOW_TOKENS and current_window_text:
            # Procesar la ventana actual
            window_counter += 1
            window_id = f"{document_id}_win_{window_counter}"
            logger.info(f"Procesando ventana '{window_id}' (tokens: {current_token_count})")
            success = await ares_core.aingest(window_id, current_window_text.strip(), document_metadata)
            if success:
                total_ingested_successfully += 1
            else:
                total_failed +=1
            
            # Reset para la nueva ventana
            current_window_text = word_with_space
            current_token_count = word_token_count
        else:
            current_window_text += word_with_space
            current_token_count += word_token_count

        # Asegurar que la última parte del texto también se procese
        if word_idx == len(words) - 1 and current_window_text.strip():
            window_counter += 1
            window_id = f"{document_id}_win_{window_counter}"
            logger.info(f"Procesando última ventana '{window_id}' (tokens: {current_token_count})")
            success = await ares_core.aingest(window_id, current_window_text.strip(), document_metadata)
            if success:
                total_ingested_successfully += 1
            else:
                total_failed +=1
                
    logger.info(f"Procesamiento del documento '{document_id}' completado. Ventanas exitosas: {total_ingested_successfully}, Fallidas: {total_failed}")


async def main():
    # --- Configuración de Logging Específica para esta Prueba (opcional) ---
    # Si quieres logs separados o niveles diferentes para esta prueba.
    # Por defecto, usará la configuración de AresCore.
    # setup_logging(log_level_str="DEBUG", log_file="./ares_data_doc_test/doc_test.log")

    print("\n" + "="*15 + " Iniciando Prueba de Documento ARES " + "="*15)
    ares_core: Optional[AresCore] = None
    try:
        ares_core = AresCore(config_path=CONFIG_PATH)
        logger.info("AresCore inicializado para prueba de documento.")
    except Exception as e:
        print(f"\nERROR CRÍTICO: No se pudo inicializar AresCore.")
        logger.exception("Fallo en inicialización de AresCore para prueba de documento.")
        return

    # --- Contenido del Documento ---
    # Pega aquí el texto largo que quieres probar.
    # Puedes leerlo de un archivo si prefieres:
    # with open("ruta/a/tu/documento_largo.txt", "r", encoding="utf-8") as f:
    #     large_text_content = f.read()
    
    large_text_content = """
ALICIA
EN EL PAIS DE LAS MARAVILLAS 

PREFACIO
En el dorado anochecer
bogamos lentamente;
los brazos siéntense ceder
al remo débilmente.
¡Qué dichoso desfallecer
las manos sin oriente!
y qué imPlacable triple voz
suena en el du,lce olvido
Pidiendo ·extrañas invenciones
de quieto y lírico sentido.
¿Cómo callar indiferente
sintiendo su latido?
Dice apremiante la primera
voz que comience el cuento;
la segunda no nos reclama
lógica de argumento}
y nos acucia la tercera
con anheloso acento.
¡Oh} qué silencio más profundo
se impone a todo ruido! 

ALICIA
Es la tierra un maravilloso
país desconocido,
lleno de seres q~te convierten
en real lo fingido.
Cuando la fuente imaginaria
se agota en la inventiva
y a los cristales del ensueño
la luz se les esquiva,
"¡Siga el cuento - claman los seresque tanto nos cautiva!"
Así el país mara'l/illoso
sobre el yunque del yo)
episodio tras episodio,
su leyenda forjó,
y al ocaso) un mundo de amigos
el alma nos pobló.
Recibe, Alicia, este pueril
libro con mano tierna
y ponlo allí donde la infancia
salva la vida interna,
como el ferviente peregrino
guarda una flor eterna. 


CAPíTULO PRIMERO
EN LA MADRIGUERA
ALICIA empezaba a sentirse cansadísima de estar sentada en un margen,
al lado de su hermana, sin saber qué
hacer: por dos veces había atisbado el
libro que ella leía; pero era un libro
sin grabados, sin diálogo, y "¿ de qué
sirve un libro - se dijo Alicia - si no
tiene diálogo ni grabados?"
Iy dándose a pensar de la mejor manera que le permitían
la somnolencia y el atontamiento en que la había sumido el
calor de aquella jornada, consideraba en su fuero interno si
valdría la pena de entretenerse en arrancar margaritas por
el gusto de hacer una cadena con ellas, cuando de pronto saltó a su lado un Conejo Blanco de ojuelos encarnados.
No había en ello nada de extraordinario, ni le pareció a
Alicia cosa fuera de 10 corriente, oír que el Conejo se dijera
a sí mismo:
- j Dios mío! j Dios mío I Voy a llegar tarde.
Cuando lo reflexionara después, comprendería que debía 

ALICIA
haberse maravillado, pero en tanto le parecía la cosa más
natural del mundo; no obstante, viendo que el Conejo se sacaba un reloj del bolsillo del chaleco, 10 miraba y echaba a
correr, Alicia se puso en pie, porque en seguida reflexionó
que nunca había visto un conejo con chaleco, ni con reloj
para saber la hora, y encendida de curiosidad, se fué corriendo por el campo y llegó a tiempo de ver que el Conejo se
metía de cabeza en una gran madriguera al pie de una
barda.
En un instante Alicia se metió detrás de él sin pensar ni
remotamente si le sería posible salir.
La madriguera empezaba siendo horizontal como un túnel, pero luego se hundía bruscamente, tanto que Alicia no
tuvo tiempo de pensar en detenerse, pues se encontró con que
se caía por un sitio semejante a un hondo pozo.
O el pozo era profundísimo, o ella caía muy despacio,
porque si no había tenido tiempo de pensar en detenerse, en
cambio 10 tuvo para mirar alrededor y preguntarse qué era
lo que iba a suceder. En seguida procuró mirar al suelo porque quería ver dónde pisaría; pero estaba demasiado obscuro para ver nada. Luego miró a las paredes del pozo, y
observó que estaban llenas de armarios y de anaqueles de
libros; aquí y allá vió algunos mapas y cuadros colgados
de unos ganchos. Cogió una jarra de uno de los estantes, al
pasar; tenía un marbete que decía "MERMELADA DE NARANJA"; mas, para desencanto suyo, estaba vacía. No le parecía bien arrojarla al fondo por temor de matar a alguien;
así es que procuró dejarla en otro de los estantes mientras
iba descendiendo. 

 ¡Bien - pensó Alicia para su corpiño, - después de
una caída como ésta no hay miedo de rodar por las escaleras!

 Qué valiente me van a encontrar en casa! ¡Como
que ni he de quejarme cuando me encuentren, aunque esto
sea que me he caído del tejado! (cosa que le parecía muy
dentro de lo posible).
y en tanto bajaba, bajaba, bajaba. ¿ No acabaría nunca
aquel descenso?
- ¿ Cuántas millas habré bajado ya? - se preguntó en
voz alta. - Llegaré a algún sitio cerca del centro de la tierra. A ver: creo que lo menos he corrido hacia abajo cuatro
mil millas.
Alicia había aprendido algo de eso en las lecciones que
daba en el colegio, y por más que no fuera aquélla una excelente ocasión para hacer gala de sus conocimientos, ya que
nadie podía oírla, le pareció bien hacer un poco de repaso.
- Sí, ésta es aproximadamente la distancia que he recorrido ... Pero, ¿ hacia qué latitud o longitud me encamino?
N o tenía Alicia la menor idea de 10 que las palabras latitud y longitud significasen, pero se le antojaba que estaba 

bien pronunciar vocablos tan bellos y sonoros. Pronto volvió a decirse:
- j No sé si estoy cayendo a través de la tierra! ¡Qué
divertido sería salir por el otro lado, donde la gente anda de
cabeza! Los "antipáticos"; creo que se llaman así. - Ahora
se alegraba de que nadie la oyera, pues no le sonaba del todo
apropiada esta palabra: - Pero claro, bien tendré que preguntarles el nombre de su país. ¿ Tiene usted la bondad, señora, de decirme si esto es Nueva Zelanda o Australia?
y así pensando, trataba de ensayar cortesías. j Cortesías, al paso que caía por el espacio! ¿ Cómo se las compondría?
- j Pero, qué ignorante iba a parecerle a la señora a quien
hiciera tal pregunta! N o, no es cosa de ir preguntando.
Acaso lo vea escrito en alguna parte.
Abajo, abajo, abajo. No cabía hacer otra cosa. Así P.S
que Alicia pronto reanudó el monólogo:
- Creo que Dina me echará mucho de menos esta noche. - Dina era la gata. - Me figuro que no se olvidarán
de ponerle su platito de leche a la hora de merendar. j Dina,
querida mía, quisiera tenerte aquí, a mi lado! Es verdad que
en el aire no hay ratones, pero podrías cazar algún murciélago, pues los murciélagos, ¿ sabes?, se parecen mucho a los
ratones. Ahora, que yo no sé si a los gatos les gustan los
murciélagos - y a esto, Alicia empezó a adormecerse de una
manera extraña, repitiéndose: - ¿ Comen murciélagos los
gatos? ¿ Comen murciélagos los gatos? - Y a veces se equivocaba: - ¿ Comen gatos los murciélagos?, - porque, ¿ comprendéis?, como no podía contestarse, no importaba alterar
así la pregunta. Empezaba a sentir que se dormía de veras,
soñando que paseaba de la mano con Dina, e iba diciéndose 

con viva impaciencia: - Ahora, Dina, dime la verdad:
¿,Has comido alguna vez murciélago?, - cuando de pronto,
i cataplún!, fué a dar sobre un montón de ramas y hojas
secas, donde terminó su viaje por el espacio.
- No sufrió el menor daño y se puso en pie de un salto.
Miró hacia arriba, peo allá todo estaba obscuro; en frente de ella
se extendía otro largo
pasillo, y el Conejo
Blanco corría hacia
abajo aun al alcance
de su vista. No había
momento que perder;
siguió por Allí Alicia
ligera como el viento,
y aun llegó a tiempo
de oírle decir al volver una esquma:
- j Válganme mis
orejas y mis bigotes, y
qué tarde se me hace!
Alicia llegó casi al mismo tiempo que el Conejo a la esquina, pero él dobló rápidamente y ella le perdió de vista. Se
encontró sola en uria larga sala alumbrada por una hilera de
lámparas que colgaban del techo.
Había puertas alrededor, pero todas estaban cerradas, y
después de ir de un lado a otro probando de abrirlas inútilmente, se dirigió al centro de la estancia pensando cómo se
las compondría para salir de allí.
Al pronto se dió cuenta de que había una mesa de tres 

patas toda de cristal macizo. N o había encima de ella más
que una llavecita dorada, y lo primero que se le ocurrió a
Alicia fué pensar que sería de alguna de las puertas de la
sala. i Pero ay!, para unas cerraduras era grande y en otras
iba demasiado holgada, y de ninguna manera pudo abrirlas.
Sin embargo, a la segunda vuelta advirtió que había una
cortinilla ocultando una portezuela que no llegaría a un metro
de altura. Probó allí la llave, y con alegría vió que iba bien.
Alicia abrió la puerta y se encontró con que daba a un
angosto pasadizo, no mucho más ancho que una madriguera.
Se arrodilló para mirar por allí y "ió que al otro lado se
extendía el jardín más delicioso que imaginar pudiera.
j Cuántas ganas tenía de salir de aquella obscura sala e ir a
pasear entre aquel10s macizos de vistosas flores y aquellas
frescas fuentes!, pero apenas podía meter la cabeza por la
abertura.
- Aun suponiendo que mi cabeza pasara - pensó la
pobre Alicia - de poco me serviría, separada de los hombros. i Oh cuánto me gustaría poderme encoger como un telescopio! Y creo que podría hacerlo con sólo saber como empezar. - Porque hay que comprender que habían comenzado
a pasar cosas tan extraordinarias que Alicia ya creía que
apenas había nada en realidad imposible.
Poco sacaría de estar esperando delante de aquella abertura; así es que volvió a la mesa de cristal con la confianza de
encontrar otra llave, o, en último caso, algún manual de reglas para enseñar a la gente a encogerse como los telescopios.
Lo que encontró esta vez fué una botellita, que, "en verdad,
no estaba la vez anterior", se dijo Alicia, y que llevaba atado al cuello un marbete con la palabra "Bébeme", lindamente impresa en grandes caracteres. 

Estaba muy bien decir "Bébeme", pero la pequeña y prudente Alicia no 10 haría con precipitación:
- N o, antes la quiero examinar bien; no sea que tenga la
indicación de veneno, - pues muchas veces había leído historias de niños que se hablan quemado y que habían sido devorados por las fieras, y cosas por el estilo, todo por olvidarse de las indicaciones sencillas que las personas que les querían les-habían hecho, tales como
que el tener demasiado rato cogido el hurgón de
la chimenea, quema la mano, y
que al hacerse uno
un corte hondo en un dedo, sale sangre, y 10 que ella nunca olvidaba, o sea que el beber el contenido de una botella
que lleve la indicación de veneno suele, a la corta o a la larga,
hacer daño.
Aquella botella, no obstante, no tenía tal indicación, por
lo cual Alicia se aventuró a probar su contenido; y encontrándolo en verdad delicioso, pues tenía una especie de mezcla de aromas de tarta de cerezas, de almíbar, de piña, de
pavo asado, de caramelo y de empanada caliente de manteca,
se lo acabó en un momento.
- i Qué sensación más extraña! - exclamó Alicia. - Me
debo de estar encogiendo como un telescopio. 

y así era, en efecto; ya no tenía más que veinticinco centímetros, y se le encendieron los ojos de alegría pensando que
tendría la medida necesaria para pasar por la abertura del
jardín. De todos modos esperó un poco para convencerse de
que no se encogía ya más; y se sintió inquieta al pensarlo:
- porque podría suceder - se decía Alicia para sus adentros, como comprenderéis - que llegara a acabarme del todo
como una bujía. Y la verdad, no sé lo que sería de mí, entonces. - -Y se esforzaba por imaginarse cómo es la llama de una
bujía cuando se apaga, y no 'recordaba paberlo visto nunca.
Al cabo de un rato, viendo que no pasaba nada más, decidió salir en seguida al jardín. j Pero, ah, pobre Alicia!, al
llegar a la puerta se encontró con que se había olvidado de
la Ilavecita dorada, y al volver a buscarla vió que ya no alcanzaba a cogerla de la mesa: La veía claramente a través
del cristal e hizo cuanto pudo por encaramarse por una de
las patas, pero era muy resbaladiza, y cuando se rindió de tal
esfuerzo, la pobrecilla sentóse en el suelo y se echó a llorar.
- i Vamos, que no sacarás nada llorando de esta manera!
- se dijo Alicia a sí misma con un poco de dureza. - Te
aconsejo que dejes de llorar al puntó. - Con frecuencia se
daba excelentes consejos, que casi nunca seguía, y a veces
negaba a reprenderse con tal severidad que se arrancaba
lágrimas de los ojos; hasta se acordaba de que una vez se quiso tirar de las orejas por haberse engañado en una partida de
croquet que había estado jugando ella sola, pues esta curiosa criatura era muy dada a fingir desdoblamiento de su
persona, creyendo ser dos en vez de una. - Pero de nada
me servirá ahora - pensaba la pobre Alicia - pretender ser
dos personas, cuando j bastante tengo con hacerme una sola
persona respetable! 

Pronto su mirada se fijó en una cajita de cristal que había
debajo de la mesa; la abrió y encontró dentro una diminuta
torta en la que se leía esta palabra: "Cómeme", que estaba
trazada con almíbar de grosella.
- Bien, me la comeré - se dijo Alicia - y si me hace
crecer otra vez, podré coger la llave, y si me hace más pequeña todavía, pasaré por debajo de la puerta; así, de todas
maneras, entraré en el jardín, y pase lo que Dios quiera.
Dió un mordisquito y se preguntó llena de ansiedad:
- ¿ Hacia dónde?, ¿ hacia dónde?, y se llevaba la mano a
la cabeza para comprobar si crecía o disminuía, pero le sorprendió mucho encontrarse con que continuaba de la misma
manera. Esto es, en efecto, 10 que pasa siempre que se come
torta, pero Alicia se había acostumbrado de tal manera a no
eSllerar más que cosas extraordinarias, que le pareció muy
triste y estúpido seguir viviendo de una manera normal.
Así es que tomó una decisión y en un momento se comió
toda la torta. 
    """
    if not large_text_content.strip():
        print("El contenido del documento está vacío. Añade texto a `large_text_content`.")
        return

    document_id = "test_document_large_001"
    document_metadata = {
        "source": "manual_input_document_test.txt",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "category": "research_paper_test"
    }

    # Borrar datos anteriores de este document_id (opcional, para pruebas limpias)
    # Esto es más complejo porque tendrías que interactuar con cada store (KV, VectorDB, BM25)
    # Por ahora, asumimos que el working_dir se limpia manualmente si es necesario para pruebas repetidas.
    
    logger.info(f"Iniciando procesamiento de documento largo: {document_id}")
    await process_large_text(ares_core, document_id, large_text_content, document_metadata)
    logger.info("Procesamiento de documento largo finalizado.")

    # --- Bucle de Chat (Opcional para esta prueba) ---
    # Puedes añadir un bucle de chat aquí si quieres interactuar con el documento ingestado.
    # Copia la lógica del bucle de chat de ares/main.py si es necesario.
    # Por ahora, nos enfocaremos en la ingesta.
    print("\n--- Documento ingestado. Puedes ejecutar ares.main normalmente para chatear ---")
    print("--- o añadir un bucle de chat aquí para probar inmediatamente. ---")


    # --- Cerrar ARES ---
    if ares_core:
        await ares_core.close()
    print("\n" + "="*17 + " Prueba ARES Cerrada " + "="*17)

if __name__ == "__main__":
    # Asegurar que el encoder de tiktoken se inicialice si es necesario globalmente
    # Esto ya se hace en ares.main, pero si ejecutas este script directamente, podría ser útil
    try:
        from ares.utils.helpers import _init_encoder
        asyncio.run(_init_encoder()) 
    except Exception as encoder_init_error:
        logger.error(f"Error inicializando el encoder de tiktoken globalmente: {encoder_init_error}")

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nInterrupción. Saliendo.")
    except Exception as e:
        print(f"\nERROR INESPERADO EN main_document_test: {e}")
        logger.exception("Error fatal no capturado en prueba de documento.")


---
File: /main_document_test.py
---

import asyncio
import os
import sys
from typing import List, Dict, Tuple, Optional, Any
import time
from datetime import datetime, timezone
from memory_profiler import profile # <--- AÑADIR ESTO

# --- Inicio de la corrección sys.path ---
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root_parent = os.path.dirname(script_dir)
if project_root_parent not in sys.path:
     sys.path.insert(0, project_root_parent)
# --- Fin de la corrección sys.path ---

from ares.core.orchestrator import AresCore
from ares.utils.logging_setup import logger, setup_logging # Importar setup_logging también
from ares.core.config_loader import get_config_value # Para working_dir

# --- Configuración para esta prueba ---
# Asegúrate que el config_path aquí sea el mismo que usarías para `python -m ares.main`
# o define uno específico si es necesario.
CONFIG_PATH: Optional[str] = "./ares/configs/ares_baseline_test_config.toml" 

#CONFIG_PATH: Optional[str] = None # Usa el default_config.toml de AresCore
# ------------------------------------

async def process_large_text(ares_core: AresCore, document_id: str, large_text_content: str, document_metadata: Dict):
    """
    Procesa un texto largo dividiéndolo en "ventanas" si excede un límite,
    y luego ingesta cada ventana.
    """
    # Un límite arbitrario para simular la división en ventanas.
    # En un sistema real, esto podría ser el context_window del LLM de descripción.
    # O, para Late Chunking puro, podría ser el max_length del embedder si no se quieren
    # embeddings que crucen "documentos" muy grandes.
    # Por ahora, usemos un límite de tokens para el texto de la ventana.
    # Jina V3 maneja hasta 8192 tokens. Podríamos usar un valor un poco menor.
    MAX_WINDOW_TOKENS = get_config_value(ares_core.config, "ingestion.max_window_tokens_for_description", 7800)
    
    # Usar el tokenizer del embedder para un conteo de tokens más preciso
    # (asumiendo que ares_core.embedder.tokenizer está disponible)
    try:
        tokenizer = ares_core.embedder.tokenizer
        def count_tokens_func(text):
            return len(tokenizer.encode(text, add_special_tokens=False))
    except Exception:
        logger.warning("No se pudo acceder al tokenizer del embedder, usando split() para contar tokens en main_document_test.")
        def count_tokens_func(text):
            return len(text.split())


    current_token_count = 0
    current_window_text = ""
    window_counter = 0
    total_ingested_successfully = 0
    total_failed = 0

    # Simular división por párrafos o una heurística simple para no cortar a mitad de palabra
    # En la práctica, Chonkie o un pre-chunker a nivel de documento se encargaría de esto.
    # Aquí, vamos a simplificar y asumir que `large_text_content` es una sola gran cadena.
    # Y la dividiremos basada en conteo de tokens.

    words = large_text_content.split(' ') # División simple por palabras
    
    for word_idx, word in enumerate(words):
        word_with_space = word + " "
        word_token_count = count_tokens_func(word_with_space)

        if current_token_count + word_token_count > MAX_WINDOW_TOKENS and current_window_text:
            # Procesar la ventana actual
            window_counter += 1
            window_id = f"{document_id}_win_{window_counter}"
            logger.info(f"Procesando ventana '{window_id}' (tokens: {current_token_count})")
            success = await ares_core.aingest(window_id, current_window_text.strip(), document_metadata)
            if success:
                total_ingested_successfully += 1
            else:
                total_failed +=1
            
            # Reset para la nueva ventana
            current_window_text = word_with_space
            current_token_count = word_token_count
        else:
            current_window_text += word_with_space
            current_token_count += word_token_count

        # Asegurar que la última parte del texto también se procese
        if word_idx == len(words) - 1 and current_window_text.strip():
            window_counter += 1
            window_id = f"{document_id}_win_{window_counter}"
            logger.info(f"Procesando última ventana '{window_id}' (tokens: {current_token_count})")
            success = await ares_core.aingest(window_id, current_window_text.strip(), document_metadata)
            if success:
                total_ingested_successfully += 1
            else:
                total_failed +=1
                
    logger.info(f"Procesamiento del documento '{document_id}' completado. Ventanas exitosas: {total_ingested_successfully}, Fallidas: {total_failed}")

@profile 
async def main():
    # --- Configuración de Logging Específica para esta Prueba (opcional) ---
    # Si quieres logs separados o niveles diferentes para esta prueba.
    # Por defecto, usará la configuración de AresCore.
    # setup_logging(log_level_str="DEBUG", log_file="./ares_data_doc_test/doc_test.log")

    print("\n" + "="*15 + " Iniciando Prueba de Documento ARES " + "="*15)
    ares_core: Optional[AresCore] = None
    try:
        ares_core = AresCore(config_path=CONFIG_PATH)
        logger.info("AresCore inicializado para prueba de documento.")
    except Exception as e:
        print(f"\nERROR CRÍTICO: No se pudo inicializar AresCore.")
        logger.exception("Fallo en inicialización de AresCore para prueba de documento.")
        return

    # --- Contenido del Documento ---
    # Pega aquí el texto largo que quieres probar.
    # Puedes leerlo de un archivo si prefieres:
    # with open("ruta/a/tu/documento_largo.txt", "r", encoding="utf-8") as f:
    #     large_text_content = f.read()
    
    large_text_content = """
    Max Stirner.
El Único y su propiedad.
INTRODUCCIÓN.
¿Qué causa es la que debo defender? Antes que nada la buena causa, la causa de Dios, de la verdad, de la libertad, de la
humanidad, de la justicia; luego la de mi pueblo, la de mi gobernante, la de mi patria; más tarde será la del Espíritu4 y miles más
después. Únicamente mi causa no puede ser nunca mi causa.
“Vergüenza del egoísta que no piensa más que en sí mismo”.
¿Pero esos cuyos intereses son sagrados, esos por quienes debemos
trabajar, sacrifi carnos y entusiasmarnos, cómo entienden su causa?
Ustedes que saben de Dios tantas y tan profundas cosas; ustedes que durante siglos “exploraron las profundidades de la divinidad” y penetraron con sus miradas hasta lo profundo de su
corazón, ¿pueden decirme cómo entiende Dios la “causa divina”
que debemos servir nosotros? Y ya que tampoco nos ocultan los
designios del Señor. ¿Qué quiere? ¿Qué persigue? ¿Abrazó, como
a nosotros se nos pide, una causa ajena y se ha hecho el campeón
de la verdad y del amor? Este absurdo indigna; nos enseñan que
siendo Dios todo amor y toda verdad, las causas del amor y de la
verdad se confunden con la suya y le son consustanciales. Les repugna admitir que Dios pueda, como nosotros, hacer suya la causa de otro. “¿Pero abrazaría Dios la causa de la verdad si no fuera
la suya?” Dios no se ocupa más que de su causa, porque al ser él
todo en todo, todo es su causa. Pero nosotros no somos todo en
todo, y nuestra causa es bien mezquina, bien despreciable; por
eso debemos servir a una “causa superior”. Más claro: Dios no se
preocupa más que de lo suyo, no se ocupa más que de sí mismo,
no piensa en nadie más que en sí mismo y no se fi ja más que en sí
mismo; ¡pobre del que contradiga sus mandatos! No sirve a nada
superior y no trata más que de satisfacerse. La causa que defi ende
es únicamente la suya. Dios es un ególatra.

¿Y la humanidad, cuyos intereses debemos defender como
nuestros, qué causa defi ende? ¿La de otro? ¿Una superior? No.
La humanidad no se ve más que a sí misma, la humanidad no
tiene otro objeto que la humanidad; su causa es ella misma. Con
tal que ella se desarrolle no le importa que mueran los individuos y los pueblos; saca de ellos lo que puede sacar, y cuando
han cumplido la tarea que les reclamaba, los echa al cesto de
papeles inservibles de la historia. ¿La causa que defi ende la humanidad, no es puramente egoísta?
Es inútil que siga y demuestre cómo cada una de esas cosas,
Dios, Humanidad, etc., se preocupan sólo de su bien y no del
nuestro. Revisen a los demás y vean por ustedes mismos si la
Verdad, la Libertad, la Justicia, etc., se preocupan de ustedes para
otra cosa que no sea pedirles su entusiasmo y sus servicios.
Que sean servidores dedicados, que les rindan homenaje,
eso es todo lo que les piden. Miren a un pueblo redimido por
nobles patriotas; los patriotas caen en la batalla o revientan
de hambre y de miseria; ¿qué dice el pueblo? ¡Abonado con
sus cadáveres se hace “fl oreciente”!. Mueren los individuos
“por la gran causa del pueblo”, que se conforma con dedicarles alguna que otra lamentable frase de reconocimiento y se
guarda para sí todo el provecho. Eso me parece un egoísmo
demasiado lucrativo.
Pero vean al sultán que cuida tan tiernamente a “los suyos”. ¿No es la imagen de la más pura abnegación, y no es su
vida un constante sacrifi cio? ¡Sí, por “los suyos”! ¿Se quiere
hacer un ensayo? Qué se muestre que no se es “el suyo”, sino
“el tuyo”, que se rechace su egoísmo y será uno perseguido,
encarcelado, torturado. El sultán no basa su causa más que en
sí mismo; es todo en todo, es el único, y no tolera a nadie que
no sea uno de “los suyos”.
¿No les dicen nada estos ejemplos? ¿No les hacen pensar
que un egoísta tiene razón? Yo, al menos, aprendo de ellos, y en
vez de continuar sirviendo con desinterés a esos grandes egoístas, seré yo mismo el egoísta.
Dios y la humanidad no basaron su causa sobre nada, sobre
nada más que ellos mismos. Yo basaré, entonces, mi causa sobre mí; soy, como Dios, la negación de todo lo demás, soy todo
para mí, soy el único.

Si Dios y la Humanidad son poderosos con lo que contienen,
hasta el punto de que para ellos mismos todo está en todo, yo advierto que me falta a mi mucho menos todavía, y que no tengo que
quejarme de mi “futilidad”. Yo no soy nada en el sentido de vacío,
pero soy la nada creadora, la nada de la que saco todo.
¡Fuera entonces toda causa que no sea entera y exclusivamente la mía! Mi causa, me dirán, debería ser, al menos, la
“buena causa”. ¿Qué es lo bueno, qué es lo malo? Yo mismo
soy mi causa, y no soy ni bueno ni malo; esas no son, para mí,
más que palabras.
Lo divino mira a Dios, lo humano mira al hombre. Mi causa
no es divina ni humana, no es ni lo verdadero, ni lo bueno, ni
lo justo, ni lo libre, es lo mío, no es general, sino única, como
yo soy único.
Nada está por encima de mí.

PRIMERA PARTE
El hombre
“El hombre es para el hombre el Ser Supremo para el hombre”,
dice Feuerbach5, “el hombre acaba de ser descubierto”, dice
Bruno Bauer6. Examinemos más de cerca es Ser Supremo y ese
nuevo descubrimiento.

I
LA VIDA DE UN HOMBRE
En cuanto el hombre despierta a la vida, intenta encontrarse
y conquistarse a sí mismo en medio del caos en que rueda confundido con todo lo demás.
Pero el niño sólo puede afi rmar a cada paso su dependencia.
Porque si cada cosa se preocupa por sí misma, al mismo
tiempo, choca con las demás, y la lucha por la autonomía y la
supremacía es inevitable.
Vencer o ser vencido, no hay otra alternativa. El vencedor
será el amo, el vencido será el esclavo. Uno disfrutará de la soberanía y de los “derechos del señor” y el otro cumplirá con
respeto sus “deberes de súbdito”.
Pero los enemigos no bajan las armas; cada uno permanece al
acecho, espiando las debilidades de los otros, los hijos la de los padres, los padres las de los hijos, el que no da los golpes los recibe.
Para alcanzar la liberación, en la infancia tratamos de penetrar las cosas o de “ir detrás” de ellas; para eso espiamos
su parte débil (en esto los niños tienen un instinto que no los
engaña); nos gusta romper lo que encontramos a mano, revisar los rincones prohibidos, explorar todo lo que se oculta a
nuestras miradas; ensayamos nuestras fuerzas. Y, descubierto
al fi n el secreto, nos sentimos seguros de nosotros mismos; y si,
por ejemplo, llegamos a convencernos de que el palo no puede
contra nuestra obstinación, ya no le tenemos miedo, lo hemos
superado. ¡Detrás de los golpes se levantan, más poderosos que
ellos, nuestra audacia y nuestra obstinada libertad! Nos deslizamos poco a poco a través de todo lo inquietante, a través
de la fuerza temible del látigo, a través del rostro enojado de
nuestro padre, y detrás de todo descubrimos nuestra ataraxia7 y
ya nada nos incomoda, ya nada nos espanta; tenemos conciencia de nuestro poder de resistir y vencer, descubrimos que nada
puede violentarnos. Lo que nos inspiraba miedo y respeto, en
lugar de intimidarnos nos alienta: detrás del rudo mandato de
los superiores y de los padres, se levanta más obstinada nuestra
voluntad, más rápida nuestra astucia. Cuando más aprendemos
a conocernos, más nos reímos de lo que considerábamos insuperable. Pero, ¿qué son nuestra destreza, nuestro valor, nuestra
audacia, sino el espíritu?
Durante largo tiempo escapamos a una lucha cansadora y
triste: la lucha contra la razón. Lo mejor de la infancia pasa sin
que tengamos que pelear contra la razón. No nos preocupamos
de ella, no nos mezclamos con ella, ni admitimos ninguna razón.
Que nos quieran convencer nos parece entonces un absurdo:
somos sordos a las buenas razones y a los argumentos sólidos,
reaccionamos solamente a las caricias y los castigos.
Más tarde comienza el difícil combate contra la razón y con
él se abre una nueva fase de nuestra vida. De niños habíamos
vivido sin meditar.
Con el espíritu nos conocemos a nosotros mismos, y él es
el primer nombre bajo el cual des-divinizamos lo divino, es
decir, el objeto de nuestras inquietudes, nuestros fantasmas,
“los poderes superiores”. Nada se impone desde entonces a
nuestro respeto; estamos llenos del juvenil sentimiento de
nuestra fuerza, y el mundo pierde ante nuestros ojos todo
interés, porque nos sentimos superiores a él, nos sentimos
espíritu. Notamos que, hasta entonces, habíamos mirado al
mundo sin verlo, que nos lo habíamos contemplado nunca
con los ojos del espíritu.
Ensayamos sobre las fuerzas de la naturaleza nuestras primeras fuerzas. Los padres se nos imponen como fuerzas naturales; más tarde pensamos que se debe abandonar padre y
madre para romper todo poder natural. Llega un día en que
el lazo se rompe. Para el hombre que piensa, es decir para el
hombre “espiritual”, la familia no es un poder natural y debe
renunciar a los padres, los hermanos, etc. Si éstos “renacen”
en lo sucesivo como potencias espirituales y racionales, esas
potencias nuevas no son, de ningún modo, lo que eran antes.
Y no sólo es el yugo de los padres lo que se sacude el joven,
es toda autoridad humana; los hombres ya no son un obstáculo
ante el que es preciso detenerse, porque “hay que obedecer a
Dios antes que a los hombres”.

El nuevo punto de vista en que se coloca es el “celestial”, y
visto desde esa altura, todo lo “terrestre” retrocede, se empequeñece y se borra en una lejana bruma de desprecio.
De ahí el cambio radical en la orientación intelectual del joven y el cuidado exclusivo de lo espiritual, en tanto que el niño,
que no se sentía aún espíritu, quedaba limitado a la letra de los
libros. El joven ya no se adhiere a las cosas, sino que procura
aprehender los pensamientos que esas cosas encubren; así, por
ejemplo, cesa de acumular confusamente en su cabeza los hechos y las fechas de la historia, para esforzarse en penetrar en
su espíritu; el niño, por el contrario, aunque comprenda bien
el encadenamiento de los hechos, es incapaz de sacar de ellos
ideas, espíritu; y amontona los conocimientos que adquiere sin
seguir un plan a priori, sin sujetarse a un método teórico; en
resumen, sin buscar ideas.
En la niñez tenía que superar las leyes del mundo; ahora, ante cualquier cosa que se proponga, choca con una objeción del espíritu. “¡Esto no es razonable, no es cristiano, no
es patriótico!” nos grita la conciencia; y nos abstenemos. No
tememos al poder vengador de las Euménides, ni a la cólera
de Poseidón, ni a Dios que ve las cosas ocultas, ni al castigo
paterno: tememos a la Conciencia.
Somos, desde entonces, “los servidores de nuestros pensamientos”; obedecemos sus órdenes como en otros tiempos obedecimos las paternas o las de otros hombres. Son ellas (ideas,
representaciones, creencias) las que reemplazan a los mandatos
paternos y las que gobiernan nuestra vida. De niños ya pensábamos; pero nuestros pensamientos entonces no eran incorpóreos,
abstractos, absolutos, no eran nada más que un puro mundo de
pensamientos; no eran pensamientos lógicos.
Sólo teníamos los pensamientos que nos inspiraban los acontecimientos y las cosas; pensábamos que una cosa determinada
era de tal o cual naturaleza. Pensábamos que “es Dios quien
ha creado este mundo que vemos”; pero nuestro pensamiento
no iba más lejos, no “escrutábamos las profundidades mismas
de la divinidad”. Decíamos “esto es verdadero, esto es la verdad”; pero sin indagar lo verdadero en sí, la verdad en sí, sin
preguntarnos si “Dios es la verdad”. Poco nos importaban “las
profundidades de la divinidad”, ni cuál fuese la “verdad”. Pilato

no se detiene en cuestiones de pura lógica (o, en otros términos,
de pura teología) como “¿qué es la verdad?”, y sin embargo,
llegada la ocasión, no vacila en distinguir “lo que hay de verdadero y lo que hay de falso en un asunto”, es decir, si tal cosa
particular es verdadera.
Todo pensamiento inseparable de un objeto no es todavía
más que un pensamiento absoluto.
No hay para el joven mayor placer que descubrir y apropiarse del pensamiento puro; la verdad, la libertad, la humanidad,
el hombre, etc.; esos astros brillantes que alumbran el mundo de
las ideas, iluminan y exaltan las almas juveniles.
Pero una vez reconocido el espíritu como esencial, aparece
una diferencia: el espíritu puede ser rico o pobre; y nos esforzamos, entonces, en hacernos ricos de espíritu; el espíritu quiere
extenderse, fundar su reino, reino que no es de este mundo, sino
de mucho más allá; así aspira a resumir en sí todo espiritualismo. Aún espíritu como soy, no soy espíritu perfecto y debo
empezar por buscar ese espíritu perfecto.
Yo, que hace no mucho me había reconocido en el espíritu, me
pierdo de nuevo, penetrado por mi inanidad, me humillo ante el espíritu perfecto, reconociendo que no está en mí, sino más allá de mí.
Todo depende del espíritu; ¿pero todo espíritu es “correcto”?
El espíritu bueno y verdadero es el ideal de espíritu, el “Espíritu
Santo”. No es ni el mío ni el tuyo, es un espíritu ideal, superior:
es “Dios”. “Dios es el espíritu”. Y “¿cuánto más vuestro Padre
celestial dará el Espíritu Santo a los que se lo pidan?”8
.
El hombre ya maduro difi ere del joven en que toma el mundo como es, sin ver por todas partes males que corregir, entuertos que enderezar, y sin pretender moldearlo a su ideal. En él se
fortifi ca la opinión de que uno debe obrar en el mundo, según
su interés y no según su ideal.
En tanto que uno no ve en sí mas que el espíritu y pone todo
su mérito en ser espíritu, no cuesta nada arriesgar la vida, lo “corporal”, por una insignifi cancia de amor propio; y como no tiene
uno más que pensamientos, ideas que espera ver realizadas algún
día, cuando haya encontrado su camino, hallado una salida a su
actividad, esos pensamientos, esas ideas permanecen provisionalmente incumplidas, irrealizadas: no se tiene más que un ideal.

Pero desde que se ama el “pellejo” (lo que sucede ordinariamente en la edad madura) y se experimenta placer en ser tal
como uno es, con vivir su vida, cesa de perseguir el ideal para
apegarse a un interés personal, egoísta, es decir, a un interés no
sólo del espíritu, sino a la satisfacción de todo el individuo, y
el interés viene a ser, desde entonces, verdaderamente interesado. Comparen, entonces, al hombre maduro con el hombre
joven. ¿No les parece más duro, más egoísta, menos generoso?
¡Sin duda! ¿Es por eso más malo? No; lo que pasa es que se
ha hecho más positivo, más “práctico”. El punto principal es
que hace de sí el centro de todo, cosa que no hace el joven,
distraído por un montón de cosas que no son él: dios, la patria
y otros pretextos de “entusiasmo”.
El hombre se descubre así por segunda vez. El joven había
advertido su espiritualidad y se había extraviado después en
la investigación del espíritu universal y perfecto, del Espíritu
Santo, del Hombre, de la Humanidad, en una palabra, de todos
los ideales. El hombre se recobra y vuelve a encontrar su espíritu
encarnado en él, hecho carne.
Un niño no desea ni ideas ni pensamientos; un joven no persigue más que intereses espirituales; los intereses del hombre, en
cambio, son materiales, personales y egoístas.
Cuando el niño no tiene ningún objeto en que ocuparse, se aburre, porque no sabe todavía ocuparse de sí mismo. El joven, al contrario, se cansa pronto de los objetos, porque de esos objetos salen
para él pensamientos, y él, ante todo, se interesa en los sueños, en lo
que espiritualmente le ocupa: “su espíritu está ocupado”.
En todo lo que no es espiritual el joven no ve más que
“futilidades” Si se le ocurre tomar en serio las más insignifi cantes niñerías (por ejemplo, las ceremonias de la vida
universitaria), es porque se apoderan de su espíritu, es decir,
porque ve en ellas símbolos.
Yo me he colocado detrás de las cosas, y he descubierto mi
espíritu; igualmente más tarde me encuentro detrás de los pensamientos, y me siento su creador y su poseedor. A la edad de
las visiones, mis pensamientos proyectaban sombra sobre mi cerebro, como el árbol sobre el suelo que lo nutre; se cernían a mí
alrededor ensueños febriles, y me estremecían con su espantoso poder. Los pensamientos mismos se habían revestido de una 
forma corporal, y si veía a esos fantasmas, se llamaban Dios, el
Emperador, el Papa, la Patria, etc. Hoy destruyo esas vaguedades engañosas, entro en posesión de mis pensamientos, y digo:
Yo sólo tengo un cuerpo y soy alguien. No veo ya en el mundo
más que lo que él es para mí; es mío, es mi propiedad. Yo lo
refi ero todo a mí. No hace mucho era espíritu, y el mundo era
a mis ojos digno sólo de desprecio; hoy soy yo, soy propietario,
y rechazo esos espíritus o esas ideas cuya vanidad he medido.
Todo eso no tiene sobre mí más poder que las “fuerzas de la
tierra” tienen sobre el espíritu.
El niño era realista, ocupado por las cosas de este mundo,
hasta que llegó poco a poco a penetrarlas. El joven es idealista, ocupado en sus pensamientos, hasta el día en que llegará a
ser hombre egoísta que no persigue a través de las cosas y de
los pensamientos más que la felicidad de su corazón, y pone
por encima de todo su interés personal. En cuanto al anciano…
cuando yo lo sea… tendré tiempo de hablar de él.
    """
    if not large_text_content.strip():
        print("El contenido del documento está vacío. Añade texto a `large_text_content`.")
        return

    document_id = "test_document_large_001"
    document_metadata = {
        "source": "manual_input_document_test.txt",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "category": "research_paper_test"
    }

    # Borrar datos anteriores de este document_id (opcional, para pruebas limpias)
    # Esto es más complejo porque tendrías que interactuar con cada store (KV, VectorDB, BM25)
    # Por ahora, asumimos que el working_dir se limpia manualmente si es necesario para pruebas repetidas.
    
    logger.info(f"Iniciando procesamiento de documento largo: {document_id}")
    await process_large_text(ares_core, document_id, large_text_content, document_metadata)
    logger.info("Procesamiento de documento largo finalizado.")

    # --- Bucle de Chat (Opcional para esta prueba) ---
    # Puedes añadir un bucle de chat aquí si quieres interactuar con el documento ingestado.
    # Copia la lógica del bucle de chat de ares/main.py si es necesario.
    # Por ahora, nos enfocaremos en la ingesta.
    print("\n--- Documento ingestado. Puedes ejecutar ares.main normalmente para chatear ---")
    print("--- o añadir un bucle de chat aquí para probar inmediatamente. ---")


    # --- Cerrar ARES ---
    if ares_core:
        await ares_core.close()
    print("\n" + "="*17 + " Prueba ARES Cerrada " + "="*17)

if __name__ == "__main__":
    # Asegurar que el encoder de tiktoken se inicialice si es necesario globalmente
    # Esto ya se hace en ares.main, pero si ejecutas este script directamente, podría ser útil
    try:
        asyncio.run(main())
        from ares.utils.helpers import _init_encoder
        asyncio.run(_init_encoder()) 
    except Exception as encoder_init_error:
        logger.error(f"Error inicializando el encoder de tiktoken globalmente: {encoder_init_error}")

    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nInterrupción. Saliendo.")
    except Exception as e:
        print(f"\nERROR INESPERADO EN main_document_test: {e}")
        logger.exception("Error fatal no capturado en prueba de documento.")


---
File: /main.py
---

# ares/main.py

import asyncio
import os
import sys
import time
from typing import List, Dict, Optional, Tuple, Any
from datetime import datetime, timezone
import shutil

# --- Bloque de corrección de sys.path (esencial) ---
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(script_dir)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from ares.core.orchestrator import AresCore
from ares.utils.logging_setup import logger
from ares.ingestion_plugins.fast_pdf_parser import FastPDFParser
from ares.core.config_loader import get_config_value

# --- CONFIGURACIÓN PARA LA PRUEBA FINAL ---
# --------------------------------------------------------------------------
# 1. Ruta al archivo de configuración de ARES que se usará.
CONFIG_FILENAME = "ares_vllm_config_full_test.toml"
ARES_CONFIG_PATH: Optional[str] = os.path.join(project_root, "ares", "configs", CONFIG_FILENAME)

# 2. Ruta al documento que desea ingestar.
INITIAL_DOCUMENT_PATH = "/home/moony/Documentos/La filosofía de la redención (Philipp Mainländer) (Z-Library).pdf"

# 3. Un ID único para este documento.
DOCUMENT_ID = "filosofia_redencion_mainlander_v1"

# 4. El tipo de parser a usar ('pdf' para PyMuPDF, 'text' para .txt).
PARSER_TYPE = "pdf"

# 5. [OPCIONAL] Poner en `True` para borrar el directorio de trabajo antes
#    de cada ejecución. Útil para pruebas limpias. ¡CUIDADO, BORRA DATOS!
CLEAN_WORKING_DIR_BEFORE_RUN = True
# --------------------------------------------------------------------------


async def ingest_document(ares_core: AresCore, doc_path: str, doc_id: str, parser_type: str):
    """Parsea e ingesta un único documento, midiendo el tiempo."""
    if not os.path.exists(doc_path):
        logger.error(f"El archivo del documento no se encuentra en: {doc_path}")
        print(f"ERROR: El documento en '{doc_path}' no existe. Por favor, verifique la ruta.")
        return False

    logger.info(f"Iniciando ingesta de '{doc_path}' con parser '{parser_type}'...")
    ingest_start_time = time.perf_counter()

    try:
        if parser_type == "pdf":
            parser = FastPDFParser()
            text_content, metadata = await asyncio.to_thread(parser.parse, doc_path)
        elif parser_type == "text":
            with open(doc_path, "r", encoding="utf-8") as f:
                text_content = f.read()
            metadata = {"source_file": os.path.basename(doc_path)}
        else:
            logger.error(f"Tipo de parser no reconocido: {parser_type}")
            return False

        if not text_content.strip():
            logger.error("El parser no pudo extraer texto del documento.")
            return False

        windows_to_process = [(f"{doc_id}_win_0", text_content, {**metadata, "document_id": doc_id})]
        
        await ares_core.ingestion_pipeline.process_windows_batch(windows_to_process)
        duration = time.perf_counter() - ingest_start_time
        
        logger.info(f"INGESTA COMPLETA. Documento '{doc_id}' procesado en {duration:.2f} segundos.")
        print(f"\n--- Ingesta del documento completada en {duration:.2f}s ---")
        return True

    except Exception as e:
        logger.exception(f"Fallo el proceso de ingesta para el documento '{doc_id}'.")
        return False


async def chat_loop(ares_core: AresCore):
    """Bucle principal de chat interactivo con el usuario."""
    print("\n" + "="*20 + " Chat Interactivo con ARES " + "="*20)
    print("El conocimiento del documento ha sido cargado. ¡Puedes empezar a preguntar!")
    print("Escribe tu consulta o 'exit' para salir.")
    
    conversation_history: List[Dict[str, str]] = []

    while True:
        try:
            user_input = await asyncio.to_thread(input, "\nTú: ")
        except (EOFError, KeyboardInterrupt):
            print("\nSaliendo del chat...")
            break

        if user_input.lower().strip() == 'exit':
            break
        if not user_input.strip():
            continue

        recent_history = conversation_history[-10:]
        try:
            response = await ares_core.aquery(user_input, recent_history)
            print(f"\nARES: {response}")

            if "[Error" not in response:
                conversation_history.extend([
                    {"role": "user", "content": user_input},
                    {"role": "assistant", "content": response}
                ])
                if len(conversation_history) > 100:
                    conversation_history = conversation_history[-100:]
        except Exception as e:
            logger.exception("Error crítico durante la ejecución de aquery en el bucle de chat.")
            print(f"\nARES: Lo siento, ha ocurrido un error grave al procesar tu solicitud.")


async def main():
    """Función principal que inicializa ARES, realiza la ingesta y comienza el chat."""
    print("\n" + "="*25 + " Iniciando ARES " + "="*25)
    
    # Limpieza opcional del directorio de trabajo
    if CLEAN_WORKING_DIR_BEFORE_RUN:
        temp_config = {}
        if ARES_CONFIG_PATH and os.path.exists(ARES_CONFIG_PATH):
            from ares.core.config_loader import load_config
            temp_config = load_config(ARES_CONFIG_PATH)
        
        working_dir = get_config_value(temp_config, "general.working_dir", "./ares_data_full_test")
        if os.path.exists(working_dir):
            print(f"ADVERTENCIA: Limpiando directorio de trabajo '{working_dir}'...")
            try:
                shutil.rmtree(working_dir)
                print("Directorio limpiado.")
            except Exception as e:
                print(f"Error limpiando el directorio: {e}")
                return # Salir si la limpieza falla y era requerida
        os.makedirs(working_dir, exist_ok=True)

    ares_core: Optional[AresCore] = None
    try:
        ares_core = AresCore(config_path=ARES_CONFIG_PATH)
        await ares_core.start_services()

        ingestion_success = await ingest_document(ares_core, INITIAL_DOCUMENT_PATH, DOCUMENT_ID, PARSER_TYPE)

        if ingestion_success:
            await chat_loop(ares_core)
        else:
            print("\nLa ingesta del documento falló. El chat no puede continuar. Revise los logs.")

    except Exception as e:
        logger.exception("Error fatal no capturado en la función main.")
        print(f"ERROR INESPERADO: {e}")
    finally:
        if ares_core:
            await ares_core.close()
    print("\n" + "="*25 + " ARES Finalizado " + "="*25)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nPrograma interrumpido por el usuario. Adiós.")


---
File: /requirements_api.txt
---

# Embeddings/Reranker/LLM
openai>=1.0.0
anthropic>=0.20.0
cohere>=5.0.0
# Vector DB
pinecone-client>=3.0.0
# KG
neo4j>=5.10.0
# Otros
# ...


---
File: /requirements_general.txt
---

# ==============================================================================
# ARES - Local Execution Requirements
# ==============================================================================
#
# Contiene todas las dependencias para ejecutar ARES con modelos y bases de
# datos locales (en CPU o GPU). Ideal para desarrollo offline o en
# hardware personal.

# --- Core Framework & Configuration ---
# Para leer archivos de configuración .toml
toml
# Para manejo de datos numéricos, esencial para embeddings
numpy
# Para la funcionalidad base del Knowledge Graph (incluso si está desactivado)
networkx

# --- Asynchronous Operations ---
# aiohttp es usado por algunas librerías de cliente de forma interna
aiohttp
# httpx es un cliente HTTP moderno usado por openai y otros clientes API
httpx

# --- Chunking & Text Splitting ---
# Librería principal para chunking semántico
chonkie[all]
# Tokenizer de OpenAI, usado por muchas librerías y como fallback
tiktoken

# --- Local Storage ---
# Para el KVStore por defecto (chunks, metadatos, caché)
# sqlite3 viene con Python, no es necesario listarlo.
# Para el VectorDB local por defecto
qdrant-client
# Para la búsqueda lexical (BM25) si no se usan los sparse vectors de Qdrant
# (El código antiguo `bm25.py` lo usaba, el nuevo usa FastEmbed/Qdrant)
# whoosh>=2.7.0

# --- LLM / Embeddings / Reranking (Local Execution) ---
# El corazón de la ejecución local de modelos
# Interfaz principal para modelos de Hugging Face (Jina, Qwen, etc.)
transformers
# Necesario para los tokenizers de muchos modelos (Llama, T5, etc.)
sentencepiece
# A menudo requerido por modelos en Hugging Face
protobuf
# Para la búsqueda lexical eficiente (BM25/SPLADE) a través de Qdrant
fastembed

# --- Local LLM API Clients (Opcional, pero común en ARES) ---
# Cliente oficial para interactuar con Ollama
#ollama
# Usado por el LMStudioWrapper para comunicarse con su API compatible con OpenAI
# La librería de 'openai' se lista abajo, ya que también sirve para la API real.
# 'requests' es una dependencia transitiva de muchas de estas librerías.

# --- LLM APIs (Aunque es local, LMStudio usa la interfaz de OpenAI) ---
# Necesario para el LMStudioWrapper y si se quiere usar la API de OpenAI
openai

# --- CLI & Utilities ---
# Para la interfaz de línea de comandos de los scripts de ejemplo
#click
# Utilidades de Pydantic para validación de datos (usado en prompts)
#pydantic
# Para reparar JSON malformado devuelto por LLMs
#json_repair


---
File: /requirements_local.txt
---

# KV/Description Store
sqlite3 # Viene con Python
duckdb # Opcional
# Vector DB
qdrant-client>=1.7.0 # O la versión que necesites
chromadb>=0.4.22 # Opcional
# KG
# NetworkX ya está en requirements.txt
# Embedding/Reranker/LLM (CPU por defecto, si usan GPU añadir torch con soporte CUDA/ROCm)
torch>=2.0.0
transformers>=4.30.0 # O la versión que necesites para Jina/Phi3/Llama3
sentencepiece # Necesario para muchos tokenizers HF
protobuf # A menudo requerido por HF
# LLM Local APIs
ollama # Si se usa el cliente oficial
requests # Para LM Studio y potencialmente otros
# Jina Models (se descargan vía transformers, pero listar explícitamente es bueno)
# jinaai>=... # Si hubiera una librería específica, si no, se baja por HF


---
File: /requirements.txt
---

# Core de ARES
toml
numpy
networkx
tqdm
tenacity

# Almacenamiento
qdrant-client>=1.9.0
# whoosh>=2.7.4
# sqlite3 viene con Python

fastembed

# Inferencia y Modelos (vLLM y dependencias)
vllm>=0.9.1
openai>=1.0.0 # Cliente para la API de vLLM
transformers>=4.41.0 # Para tokenizers y carga de modelos
sentencepiece # Dependencia común de tokenizers
protobuf

# Extracción de Documentos (Plugins)
PyMuPDF # Para el fast_pdf_parser

# Opcional para desarrollo/pruebas
memory_profiler
